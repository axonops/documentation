{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"/* Hide table of contents */ @media screen and (min-width: 60em) { .md-sidebar--secondary { display: none; } } /* Hide navigation */ @media screen and (min-width: 76.25em) { .md-sidebar--primary { display: none; } } Welcome to AxonOps \u00b6 AxonOps is the only platform available for the one-stop operations of Apache Cassandra. Built by Cassandra experts, AxonOps provides access to all of the capability required to effectively monitor, maintain and backup an Apache Cassandra environment. Accessed through a single intuitive UI and driven by a highly efficient bi-directional protocol ensuring unprecedented functionality and exceptional performance. Features \u00b6 Monitoring \u00b6 Visualize metrics & logs with dynamic dashboards Proactive Service Checks to never miss an issue Comprehensive alerting with enterprise-wide integration Maintenance \u00b6 Adaptive and Scheduled Cassandra Repairs to always be one step ahead Maintenance Job Scheduler to automate your key tasks Detailed cluster Configuration Views for the insights you need Backup \u00b6 Backup scheduling Visualise your backups and restores Restore backups with confidence Integrations \u00b6 AxonOps provide various integrations for notifications and backups Notifications: Backups: Soon you\u2019ll be enjoying the only Cassandra management tool that combines: Dynamic responsive dashboards curated by experts Adaptive Repair process that will never let you down Efficient and intuitive backup you can rely on Easy log-file association and interrogation Reliable rolling restart through automation Sophisticated alerting integration and routing PDF reporting to quickly deliver insights across the team Highly efficient protocol ensuring exceptional performance All of this is underpinned by an efficient bi-directional protocol ensuring exception performance and scale. If you would like to schedule some time with an AxonOps Cassandra expert to walk you through the install and the AxonOps platform please follow the link below. AxonOps Editions \u00b6 We offer AxonOps Cloud(SaaS) and Self-hosted solutions. To read more about our editions: Free Edition Enterprise Edition Ready to work with AxonOps? \u00b6 Dive into the Get Started guide and setup your account with everything you need to monitor, maintain and backup Cassandra in a matter of minutes. If you would like to schedule some time with an AxonOps Cassandra expert to walk you through the install and the AxonOps platform please follow the link below. Book an Expert","title":"Home"},{"location":"#welcome-to-axonops","text":"AxonOps is the only platform available for the one-stop operations of Apache Cassandra. Built by Cassandra experts, AxonOps provides access to all of the capability required to effectively monitor, maintain and backup an Apache Cassandra environment. Accessed through a single intuitive UI and driven by a highly efficient bi-directional protocol ensuring unprecedented functionality and exceptional performance.","title":"Welcome to AxonOps"},{"location":"#features","text":"","title":"Features"},{"location":"#monitoring","text":"Visualize metrics & logs with dynamic dashboards Proactive Service Checks to never miss an issue Comprehensive alerting with enterprise-wide integration","title":"Monitoring"},{"location":"#maintenance","text":"Adaptive and Scheduled Cassandra Repairs to always be one step ahead Maintenance Job Scheduler to automate your key tasks Detailed cluster Configuration Views for the insights you need","title":"Maintenance"},{"location":"#backup","text":"Backup scheduling Visualise your backups and restores Restore backups with confidence","title":"Backup"},{"location":"#integrations","text":"AxonOps provide various integrations for notifications and backups Notifications: Backups:","title":"Integrations"},{"location":"#axonops-editions","text":"We offer AxonOps Cloud(SaaS) and Self-hosted solutions. To read more about our editions: Free Edition Enterprise Edition","title":"AxonOps Editions"},{"location":"#ready-to-work-with-axonops","text":"Dive into the Get Started guide and setup your account with everything you need to monitor, maintain and backup Cassandra in a matter of minutes. If you would like to schedule some time with an AxonOps Cassandra expert to walk you through the install and the AxonOps platform please follow the link below. Book an Expert","title":"Ready to work with AxonOps?"},{"location":"cluster/cluster-overview/","text":"Cluster Overview is the home page and provides an overview of your cluster health, including OS , Cassandra and JVM details. The information is automatically extracted by the AxonOps agent and pushed to AxonOps server. There is no need to configure anything on the agent or the server side for this information to be populated in the Cluster Overview dashboard. On the Axonops application menu, select Cluster Overview . You can select a node to view some details on the OS , Cassandra or the JVM . Infomy OS Details \u00b6 Operating System Details section shows general information including: General Information CPU Memory Swap Disk volumes Infomy Cassandra Details \u00b6 Cassandra Details view shows the details from cassandra.yaml loaded into Cassandra. There is a search field available near the top to filter the parameters. Infomy JVM Details \u00b6 JVM Details section shows the general information about the JVM, including the version and some configurations such as the heap and Garbage Collection settings. Infomy","title":"Overview"},{"location":"cluster/cluster-overview/#os-details","text":"Operating System Details section shows general information including: General Information CPU Memory Swap Disk volumes Infomy","title":"OS Details"},{"location":"cluster/cluster-overview/#cassandra-details","text":"Cassandra Details view shows the details from cassandra.yaml loaded into Cassandra. There is a search field available near the top to filter the parameters. Infomy","title":"Cassandra Details"},{"location":"cluster/cluster-overview/#jvm-details","text":"JVM Details section shows the general information about the JVM, including the version and some configurations such as the heap and Garbage Collection settings. Infomy","title":"JVM Details"},{"location":"configuration/agent-configuration/","text":"Configuring AxonOps Agent \u00b6 Work in progress","title":"Configuring AxonOps Agent"},{"location":"configuration/agent-configuration/#configuring-axonops-agent","text":"Work in progress","title":"Configuring AxonOps Agent"},{"location":"configuration/axondash/","text":"Configuring AxonOps Agent \u00b6 Work in progress","title":"Configuring AxonOps Agent"},{"location":"configuration/axondash/#configuring-axonops-agent","text":"Work in progress","title":"Configuring AxonOps Agent"},{"location":"configuration/server-configuration/","text":"Configuring AxonOps Server \u00b6 Work in progress","title":"Configuring AxonOps Server"},{"location":"configuration/server-configuration/#configuring-axonops-server","text":"Work in progress","title":"Configuring AxonOps Server"},{"location":"editions/enterprise_edition/","text":"Enterprise Edition \u00b6 Scale-out Cassandra with Confidence! Scale-out with one-stop operations, full data retention, extended functionality and optional fully managed Cassandra. Book a consultation with a Cassandra Expert. Book an Expert Enterprise edition features \u00b6 Cloud or Self-Hosted, you choose. Unlimited Cassandra nodes Unlimited Cloud User Accounts Maximum metrics & logs retention Pre-configured Dashboards All metrics at 5 sec resolutions Hands-free Adaptive Repair Integrated Log & Event Search Cluster Health Service Checks Automate Rolling Restarts Backups & Restore for all storage needs Extensive Alerting Integrations Optional Cassandra Support Optional Consulting Services Optional Fully Managed Cassandra","title":"Enterprise Edition"},{"location":"editions/enterprise_edition/#enterprise-edition","text":"Scale-out Cassandra with Confidence! Scale-out with one-stop operations, full data retention, extended functionality and optional fully managed Cassandra. Book a consultation with a Cassandra Expert. Book an Expert","title":"Enterprise Edition"},{"location":"editions/enterprise_edition/#enterprise-edition-features","text":"Cloud or Self-Hosted, you choose. Unlimited Cassandra nodes Unlimited Cloud User Accounts Maximum metrics & logs retention Pre-configured Dashboards All metrics at 5 sec resolutions Hands-free Adaptive Repair Integrated Log & Event Search Cluster Health Service Checks Automate Rolling Restarts Backups & Restore for all storage needs Extensive Alerting Integrations Optional Cassandra Support Optional Consulting Services Optional Fully Managed Cassandra","title":"Enterprise edition features"},{"location":"editions/free_editions/","text":"Free Editions \u00b6 The free plan is avaialble for AxonOps SaaS. Developer which has a limited set of features and a Starter version that is free up to 6 nodes. Free Sign Up Developer edition features \u00b6 Develop software with ease with a desktop docker-compose install of a 3-node Cassandra cluster and AxonOps. Installed on developer desktop One command deployment A 3-node Cassandra cluster Only 1.5GB memory required Visualize your application queries Check your query consistency level Measure data model effectiveness Troubleshoot and optimize Accelerate production readiness No servers required Starter edition features \u00b6 One-stop operations for your production cluster with everything you need to monitor, maintain and backup Cassandra. Cloud or Self-Hosted, you choose. Up to 6 Cassandra nodes 2 Cloud User Accounts 14 day metrics & logs retention Pre-configured Dashboards All metrics at 5 sec resolutions Hands-free Adaptive Repair Integrated Log & Event Search Alerting Integration Cluster Health Service Checks Automate Rolling Restarts Backups & Restore for all storage needs Optional Cassandra Support Optional Consulting Services","title":"Free Editions"},{"location":"editions/free_editions/#free-editions","text":"The free plan is avaialble for AxonOps SaaS. Developer which has a limited set of features and a Starter version that is free up to 6 nodes. Free Sign Up","title":"Free Editions"},{"location":"editions/free_editions/#developer-edition-features","text":"Develop software with ease with a desktop docker-compose install of a 3-node Cassandra cluster and AxonOps. Installed on developer desktop One command deployment A 3-node Cassandra cluster Only 1.5GB memory required Visualize your application queries Check your query consistency level Measure data model effectiveness Troubleshoot and optimize Accelerate production readiness No servers required","title":"Developer edition features"},{"location":"editions/free_editions/#starter-edition-features","text":"One-stop operations for your production cluster with everything you need to monitor, maintain and backup Cassandra. Cloud or Self-Hosted, you choose. Up to 6 Cassandra nodes 2 Cloud User Accounts 14 day metrics & logs retention Pre-configured Dashboards All metrics at 5 sec resolutions Hands-free Adaptive Repair Integrated Log & Event Search Alerting Integration Cluster Health Service Checks Automate Rolling Restarts Backups & Restore for all storage needs Optional Cassandra Support Optional Consulting Services","title":"Starter edition features"},{"location":"editions/intro/","text":"Get ready for enjoying one-stop Cassandra operations from AxonOps. \u00b6 Soon you\u2019ll be enjoying the only Cassandra management tool that combines: Dynamic responsive dashboards curated by experts Adaptive Repair process that will never let you down Efficient and intuitive backup you can rely on Easy log-file association and interrogation Reliable rolling restart through automation Sophisticated alerting integration and routing PDF reporting to quickly deliver insights across the team Highly efficient protocol ensuring exceptional performance All of this is underpinned by an efficient bi-directional protocol ensuring exceptional performance and scale. If you would like to schedule some time with an AxonOps Cassandra expert to walk you through the install and the AxonOps platform please follow the link below. If you would like to schedule some time with an AxonOps Cassandra expert to walk you through the install and the AxonOps platform please follow the link below. Book an Expert","title":"Get ready for enjoying one-stop Cassandra operations from AxonOps."},{"location":"editions/intro/#get-ready-for-enjoying-one-stop-cassandra-operations-from-axonops","text":"Soon you\u2019ll be enjoying the only Cassandra management tool that combines: Dynamic responsive dashboards curated by experts Adaptive Repair process that will never let you down Efficient and intuitive backup you can rely on Easy log-file association and interrogation Reliable rolling restart through automation Sophisticated alerting integration and routing PDF reporting to quickly deliver insights across the team Highly efficient protocol ensuring exceptional performance All of this is underpinned by an efficient bi-directional protocol ensuring exceptional performance and scale. If you would like to schedule some time with an AxonOps Cassandra expert to walk you through the install and the AxonOps platform please follow the link below. If you would like to schedule some time with an AxonOps Cassandra expert to walk you through the install and the AxonOps platform please follow the link below. Book an Expert","title":"Get ready for enjoying one-stop Cassandra operations from AxonOps."},{"location":"get_started/agent_setup/","text":"Axon Agent Setup \u00b6 On the left menu click 'Agent setup' From 'Agent setup' click 'Agent configuration' then follow the on-screen steps. Once the Agents have been setup please use the Using AxonOps to familiarise yourself with AxonOps UI.","title":"Agent setup"},{"location":"get_started/agent_setup/#axon-agent-setup","text":"On the left menu click 'Agent setup' From 'Agent setup' click 'Agent configuration' then follow the on-screen steps. Once the Agents have been setup please use the Using AxonOps to familiarise yourself with AxonOps UI.","title":"Axon Agent Setup"},{"location":"get_started/docker/","text":"Docker-compose 3-Node Cassandra Desktop Install \u00b6 Install and start testing on a 3-node Cassandra cluster in minutes. No registration required just head to GitHub at: axonops-cassandra-dev-cluster Includes fully functional AxonOps management environment to enhance the developer experience. Visualize your application queries Check your query consistency level Measure data model effectiveness Troubleshoot and optimize","title":"Quickstart - Docker"},{"location":"get_started/docker/#docker-compose-3-node-cassandra-desktop-install","text":"Install and start testing on a 3-node Cassandra cluster in minutes. No registration required just head to GitHub at: axonops-cassandra-dev-cluster Includes fully functional AxonOps management environment to enhance the developer experience. Visualize your application queries Check your query consistency level Measure data model effectiveness Troubleshoot and optimize","title":"Docker-compose 3-Node Cassandra Desktop Install"},{"location":"get_started/saas/","text":"Setup your account \u00b6 Click on the Free Sign Up button to get started. Fill in your details on the registration screen Once your account has been registered a confirmation email will be sent. Click on the link in the link in the email to complete the setup process. You will be taken to the sign up screen where you can enter your sign-up details or choose to use Google or Microsoft Azure to complete the process. There will be another email sent to your email account to verify this is correct. Once the email has been verified you are signed on to AxonOps Cloud where you can now setup your Organisation On left hand menu select 'Subscriptions' From 'Subscriptions' select 'Plans' From 'Plans' select 'Get Started Free' Finally follow the Agent Setup to connect your cluster to AxonOps Cloud.","title":"Setup account"},{"location":"get_started/saas/#setup-your-account","text":"Click on the Free Sign Up button to get started. Fill in your details on the registration screen Once your account has been registered a confirmation email will be sent. Click on the link in the link in the email to complete the setup process. You will be taken to the sign up screen where you can enter your sign-up details or choose to use Google or Microsoft Azure to complete the process. There will be another email sent to your email account to verify this is correct. Once the email has been verified you are signed on to AxonOps Cloud where you can now setup your Organisation On left hand menu select 'Subscriptions' From 'Subscriptions' select 'Plans' From 'Plans' select 'Get Started Free' Finally follow the Agent Setup to connect your cluster to AxonOps Cloud.","title":"Setup your account"},{"location":"how-to/backup-restore-notifications/","text":"Setup Backup - Restore Notifications \u00b6 On the AxonOps application menu, click Operations -> Backups -> Setup and select Notifications tab. Notification Severities. \u00b6 Notification Severities. For each notifications severity Info Warning Error you can either use the slider to use the default routing or use the icon to customize the notification integrations. Notice: not available when default routing selected Infomy Customize Notifications. \u00b6 To customize notifications click on select the integrations that you require and click Close . Infomy Noticed: The Warning Integration were customized. You can remove these by clicking the . If you want to remove default routing groups from a severity and create custom groups , use the slider bar to remove default routing click the and follow this steps If you do not require any notifications ensure the default routing is off and delete any previously created custom notification. Infomy","title":"Setup Backup - Restore Notifications"},{"location":"how-to/backup-restore-notifications/#setup-backup-restore-notifications","text":"On the AxonOps application menu, click Operations -> Backups -> Setup and select Notifications tab.","title":"Setup Backup - Restore Notifications"},{"location":"how-to/backup-restore-notifications/#notification-severities","text":"Notification Severities. For each notifications severity Info Warning Error you can either use the slider to use the default routing or use the icon to customize the notification integrations. Notice: not available when default routing selected Infomy","title":"Notification Severities."},{"location":"how-to/backup-restore-notifications/#customize-notifications","text":"To customize notifications click on select the integrations that you require and click Close . Infomy Noticed: The Warning Integration were customized. You can remove these by clicking the . If you want to remove default routing groups from a severity and create custom groups , use the slider bar to remove default routing click the and follow this steps If you do not require any notifications ensure the default routing is off and delete any previously created custom notification. Infomy","title":"Customize Notifications."},{"location":"how-to/default-routing/","text":"Setup Default Routing \u00b6 Default Routing. Allows you to set up the channels though which alerts & notifications will be received and the specific groups that will receive the alerts & notifications Setup Default Routing \u00b6 On the Axonops application menu, select Alert & Notifications -> Integration and select Default Routing tab. Alert & Notification types can be set up Infomy Info Warning Error Info \u00b6 To Setup Default Routing For Info click On Select the desired group(s) from the dropdown menu for the desired integrations(s) and click` to confirm selections Infomy The group should now appear in the Info Info box on the Default Routing Tab Infomy Warning - Error \u00b6 Repeat these steps to setup the Default Routing for Warning Error Edit Default Routing \u00b6 To Edit Default Routing click on the icon on either Add or Remove existing integrations using the dropdown menus. Delete Default Routing \u00b6 To Remove a group click on the Delete icon Infomy","title":"Setup Default Routing"},{"location":"how-to/default-routing/#setup-default-routing","text":"Default Routing. Allows you to set up the channels though which alerts & notifications will be received and the specific groups that will receive the alerts & notifications","title":"Setup Default Routing"},{"location":"how-to/default-routing/#setup-default-routing_1","text":"On the Axonops application menu, select Alert & Notifications -> Integration and select Default Routing tab. Alert & Notification types can be set up Infomy Info Warning Error","title":"Setup Default Routing"},{"location":"how-to/default-routing/#info","text":"To Setup Default Routing For Info click On Select the desired group(s) from the dropdown menu for the desired integrations(s) and click` to confirm selections Infomy The group should now appear in the Info Info box on the Default Routing Tab Infomy","title":"Info"},{"location":"how-to/default-routing/#warning-error","text":"Repeat these steps to setup the Default Routing for Warning Error","title":"Warning - Error"},{"location":"how-to/default-routing/#edit-default-routing","text":"To Edit Default Routing click on the icon on either Add or Remove existing integrations using the dropdown menus.","title":"Edit Default Routing"},{"location":"how-to/default-routing/#delete-default-routing","text":"To Remove a group click on the Delete icon Infomy","title":"Delete Default Routing"},{"location":"how-to/reuse-host-id/","text":"Re-using an existing Host ID \u00b6 Each agent connected to the AxonOps server is assigned a unique host ID that is used internally to associate metrics and events with the node. If a Cassandra host dies and is replaced by another one with the same IP and token range then normally a new host ID will be generated and the replacement server will appear as a new machine in AxonOps. In this situation it is possible to re-use the same host ID so AxonOps sees the replacement server as the same as the original one. You can find the host ID of a node in the AxonOps GUI by going to Cluster Overview and selecting a node. In Graph View the host ID is shown next to the hostname in the details panel and in List View it is shown as Agent ID at the top of the details popup. If the old server's filesystem is still accessible you can also find the host ID stored in /var/lib/axonops/hostId . Follow these steps to start up a replacement server using the old host ID: Install axon-agent on the replacement server Stop axon-agent if it is running then delete these files if they exist: /var/lib/axonops/hostId , /var/lib/axonops/local.db Create a new file at /var/lib/axonops/hostId containing the host ID you wish to use Start axon-agent","title":"Re-use an existing host ID"},{"location":"how-to/reuse-host-id/#re-using-an-existing-host-id","text":"Each agent connected to the AxonOps server is assigned a unique host ID that is used internally to associate metrics and events with the node. If a Cassandra host dies and is replaced by another one with the same IP and token range then normally a new host ID will be generated and the replacement server will appear as a new machine in AxonOps. In this situation it is possible to re-use the same host ID so AxonOps sees the replacement server as the same as the original one. You can find the host ID of a node in the AxonOps GUI by going to Cluster Overview and selecting a node. In Graph View the host ID is shown next to the hostname in the details panel and in List View it is shown as Agent ID at the top of the details popup. If the old server's filesystem is still accessible you can also find the host ID stored in /var/lib/axonops/hostId . Follow these steps to start up a replacement server using the old host ID: Install axon-agent on the replacement server Stop axon-agent if it is running then delete these files if they exist: /var/lib/axonops/hostId , /var/lib/axonops/local.db Create a new file at /var/lib/axonops/hostId containing the host ID you wish to use Start axon-agent","title":"Re-using an existing Host ID"},{"location":"how-to/rolling-restart/","text":"","title":"Rolling restart"},{"location":"how-to/setup-alert-rules/","text":"Setup alert rules \u00b6 Insert Alert Rules Credentials \u00b6 On the Axonops application menu, click Dashboards and select required Dashboard. eg. System Hover over the desired Chart click on the button. Infomy Complete The Fields In Form \u00b6 Below the chart click on the alert tab. Infomy A form will appear Infomy Complete Alert settings in Comparator Warning value or Critical value or Both and the Duration ==> (how often to check) In Infomy Annotations \u00b6 In the Summary box you can include free text & type one or many of the following $labels $labels : - cluster - dc - hostname - org - rack - type - keyspace $value : In the Description box you can include free along with one or many of the above $labels Example CPU usage per DC Alerts usage on {{ $labels.hostname }} and cluster {{$labels.cluster}} Infomy Integrations \u00b6 Using the slider bar you can select any Integrations . Then click on the Info , Warning , Error icons, to select the group(s) of Integrations for the alert. Infomy Not selecting integrations If you do not select any specific Integrations the Alert will automatically follow the Global Dashboard Routing or if this has not been setup the Default Routing Integrations. Edit Alert Rule \u00b6 On the Axonops application menu, click Alerts & Notifications and click Active. Select the Alert Rules tab and click Infomy Delete Alert Rule(s) \u00b6 To Delete An Alert Either... On the Axonops application menu, click Dashboards and select required Dashboard. eg. System Hover over the desired Chart click on the button. Below the chart click on the alert tab and click on the of the alert rule you want to remove. OR... On the Axonops application menu, click Alerts & Notifications and click Active. Select the Alert Rules tab and click Infomy","title":"Setup Alert Rules"},{"location":"how-to/setup-alert-rules/#setup-alert-rules","text":"","title":"Setup alert rules"},{"location":"how-to/setup-alert-rules/#insert-alert-rules-credentials","text":"On the Axonops application menu, click Dashboards and select required Dashboard. eg. System Hover over the desired Chart click on the button. Infomy","title":"Insert Alert Rules Credentials"},{"location":"how-to/setup-alert-rules/#complete-the-fields-in-form","text":"Below the chart click on the alert tab. Infomy A form will appear Infomy Complete Alert settings in Comparator Warning value or Critical value or Both and the Duration ==> (how often to check) In Infomy","title":"Complete The Fields In Form"},{"location":"how-to/setup-alert-rules/#annotations","text":"In the Summary box you can include free text & type one or many of the following $labels $labels : - cluster - dc - hostname - org - rack - type - keyspace $value : In the Description box you can include free along with one or many of the above $labels Example CPU usage per DC Alerts usage on {{ $labels.hostname }} and cluster {{$labels.cluster}} Infomy","title":"Annotations"},{"location":"how-to/setup-alert-rules/#integrations","text":"Using the slider bar you can select any Integrations . Then click on the Info , Warning , Error icons, to select the group(s) of Integrations for the alert. Infomy Not selecting integrations If you do not select any specific Integrations the Alert will automatically follow the Global Dashboard Routing or if this has not been setup the Default Routing Integrations.","title":"Integrations"},{"location":"how-to/setup-alert-rules/#edit-alert-rule","text":"On the Axonops application menu, click Alerts & Notifications and click Active. Select the Alert Rules tab and click Infomy","title":"Edit Alert Rule"},{"location":"how-to/setup-alert-rules/#delete-alert-rules","text":"To Delete An Alert Either... On the Axonops application menu, click Dashboards and select required Dashboard. eg. System Hover over the desired Chart click on the button. Below the chart click on the alert tab and click on the of the alert rule you want to remove. OR... On the Axonops application menu, click Alerts & Notifications and click Active. Select the Alert Rules tab and click Infomy","title":"Delete Alert Rule(s)"},{"location":"how-to/setup-dashboards-global-integrations/","text":"Setup Dashboards Global Integrations \u00b6 On the Axonops application menu, click Alerts & Notifications -> Active and select Dashboards Global Routing tab. Notification Severities. \u00b6 Notification Severities. For each notifications severity Info Warning Error you can either use the slider to use the default routing or use the icon to customize the notification integrations. Notice: not available when default routing selected Infomy Customize Notifications. \u00b6 To customize notifications click on select the integrations that you require and click Close . Infomy Noticed: The Warning Integration were customized. You can remove these by clicking the . If you want to remove default routing groups from a severity and create custom groups , use the slider bar to remove default routing click the and follow this steps If you do not require any notifications ensure the default routing is off and delete any previously created custom notification. Infomy","title":"Setup Dashboards Global Integrations"},{"location":"how-to/setup-dashboards-global-integrations/#setup-dashboards-global-integrations","text":"On the Axonops application menu, click Alerts & Notifications -> Active and select Dashboards Global Routing tab.","title":"Setup Dashboards Global Integrations"},{"location":"how-to/setup-dashboards-global-integrations/#notification-severities","text":"Notification Severities. For each notifications severity Info Warning Error you can either use the slider to use the default routing or use the icon to customize the notification integrations. Notice: not available when default routing selected Infomy","title":"Notification Severities."},{"location":"how-to/setup-dashboards-global-integrations/#customize-notifications","text":"To customize notifications click on select the integrations that you require and click Close . Infomy Noticed: The Warning Integration were customized. You can remove these by clicking the . If you want to remove default routing groups from a severity and create custom groups , use the slider bar to remove default routing click the and follow this steps If you do not require any notifications ensure the default routing is off and delete any previously created custom notification. Infomy","title":"Customize Notifications."},{"location":"how-to/setup-log-collection/","text":"AxonOps dashboards provides a comprehensive set of charts with an embedded view for logs and events. The goal is to correlate metrics with logs/events as you can zoom in the logs histogram or metrics charts to drill down both results. Log and event view is located in the bottom part of that page that you can expand/collapse with the horizontal splitter. The setup for log collection is accessible via Settings > logs Infomy newlineregex is used by the log collector to handle multilines logs. Default newlineregex for Cassandra should be ok unless you've customized it.","title":"Setup Log Collection"},{"location":"how-to/setup-servicechecks/","text":"Setup Service Checks \u00b6 Add Service Checks \u00b6 On the Axonops application menu, click Service Checks and select Setup tab. Infomy Create Services \u00b6 Below there few examples copy and Paste inside. and click save { \"shellchecks\" : [ { \"name\" : \"check_elastic\" , \"shell\" : \"/bin/bash\" , \"script\" : \"if [ 'ps auxwww | grep elastic | wc -l' -lt 1 ] then exit 2 else exit 0 fi\" , \"interval\" : \"5m\" , \"timeout\" : \"1m\" } ], \"httpchecks\" : [], \"tcpchecks\" : [ { \"name\" : \"elastic_tcp_endpoint_check\" , \"interval\" : \"5s\" , \"timeout\" : \"1m\" , \"tcp\" : \"localhost:9200\" } ] } Example: Infomy Delete Services \u00b6 To Delete a service copy and Paste inside. and click save { \"shellchecks\" : [], \"httpchecks\" : [], \"tcpchecks\" : [] } Example: Infomy","title":"Setup Service Checks"},{"location":"how-to/setup-servicechecks/#setup-service-checks","text":"","title":"Setup Service Checks"},{"location":"how-to/setup-servicechecks/#add-service-checks","text":"On the Axonops application menu, click Service Checks and select Setup tab. Infomy","title":"Add Service Checks"},{"location":"how-to/setup-servicechecks/#create-services","text":"Below there few examples copy and Paste inside. and click save { \"shellchecks\" : [ { \"name\" : \"check_elastic\" , \"shell\" : \"/bin/bash\" , \"script\" : \"if [ 'ps auxwww | grep elastic | wc -l' -lt 1 ] then exit 2 else exit 0 fi\" , \"interval\" : \"5m\" , \"timeout\" : \"1m\" } ], \"httpchecks\" : [], \"tcpchecks\" : [ { \"name\" : \"elastic_tcp_endpoint_check\" , \"interval\" : \"5s\" , \"timeout\" : \"1m\" , \"tcp\" : \"localhost:9200\" } ] } Example: Infomy","title":"Create Services"},{"location":"how-to/setup-servicechecks/#delete-services","text":"To Delete a service copy and Paste inside. and click save { \"shellchecks\" : [], \"httpchecks\" : [], \"tcpchecks\" : [] } Example: Infomy","title":"Delete Services"},{"location":"installation/axon-agent/install/","text":"axon-agent installation \u00b6 There 2 elements to the AxonOps agent. The first is the axon-agent, which is a native application for Linux running as a standalone daemon process. The second is the Java agent which is added to the Java process. Two components communicate with each other using the Unix domain socket. The reason for this approach are the following requirements we have on the agent process. No JMX Metrics must push metrics from Cassandra all the way to the AxonOps server - never pull. AxonOps Java agent will push the metrics to the AxonOps native agent, which in turn pushes them to the AxonOps server. Scraping a large volume of metrics against the JMX is slow. We also wanted to avoid exposing an HTTP endpoint within Cassandra like the Prometheus JMX exporter does. The messaging between native agent and Java agent is bidirectional, i.e. AxonOps server sends control messages to Cassandra for operations such as repair and backups without the use of JMX. This section describes how to install and configure both the native agent and Java agent. CentOS / RedHat \u00b6 sudo tee /etc/yum.repos.d/axonops-yum.repo << EOL [axonops-yum] name=axonops-yum baseurl=https://packages.axonops.com/yum/ enabled=1 repo_gpgcheck=0 gpgcheck=0 EOL sudo yum install axon-agent Debian / Ubuntu \u00b6 sudo apt-get update sudo apt-get install curl gnupg ca-certificates curl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add - echo \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-agent For new versions of Debian (>= bookworm) and Ubuntu (>= 22.04) the process of setting up the apt repository has changed. See below: sudo apt-get update sudo apt-get install -y curl gnupg ca-certificates curl -L https://packages.axonops.com/apt/repo-signing-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/axonops.gpg echo \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-agent Package details \u00b6 Configuration: /etc/axonops/axon-agent.yml Binary: usr/share/axonops/axon-agent Logs : /var/log/axonops/axon-agent.log Systemd service: /usr/lib/systemd/system/axon-agent.service certificate file used for it's OpenTSDB endpoint when SSL is active: /etc/axonops/agent.crt key file used for it's OpenTSDB endpoint when SSL is active: /etc/axonops/agent.key Configuration \u00b6 Make sure axon-agent configuration points to the correct axon-server address and your organisation name is specified: axon-server : hosts : \"axon-server_endpoint\" # Specify axon-server IP axon-server.mycompany.com axon-agent : org : \"my-company-test\" # Specify your organisation name type : \"cassandra\" NTP : host : \"ntp.mycompany.com\" # Specify you NTP server IP address or hostname Start axon-agent \u00b6 systemctl daemon-reload systemctl start axon-agent systemctl status axon-agent This will start the axon-agent process as the axonops user, which was created during the package installation. Note that you will have to refresh axon-dash page to show the newly connected node. Next Steps \u00b6 To complete your agent installation you will need to follow the steps in the link below: cassandra","title":"axon-agent installation"},{"location":"installation/axon-agent/install/#axon-agent-installation","text":"There 2 elements to the AxonOps agent. The first is the axon-agent, which is a native application for Linux running as a standalone daemon process. The second is the Java agent which is added to the Java process. Two components communicate with each other using the Unix domain socket. The reason for this approach are the following requirements we have on the agent process. No JMX Metrics must push metrics from Cassandra all the way to the AxonOps server - never pull. AxonOps Java agent will push the metrics to the AxonOps native agent, which in turn pushes them to the AxonOps server. Scraping a large volume of metrics against the JMX is slow. We also wanted to avoid exposing an HTTP endpoint within Cassandra like the Prometheus JMX exporter does. The messaging between native agent and Java agent is bidirectional, i.e. AxonOps server sends control messages to Cassandra for operations such as repair and backups without the use of JMX. This section describes how to install and configure both the native agent and Java agent.","title":"axon-agent installation"},{"location":"installation/axon-agent/install/#centos-redhat","text":"sudo tee /etc/yum.repos.d/axonops-yum.repo << EOL [axonops-yum] name=axonops-yum baseurl=https://packages.axonops.com/yum/ enabled=1 repo_gpgcheck=0 gpgcheck=0 EOL sudo yum install axon-agent","title":"CentOS / RedHat"},{"location":"installation/axon-agent/install/#debian-ubuntu","text":"sudo apt-get update sudo apt-get install curl gnupg ca-certificates curl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add - echo \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-agent For new versions of Debian (>= bookworm) and Ubuntu (>= 22.04) the process of setting up the apt repository has changed. See below: sudo apt-get update sudo apt-get install -y curl gnupg ca-certificates curl -L https://packages.axonops.com/apt/repo-signing-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/axonops.gpg echo \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-agent","title":"Debian / Ubuntu"},{"location":"installation/axon-agent/install/#package-details","text":"Configuration: /etc/axonops/axon-agent.yml Binary: usr/share/axonops/axon-agent Logs : /var/log/axonops/axon-agent.log Systemd service: /usr/lib/systemd/system/axon-agent.service certificate file used for it's OpenTSDB endpoint when SSL is active: /etc/axonops/agent.crt key file used for it's OpenTSDB endpoint when SSL is active: /etc/axonops/agent.key","title":"Package details"},{"location":"installation/axon-agent/install/#configuration","text":"Make sure axon-agent configuration points to the correct axon-server address and your organisation name is specified: axon-server : hosts : \"axon-server_endpoint\" # Specify axon-server IP axon-server.mycompany.com axon-agent : org : \"my-company-test\" # Specify your organisation name type : \"cassandra\" NTP : host : \"ntp.mycompany.com\" # Specify you NTP server IP address or hostname","title":"Configuration"},{"location":"installation/axon-agent/install/#start-axon-agent","text":"systemctl daemon-reload systemctl start axon-agent systemctl status axon-agent This will start the axon-agent process as the axonops user, which was created during the package installation. Note that you will have to refresh axon-dash page to show the newly connected node.","title":"Start axon-agent"},{"location":"installation/axon-agent/install/#next-steps","text":"To complete your agent installation you will need to follow the steps in the link below: cassandra","title":"Next Steps"},{"location":"installation/axon-dash/install/","text":"AxonOps GUI installation \u00b6 AxonOps GUI service is installed as a separate service to AxonOps Server. The GUI service (axon-dash) can be co-hosted on the same server as the AxonOps Server process, or they can be running on 2 separate servers. This section describes the installation process for the GUI service. Step 1 - Installation \u00b6 CentOS / RedHat \u00b6 sudo tee /etc/yum.repos.d/axonops-yum.repo << EOL [axonops-yum] name=axonops-yum baseurl=https://packages.axonops.com/yum/ enabled=1 repo_gpgcheck=0 gpgcheck=0 EOL sudo yum install axon-dash Debian / Ubuntu \u00b6 sudo apt-get update sudo apt-get install curl gnupg ca-certificates curl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add - echo \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-dash For new versions of Debian (>= bookworm) and Ubuntu (>= 22.04) the process of setting up the apt repository has changed. See below: sudo apt-get update sudo apt-get install -y curl gnupg ca-certificates curl -L https://packages.axonops.com/apt/repo-signing-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/axonops.gpg echo \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-dash Step 2 - Configuration \u00b6 Change axon-dash configuration to specify axon-server listening address. /etc/axonops/axon-dash.yml axon-dash : # The listening address of axon-dash host : 0.0.0.0 port : 3000 line_charts_max_results : 256 axon-server : private_endpoints : \"http://127.0.0.1:8080\" # HTTP endpoint to access axon-server API from axon-dash. context_path : \"\" # example: \"/gui\" axon-server default API port is 8080 Step 3 - axon-server configuration update \u00b6 if required, update axon-server configuration by setting the correct axon-dash host and port : /etc/axonops/axon-server.yml ... axon-dash : # This must point to the axon-dash address accessible from axon-server host : 127.0.0.1 port : 3000 https : false ... Step 4 - Restart axon-server after updating its configuration \u00b6 sudo systemctl restart axon-server Step 5 - Start axon-dash \u00b6 sudo systemctl daemon-reload sudo systemctl start axon-dash sudo systemctl status axon-dash This will start the axon-dash process as the axonops user, which was created during the package installation. The default listening address is 0.0.0.0:3000 . Package details \u00b6 Configuration: /etc/axonops/axon-dash.yml Binary: /usr/share/axonops/axon-dash Logs: /var/log/axonops/axon-dash.log Systemd service: /usr/lib/systemd/system/axon-dash.service Copyright : /usr/share/doc/axonops/axon-dash/copyright Licenses : /usr/share/axonops/licenses/axon-dash/ Step 6 - Installing agents \u00b6 Now axon-dash is installed, you can start installing cassandra-agent","title":"AxonOps Dash"},{"location":"installation/axon-dash/install/#axonops-gui-installation","text":"AxonOps GUI service is installed as a separate service to AxonOps Server. The GUI service (axon-dash) can be co-hosted on the same server as the AxonOps Server process, or they can be running on 2 separate servers. This section describes the installation process for the GUI service.","title":"AxonOps GUI installation"},{"location":"installation/axon-dash/install/#step-1-installation","text":"","title":"Step 1 - Installation"},{"location":"installation/axon-dash/install/#centos-redhat","text":"sudo tee /etc/yum.repos.d/axonops-yum.repo << EOL [axonops-yum] name=axonops-yum baseurl=https://packages.axonops.com/yum/ enabled=1 repo_gpgcheck=0 gpgcheck=0 EOL sudo yum install axon-dash","title":"CentOS / RedHat"},{"location":"installation/axon-dash/install/#debian-ubuntu","text":"sudo apt-get update sudo apt-get install curl gnupg ca-certificates curl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add - echo \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-dash For new versions of Debian (>= bookworm) and Ubuntu (>= 22.04) the process of setting up the apt repository has changed. See below: sudo apt-get update sudo apt-get install -y curl gnupg ca-certificates curl -L https://packages.axonops.com/apt/repo-signing-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/axonops.gpg echo \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-dash","title":"Debian / Ubuntu"},{"location":"installation/axon-dash/install/#step-2-configuration","text":"Change axon-dash configuration to specify axon-server listening address. /etc/axonops/axon-dash.yml axon-dash : # The listening address of axon-dash host : 0.0.0.0 port : 3000 line_charts_max_results : 256 axon-server : private_endpoints : \"http://127.0.0.1:8080\" # HTTP endpoint to access axon-server API from axon-dash. context_path : \"\" # example: \"/gui\" axon-server default API port is 8080","title":"Step 2 - Configuration"},{"location":"installation/axon-dash/install/#step-3-axon-server-configuration-update","text":"if required, update axon-server configuration by setting the correct axon-dash host and port : /etc/axonops/axon-server.yml ... axon-dash : # This must point to the axon-dash address accessible from axon-server host : 127.0.0.1 port : 3000 https : false ...","title":"Step 3 - axon-server configuration update"},{"location":"installation/axon-dash/install/#step-4-restart-axon-server-after-updating-its-configuration","text":"sudo systemctl restart axon-server","title":"Step 4 - Restart axon-server after updating its configuration"},{"location":"installation/axon-dash/install/#step-5-start-axon-dash","text":"sudo systemctl daemon-reload sudo systemctl start axon-dash sudo systemctl status axon-dash This will start the axon-dash process as the axonops user, which was created during the package installation. The default listening address is 0.0.0.0:3000 .","title":"Step 5 - Start axon-dash"},{"location":"installation/axon-dash/install/#package-details","text":"Configuration: /etc/axonops/axon-dash.yml Binary: /usr/share/axonops/axon-dash Logs: /var/log/axonops/axon-dash.log Systemd service: /usr/lib/systemd/system/axon-dash.service Copyright : /usr/share/doc/axonops/axon-dash/copyright Licenses : /usr/share/axonops/licenses/axon-dash/","title":"Package details"},{"location":"installation/axon-dash/install/#step-6-installing-agents","text":"Now axon-dash is installed, you can start installing cassandra-agent","title":"Step 6 - Installing agents"},{"location":"installation/axon-server/centos/","text":"axon-server installation (CentOS / RedHat) \u00b6 Step 1 - Prerequisites \u00b6 Elasticsearch stores the data collected by axon-server. AxonOps is currently only compatible with Elasticsearch 7.x, we recommend installing the latest available 7.x release. Installing Elasticsearch \u00b6 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.16-x86_64.rpm wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.16-x86_64.rpm.sha512 sha512sum -c elasticsearch-7.17.16-x86_64.rpm.sha512 sudo rpm -i elasticsearch-7.17.16-x86_64.rpm The sha512sum command above verifies the downloaded package and should show this output: elasticsearch-7.17.16-x86_64.rpm: OK Increase the bulk queue size of Elasticsearch by running the following command: sudo echo 'thread_pool.write.queue_size: 2000' >> /etc/elasticsearch/elasticsearch.yml Increase the default heap size of elasticsearch by editing /etc/elasticsearch/jvm.options . From: -Xms1g -Xmx1g To: -Xms8g -Xmx8g This will set the minimum and maximum heap size to 8 GB. Set Xmx and Xms to no more than 50% of your physical RAM. Elasticsearch requires memory for purposes other than the JVM heap and it is important to leave space for this. Set the following index codec by running the following command: sudo echo 'index.codec: best_compression' >> /etc/elasticsearch/elasticsearch.yml Elasticsearch uses an mmapfs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions. You can increase the limits by running the following command: sudo sysctl -w vm.max_map_count = 262144 To make this change persist across reboots run this command: echo \"vm.max_map_count = 262144\" | sudo tee /etc/sysctl.d/10-elasticsearch.conf >dev/null Also, Elasticsearch needs max file descriptors system settings at least to 65536. echo 'elasticsearch - nofile 65536' | sudo tee --append /etc/security/limits.conf > /dev/null Start Elasticsearch \u00b6 sudo systemctl start elasticsearch.service After a short period of time, you can verify that your Elasticsearch node is running by sending an HTTP request to port 9200 on localhost: curl \"localhost:9200\" Step 2 - axon-server \u00b6 sudo tee /etc/yum.repos.d/axonops-yum.repo << EOL [axonops-yum] name=axonops-yum baseurl=https://packages.axonops.com/yum/ enabled=1 repo_gpgcheck=0 gpgcheck=0 EOL sudo yum install axon-server Step 3 - axon-server configurations \u00b6 Make sure elastic_host and elastic_port are corresponding to your Elasticsearch instance. /etc/axonops/axon-server.yml host : 0.0.0.0 # axon-server listening address (used by axon-dash and axon-agent) (env variable: AXONSERVER_HOST) api_port : 8080 # axon-server HTTP API listening port (used by axon-dash) (AXONSERVER_PORT) agents_port : 1888 # axon-server listening port for agent connections elastic_hosts : # Elasticsearch endpoint (env variable:ELASTIC_HOSTS, comma separated list) - http://localhost #integrations_proxy: # proxy endpoint for integrations. (INTEGRATIONS_PROXY) # For better performance on large clusters, you can use a CQL store for the metrics. # To opt-in for CQL metrics storage, just specify at least one CQL host. # We do recommend to specify a NetworkTopologyStrategy for cql_keyspace_replication #cql_hosts: # (CQL_HOSTS, comma separated list) # - 192.168.0.10:9042 # - 192.168.0.11:9042 #cql_username: \"cassandra\" # (CQL_USERNAME) #cql_password: \"cassandra\" # (CQL_PASSWORD) #cql_local_dc: datacenter1 # (CQL_LOCAL_DC) #cql_ssl: false # (CQL_SSL) #cql_skip_verify: false # (CQL_SSL_SKIP_VERIFY) #cql_ca_file: /path/to/ca_file # (CQL_CA_FILE) #cql_cert_file: /path/to/cert_file # (CQL_CERT_FILE) #cql_key_file: /path/to/key_file # (CQL_KEY_FILE) #cql_proto_version: 4 # (CQL_PROTO_VERSION) #cql_max_concurrent_reads: 1000 # (CQL_MAX_CONCURRENT_READS) #cql_batch_size: 1 # (CQL_BATCH_SIZE) #cql_page_size: 10 # (CQL_PAGE_SIZE) #cql_autocreate_tables: true # (CQL_AUTO_CREATE_TABLES) this will tell axon-server to automatically create the metrics tables (true is recommended) #cql_keyspace_replication: \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\" # (CQL_KS_REPLICATION) keyspace replication for the metrics tables #cql_retrypolicy_numretries: 3 # (CQL_RETRY_POLICY_NUM_RETRIES) #cql_retrypolicy_min: 1s # (CQL_RETRY_POLICY_MIN) #cql_retrypolicy_max: 10s # (CQL_RETRY_POLICY_MAX) #cql_reconnectionpolicy_maxretries: 10 # (CQL_RECONNECTION_POLICY_MAX_RETRIES) #cql_reconnectionpolicy_initialinterval: 1s # (CQL_RECONNECTION_POLICY_INITIAL_INTERVAL) #cql_reconnectionpolicy_maxinterval: 10s # (CQL_RECONNECTION_POLICY_MAX_INTERVAL) #cql_metrics_cache_max_size_mb: 100 #MB # (CQL_METRICS_CACHE_MAX_SIZE_MB) #cql_read_consistency: \"LOCAL_ONE\" # (CQL_READ_CONSISTENCY) #One of the following: ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE #cql_write_consistency: \"LOCAL_ONE\" # (CQL_WRITE_CONSISTENCY) #One of the following: ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE #cql_lvl1_compaction_window_size: 12 # (CQL_LVL1_COMPACTION_WINDOW_SIZE) #cql_lvl2_compaction_window_size: 1 # (CQL_LVL2_COMPACTION_WINDOW_SIZE) #cql_lvl3_compaction_window_size: 1 # (CQL_LVL3_COMPACTION_WINDOW_SIZE) #cql_lvl4_compaction_window_size: 10 # (CQL_LVL4_COMPACTION_WINDOW_SIZE) #cql_lvl5_compaction_window_size: 120 # (CQL_LVL5_COMPACTION_WINDOW_SIZE) axon-dash : # This must point to the axon-dash address accessible from axon-server host : 127.0.0.1 port : 3000 https : false alerting : # How long to wait before sending a notification again if it has already # been sent successfully for an alert. (Usually ~3h or more). notification_interval : 3h # Default retention settings, most can be overridden from the frontend retention : events : 8w # logs and events retention. Must be expressed in weeks (w) metrics : high_resolution : 14d # High frequency metrics. Must be expressed in days (d) med_resolution : 12w # Must be expressed in weeks (w) low_resolution : 12M # Must be expressed in months (M) super_low_resolution : 2y # Must be expressed in years (y) backups : # Those are use as defaults but can be overridden from the UI local : 10d remote : 30d # Storage options for PDF reports # Override the default local path of /var/lib/axonops/reports #report_storage_path: /my/reports/storage/directory # Alternatively store PDF reports in an object store by providing report_storage_config #report_storage_path: my-reports-s3-bucket/reports-folder #report_storage_config: # type: s3 # provider: AWS # access_key_id: MY_ACCESS_KEY_ID # secret_access_key: MY_SECRET_ACCESS_KEY # region: us-east-1 # acl: private # server_side_encryption: AES256 # storage_class: STANDARD For better performances on large clusters (100+ nodes), you can use a CQL store for the metrics such as Cassandra . To opt-in for CQL metrics storage, specify at least one CQL host with axon-server configuration. Step 4 - Start the server \u00b6 sudo systemctl daemon-reload sudo systemctl start axon-server sudo systemctl status axon-server This will start the axon-server process as the axonops user, which was created during the package installation. The default listening address is 0.0.0.0:8080 . Package details \u00b6 Configuration: /etc/axonops/axon-server.yml Binary: /usr/share/axonops/axon-server Logs: /var/log/axonops/axon-server.log Systemd service: /usr/lib/systemd/system/axon-server.service Copyright : /usr/share/doc/axonops/axon-server/copyright Licenses : /usr/share/axonops/licenses/axon-server/ Step 5 - Installing axon-dash \u00b6 Now axon-server is installed, you can start installing the GUI for it: axon-dash","title":"Installing on Centos/Redhat"},{"location":"installation/axon-server/centos/#axon-server-installation-centos-redhat","text":"","title":"axon-server installation (CentOS / RedHat)"},{"location":"installation/axon-server/centos/#step-1-prerequisites","text":"Elasticsearch stores the data collected by axon-server. AxonOps is currently only compatible with Elasticsearch 7.x, we recommend installing the latest available 7.x release.","title":"Step 1 - Prerequisites"},{"location":"installation/axon-server/centos/#installing-elasticsearch","text":"wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.16-x86_64.rpm wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.16-x86_64.rpm.sha512 sha512sum -c elasticsearch-7.17.16-x86_64.rpm.sha512 sudo rpm -i elasticsearch-7.17.16-x86_64.rpm The sha512sum command above verifies the downloaded package and should show this output: elasticsearch-7.17.16-x86_64.rpm: OK Increase the bulk queue size of Elasticsearch by running the following command: sudo echo 'thread_pool.write.queue_size: 2000' >> /etc/elasticsearch/elasticsearch.yml Increase the default heap size of elasticsearch by editing /etc/elasticsearch/jvm.options . From: -Xms1g -Xmx1g To: -Xms8g -Xmx8g This will set the minimum and maximum heap size to 8 GB. Set Xmx and Xms to no more than 50% of your physical RAM. Elasticsearch requires memory for purposes other than the JVM heap and it is important to leave space for this. Set the following index codec by running the following command: sudo echo 'index.codec: best_compression' >> /etc/elasticsearch/elasticsearch.yml Elasticsearch uses an mmapfs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions. You can increase the limits by running the following command: sudo sysctl -w vm.max_map_count = 262144 To make this change persist across reboots run this command: echo \"vm.max_map_count = 262144\" | sudo tee /etc/sysctl.d/10-elasticsearch.conf >dev/null Also, Elasticsearch needs max file descriptors system settings at least to 65536. echo 'elasticsearch - nofile 65536' | sudo tee --append /etc/security/limits.conf > /dev/null","title":"Installing Elasticsearch"},{"location":"installation/axon-server/centos/#start-elasticsearch","text":"sudo systemctl start elasticsearch.service After a short period of time, you can verify that your Elasticsearch node is running by sending an HTTP request to port 9200 on localhost: curl \"localhost:9200\"","title":"Start Elasticsearch"},{"location":"installation/axon-server/centos/#step-2-axon-server","text":"sudo tee /etc/yum.repos.d/axonops-yum.repo << EOL [axonops-yum] name=axonops-yum baseurl=https://packages.axonops.com/yum/ enabled=1 repo_gpgcheck=0 gpgcheck=0 EOL sudo yum install axon-server","title":"Step 2 - axon-server"},{"location":"installation/axon-server/centos/#step-3-axon-server-configurations","text":"Make sure elastic_host and elastic_port are corresponding to your Elasticsearch instance. /etc/axonops/axon-server.yml host : 0.0.0.0 # axon-server listening address (used by axon-dash and axon-agent) (env variable: AXONSERVER_HOST) api_port : 8080 # axon-server HTTP API listening port (used by axon-dash) (AXONSERVER_PORT) agents_port : 1888 # axon-server listening port for agent connections elastic_hosts : # Elasticsearch endpoint (env variable:ELASTIC_HOSTS, comma separated list) - http://localhost #integrations_proxy: # proxy endpoint for integrations. (INTEGRATIONS_PROXY) # For better performance on large clusters, you can use a CQL store for the metrics. # To opt-in for CQL metrics storage, just specify at least one CQL host. # We do recommend to specify a NetworkTopologyStrategy for cql_keyspace_replication #cql_hosts: # (CQL_HOSTS, comma separated list) # - 192.168.0.10:9042 # - 192.168.0.11:9042 #cql_username: \"cassandra\" # (CQL_USERNAME) #cql_password: \"cassandra\" # (CQL_PASSWORD) #cql_local_dc: datacenter1 # (CQL_LOCAL_DC) #cql_ssl: false # (CQL_SSL) #cql_skip_verify: false # (CQL_SSL_SKIP_VERIFY) #cql_ca_file: /path/to/ca_file # (CQL_CA_FILE) #cql_cert_file: /path/to/cert_file # (CQL_CERT_FILE) #cql_key_file: /path/to/key_file # (CQL_KEY_FILE) #cql_proto_version: 4 # (CQL_PROTO_VERSION) #cql_max_concurrent_reads: 1000 # (CQL_MAX_CONCURRENT_READS) #cql_batch_size: 1 # (CQL_BATCH_SIZE) #cql_page_size: 10 # (CQL_PAGE_SIZE) #cql_autocreate_tables: true # (CQL_AUTO_CREATE_TABLES) this will tell axon-server to automatically create the metrics tables (true is recommended) #cql_keyspace_replication: \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\" # (CQL_KS_REPLICATION) keyspace replication for the metrics tables #cql_retrypolicy_numretries: 3 # (CQL_RETRY_POLICY_NUM_RETRIES) #cql_retrypolicy_min: 1s # (CQL_RETRY_POLICY_MIN) #cql_retrypolicy_max: 10s # (CQL_RETRY_POLICY_MAX) #cql_reconnectionpolicy_maxretries: 10 # (CQL_RECONNECTION_POLICY_MAX_RETRIES) #cql_reconnectionpolicy_initialinterval: 1s # (CQL_RECONNECTION_POLICY_INITIAL_INTERVAL) #cql_reconnectionpolicy_maxinterval: 10s # (CQL_RECONNECTION_POLICY_MAX_INTERVAL) #cql_metrics_cache_max_size_mb: 100 #MB # (CQL_METRICS_CACHE_MAX_SIZE_MB) #cql_read_consistency: \"LOCAL_ONE\" # (CQL_READ_CONSISTENCY) #One of the following: ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE #cql_write_consistency: \"LOCAL_ONE\" # (CQL_WRITE_CONSISTENCY) #One of the following: ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE #cql_lvl1_compaction_window_size: 12 # (CQL_LVL1_COMPACTION_WINDOW_SIZE) #cql_lvl2_compaction_window_size: 1 # (CQL_LVL2_COMPACTION_WINDOW_SIZE) #cql_lvl3_compaction_window_size: 1 # (CQL_LVL3_COMPACTION_WINDOW_SIZE) #cql_lvl4_compaction_window_size: 10 # (CQL_LVL4_COMPACTION_WINDOW_SIZE) #cql_lvl5_compaction_window_size: 120 # (CQL_LVL5_COMPACTION_WINDOW_SIZE) axon-dash : # This must point to the axon-dash address accessible from axon-server host : 127.0.0.1 port : 3000 https : false alerting : # How long to wait before sending a notification again if it has already # been sent successfully for an alert. (Usually ~3h or more). notification_interval : 3h # Default retention settings, most can be overridden from the frontend retention : events : 8w # logs and events retention. Must be expressed in weeks (w) metrics : high_resolution : 14d # High frequency metrics. Must be expressed in days (d) med_resolution : 12w # Must be expressed in weeks (w) low_resolution : 12M # Must be expressed in months (M) super_low_resolution : 2y # Must be expressed in years (y) backups : # Those are use as defaults but can be overridden from the UI local : 10d remote : 30d # Storage options for PDF reports # Override the default local path of /var/lib/axonops/reports #report_storage_path: /my/reports/storage/directory # Alternatively store PDF reports in an object store by providing report_storage_config #report_storage_path: my-reports-s3-bucket/reports-folder #report_storage_config: # type: s3 # provider: AWS # access_key_id: MY_ACCESS_KEY_ID # secret_access_key: MY_SECRET_ACCESS_KEY # region: us-east-1 # acl: private # server_side_encryption: AES256 # storage_class: STANDARD For better performances on large clusters (100+ nodes), you can use a CQL store for the metrics such as Cassandra . To opt-in for CQL metrics storage, specify at least one CQL host with axon-server configuration.","title":"Step 3 - axon-server configurations"},{"location":"installation/axon-server/centos/#step-4-start-the-server","text":"sudo systemctl daemon-reload sudo systemctl start axon-server sudo systemctl status axon-server This will start the axon-server process as the axonops user, which was created during the package installation. The default listening address is 0.0.0.0:8080 .","title":"Step 4 - Start the server"},{"location":"installation/axon-server/centos/#package-details","text":"Configuration: /etc/axonops/axon-server.yml Binary: /usr/share/axonops/axon-server Logs: /var/log/axonops/axon-server.log Systemd service: /usr/lib/systemd/system/axon-server.service Copyright : /usr/share/doc/axonops/axon-server/copyright Licenses : /usr/share/axonops/licenses/axon-server/","title":"Package details"},{"location":"installation/axon-server/centos/#step-5-installing-axon-dash","text":"Now axon-server is installed, you can start installing the GUI for it: axon-dash","title":"Step 5 - Installing axon-dash"},{"location":"installation/axon-server/elastic/","text":"Increase the bulk queue size of Elasticsearch by running the following command: sudo echo 'thread_pool.write.queue_size: 2000' >> /etc/elasticsearch/elasticsearch.yml Increase the default heap size of elasticsearch by editing /etc/elasticsearch/jvm.options . From: -Xms1g -Xmx1g To: -Xms8g -Xmx8g This will set the minimum and maximum heap size to 8 GB. Set Xmx and Xms to no more than 50% of your physical RAM. Elasticsearch requires memory for purposes other than the JVM heap and it is important to leave space for this. Set the following index codec by running the following command: sudo echo 'index.codec: best_compression' >> /etc/elasticsearch/elasticsearch.yml Elasticsearch uses an mmapfs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions. You can increase the limits by running the following command: sudo sysctl -w vm.max_map_count = 262144 To make this change persist across reboots run this command: echo \"vm.max_map_count = 262144\" | sudo tee /etc/sysctl.d/10-elasticsearch.conf >dev/null Also, Elasticsearch needs max file descriptors system settings at least to 65536. echo 'elasticsearch - nofile 65536' | sudo tee --append /etc/security/limits.conf > /dev/null Start Elasticsearch \u00b6 sudo systemctl start elasticsearch.service After a short period of time, you can verify that your Elasticsearch node is running by sending an HTTP request to port 9200 on localhost: curl \"localhost:9200\"","title":"Elastic"},{"location":"installation/axon-server/elastic/#start-elasticsearch","text":"sudo systemctl start elasticsearch.service After a short period of time, you can verify that your Elasticsearch node is running by sending an HTTP request to port 9200 on localhost: curl \"localhost:9200\"","title":"Start Elasticsearch"},{"location":"installation/axon-server/install/","text":"Step 3 - axon-server configurations \u00b6 Make sure elastic_host and elastic_port are corresponding to your Elasticsearch instance. /etc/axonops/axon-server.yml host : 0.0.0.0 # axon-server listening address (used by axon-dash and axon-agent) (env variable: AXONSERVER_HOST) api_port : 8080 # axon-server HTTP API listening port (used by axon-dash) (AXONSERVER_PORT) agents_port : 1888 # axon-server listening port for agent connections elastic_hosts : # Elasticsearch endpoint (env variable:ELASTIC_HOSTS, comma separated list) - http://localhost #integrations_proxy: # proxy endpoint for integrations. (INTEGRATIONS_PROXY) # For better performance on large clusters, you can use a CQL store for the metrics. # To opt-in for CQL metrics storage, just specify at least one CQL host. # We do recommend to specify a NetworkTopologyStrategy for cql_keyspace_replication #cql_hosts: # (CQL_HOSTS, comma separated list) # - 192.168.0.10:9042 # - 192.168.0.11:9042 #cql_username: \"cassandra\" # (CQL_USERNAME) #cql_password: \"cassandra\" # (CQL_PASSWORD) #cql_local_dc: datacenter1 # (CQL_LOCAL_DC) #cql_ssl: false # (CQL_SSL) #cql_skip_verify: false # (CQL_SSL_SKIP_VERIFY) #cql_ca_file: /path/to/ca_file # (CQL_CA_FILE) #cql_cert_file: /path/to/cert_file # (CQL_CERT_FILE) #cql_key_file: /path/to/key_file # (CQL_KEY_FILE) #cql_proto_version: 4 # (CQL_PROTO_VERSION) #cql_max_concurrent_reads: 1000 # (CQL_MAX_CONCURRENT_READS) #cql_batch_size: 1 # (CQL_BATCH_SIZE) #cql_page_size: 10 # (CQL_PAGE_SIZE) #cql_autocreate_tables: true # (CQL_AUTO_CREATE_TABLES) this will tell axon-server to automatically create the metrics tables (true is recommended) #cql_keyspace_replication: \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\" # (CQL_KS_REPLICATION) keyspace replication for the metrics tables #cql_retrypolicy_numretries: 3 # (CQL_RETRY_POLICY_NUM_RETRIES) #cql_retrypolicy_min: 1s # (CQL_RETRY_POLICY_MIN) #cql_retrypolicy_max: 10s # (CQL_RETRY_POLICY_MAX) #cql_reconnectionpolicy_maxretries: 10 # (CQL_RECONNECTION_POLICY_MAX_RETRIES) #cql_reconnectionpolicy_initialinterval: 1s # (CQL_RECONNECTION_POLICY_INITIAL_INTERVAL) #cql_reconnectionpolicy_maxinterval: 10s # (CQL_RECONNECTION_POLICY_MAX_INTERVAL) #cql_metrics_cache_max_size_mb: 100 #MB # (CQL_METRICS_CACHE_MAX_SIZE_MB) #cql_read_consistency: \"LOCAL_ONE\" # (CQL_READ_CONSISTENCY) #One of the following: ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE #cql_write_consistency: \"LOCAL_ONE\" # (CQL_WRITE_CONSISTENCY) #One of the following: ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE #cql_lvl1_compaction_window_size: 12 # (CQL_LVL1_COMPACTION_WINDOW_SIZE) #cql_lvl2_compaction_window_size: 1 # (CQL_LVL2_COMPACTION_WINDOW_SIZE) #cql_lvl3_compaction_window_size: 1 # (CQL_LVL3_COMPACTION_WINDOW_SIZE) #cql_lvl4_compaction_window_size: 10 # (CQL_LVL4_COMPACTION_WINDOW_SIZE) #cql_lvl5_compaction_window_size: 120 # (CQL_LVL5_COMPACTION_WINDOW_SIZE) axon-dash : # This must point to the axon-dash address accessible from axon-server host : 127.0.0.1 port : 3000 https : false alerting : # How long to wait before sending a notification again if it has already # been sent successfully for an alert. (Usually ~3h or more). notification_interval : 3h # Default retention settings, most can be overridden from the frontend retention : events : 8w # logs and events retention. Must be expressed in weeks (w) metrics : high_resolution : 14d # High frequency metrics. Must be expressed in days (d) med_resolution : 12w # Must be expressed in weeks (w) low_resolution : 12M # Must be expressed in months (M) super_low_resolution : 2y # Must be expressed in years (y) backups : # Those are use as defaults but can be overridden from the UI local : 10d remote : 30d # Storage options for PDF reports # Override the default local path of /var/lib/axonops/reports #report_storage_path: /my/reports/storage/directory # Alternatively store PDF reports in an object store by providing report_storage_config #report_storage_path: my-reports-s3-bucket/reports-folder #report_storage_config: # type: s3 # provider: AWS # access_key_id: MY_ACCESS_KEY_ID # secret_access_key: MY_SECRET_ACCESS_KEY # region: us-east-1 # acl: private # server_side_encryption: AES256 # storage_class: STANDARD For better performances on large clusters (100+ nodes), you can use a CQL store for the metrics such as Cassandra . To opt-in for CQL metrics storage, specify at least one CQL host with axon-server configuration. Step 4 - Start the server \u00b6 sudo systemctl daemon-reload sudo systemctl start axon-server sudo systemctl status axon-server This will start the axon-server process as the axonops user, which was created during the package installation. The default listening address is 0.0.0.0:8080 . Package details \u00b6 Configuration: /etc/axonops/axon-server.yml Binary: /usr/share/axonops/axon-server Logs: /var/log/axonops/axon-server.log Systemd service: /usr/lib/systemd/system/axon-server.service Copyright : /usr/share/doc/axonops/axon-server/copyright Licenses : /usr/share/axonops/licenses/axon-server/ Step 5 - Installing axon-dash \u00b6 Now axon-server is installed, you can start installing the GUI for it: axon-dash","title":"Install"},{"location":"installation/axon-server/install/#step-3-axon-server-configurations","text":"Make sure elastic_host and elastic_port are corresponding to your Elasticsearch instance. /etc/axonops/axon-server.yml host : 0.0.0.0 # axon-server listening address (used by axon-dash and axon-agent) (env variable: AXONSERVER_HOST) api_port : 8080 # axon-server HTTP API listening port (used by axon-dash) (AXONSERVER_PORT) agents_port : 1888 # axon-server listening port for agent connections elastic_hosts : # Elasticsearch endpoint (env variable:ELASTIC_HOSTS, comma separated list) - http://localhost #integrations_proxy: # proxy endpoint for integrations. (INTEGRATIONS_PROXY) # For better performance on large clusters, you can use a CQL store for the metrics. # To opt-in for CQL metrics storage, just specify at least one CQL host. # We do recommend to specify a NetworkTopologyStrategy for cql_keyspace_replication #cql_hosts: # (CQL_HOSTS, comma separated list) # - 192.168.0.10:9042 # - 192.168.0.11:9042 #cql_username: \"cassandra\" # (CQL_USERNAME) #cql_password: \"cassandra\" # (CQL_PASSWORD) #cql_local_dc: datacenter1 # (CQL_LOCAL_DC) #cql_ssl: false # (CQL_SSL) #cql_skip_verify: false # (CQL_SSL_SKIP_VERIFY) #cql_ca_file: /path/to/ca_file # (CQL_CA_FILE) #cql_cert_file: /path/to/cert_file # (CQL_CERT_FILE) #cql_key_file: /path/to/key_file # (CQL_KEY_FILE) #cql_proto_version: 4 # (CQL_PROTO_VERSION) #cql_max_concurrent_reads: 1000 # (CQL_MAX_CONCURRENT_READS) #cql_batch_size: 1 # (CQL_BATCH_SIZE) #cql_page_size: 10 # (CQL_PAGE_SIZE) #cql_autocreate_tables: true # (CQL_AUTO_CREATE_TABLES) this will tell axon-server to automatically create the metrics tables (true is recommended) #cql_keyspace_replication: \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\" # (CQL_KS_REPLICATION) keyspace replication for the metrics tables #cql_retrypolicy_numretries: 3 # (CQL_RETRY_POLICY_NUM_RETRIES) #cql_retrypolicy_min: 1s # (CQL_RETRY_POLICY_MIN) #cql_retrypolicy_max: 10s # (CQL_RETRY_POLICY_MAX) #cql_reconnectionpolicy_maxretries: 10 # (CQL_RECONNECTION_POLICY_MAX_RETRIES) #cql_reconnectionpolicy_initialinterval: 1s # (CQL_RECONNECTION_POLICY_INITIAL_INTERVAL) #cql_reconnectionpolicy_maxinterval: 10s # (CQL_RECONNECTION_POLICY_MAX_INTERVAL) #cql_metrics_cache_max_size_mb: 100 #MB # (CQL_METRICS_CACHE_MAX_SIZE_MB) #cql_read_consistency: \"LOCAL_ONE\" # (CQL_READ_CONSISTENCY) #One of the following: ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE #cql_write_consistency: \"LOCAL_ONE\" # (CQL_WRITE_CONSISTENCY) #One of the following: ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE #cql_lvl1_compaction_window_size: 12 # (CQL_LVL1_COMPACTION_WINDOW_SIZE) #cql_lvl2_compaction_window_size: 1 # (CQL_LVL2_COMPACTION_WINDOW_SIZE) #cql_lvl3_compaction_window_size: 1 # (CQL_LVL3_COMPACTION_WINDOW_SIZE) #cql_lvl4_compaction_window_size: 10 # (CQL_LVL4_COMPACTION_WINDOW_SIZE) #cql_lvl5_compaction_window_size: 120 # (CQL_LVL5_COMPACTION_WINDOW_SIZE) axon-dash : # This must point to the axon-dash address accessible from axon-server host : 127.0.0.1 port : 3000 https : false alerting : # How long to wait before sending a notification again if it has already # been sent successfully for an alert. (Usually ~3h or more). notification_interval : 3h # Default retention settings, most can be overridden from the frontend retention : events : 8w # logs and events retention. Must be expressed in weeks (w) metrics : high_resolution : 14d # High frequency metrics. Must be expressed in days (d) med_resolution : 12w # Must be expressed in weeks (w) low_resolution : 12M # Must be expressed in months (M) super_low_resolution : 2y # Must be expressed in years (y) backups : # Those are use as defaults but can be overridden from the UI local : 10d remote : 30d # Storage options for PDF reports # Override the default local path of /var/lib/axonops/reports #report_storage_path: /my/reports/storage/directory # Alternatively store PDF reports in an object store by providing report_storage_config #report_storage_path: my-reports-s3-bucket/reports-folder #report_storage_config: # type: s3 # provider: AWS # access_key_id: MY_ACCESS_KEY_ID # secret_access_key: MY_SECRET_ACCESS_KEY # region: us-east-1 # acl: private # server_side_encryption: AES256 # storage_class: STANDARD For better performances on large clusters (100+ nodes), you can use a CQL store for the metrics such as Cassandra . To opt-in for CQL metrics storage, specify at least one CQL host with axon-server configuration.","title":"Step 3 - axon-server configurations"},{"location":"installation/axon-server/install/#step-4-start-the-server","text":"sudo systemctl daemon-reload sudo systemctl start axon-server sudo systemctl status axon-server This will start the axon-server process as the axonops user, which was created during the package installation. The default listening address is 0.0.0.0:8080 .","title":"Step 4 - Start the server"},{"location":"installation/axon-server/install/#package-details","text":"Configuration: /etc/axonops/axon-server.yml Binary: /usr/share/axonops/axon-server Logs: /var/log/axonops/axon-server.log Systemd service: /usr/lib/systemd/system/axon-server.service Copyright : /usr/share/doc/axonops/axon-server/copyright Licenses : /usr/share/axonops/licenses/axon-server/","title":"Package details"},{"location":"installation/axon-server/install/#step-5-installing-axon-dash","text":"Now axon-server is installed, you can start installing the GUI for it: axon-dash","title":"Step 5 - Installing axon-dash"},{"location":"installation/axon-server/metricsdatabase/","text":"axon-server metrics database \u00b6 Using Cassandra as a metrics store \u00b6 You can use Cassandra as a metrics store instead of Elasticsearch for better performances. To start using a CQL store you just have to specify CQL hosts in axon-server.yml : cql_hosts : - 192 .168.0.1:9042 - 192 .168.0.2:9042 ... By default, the AxonOps server automatically creates the necessary keyspace and tables. You can override this behavior by specifying the following field in axon-server.yml : cql_autocreate_tables : false cql_keyspace : \"axonops\" cql_keyspace_replication : \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\" We recommend setting up at least a 3 nodes cluster with NetworkTopologyStrategy and a replication_factor of 3 . Connecting to encrypted Cassandra metrics store \u00b6 We recommend setting up a Secured Socket Layer connection to Cassandra with the following fields in axon-server.yml : cql_ssl: true cql_skip_verify: false cql_ca_file: '/path/to/ca_cert' cql_cert_file: '/path/to/cert_file' cql_key_file: '/path/to/key_file' Metrics cache recommendations \u00b6 When using Cassandra as a metrics store the AxonOps server can cache metrics data in memory to further improve performance. This is configured using the cql_metrics_cache_max_items and cql_metrics_cache_max_size_mb options in axon-server.yml . The default values are shown here: cql_metrics_cache_max_items : 5000000 cql_metrics_cache_max_size_mb : 400 The recommended settings for these options are as follows: Cassandra Nodes cql_metrics_cache_max_items cql_metrics_cache_max_size_mb <10 5000000 1000 <50 5000000 4000 100 10000000 10000 200 10000000 20000 These sizes can be tuned to balance memory use in AxonOps against the read workload on Cassandra. When tuning these parameters it is recommended to set cql_metrics_cache_max_items to a high value and limit the cache size with cql_metrics_cache_max_size_mb . Other CQL fields \u00b6 You can also specify the following fields: cql_proto_version int cql_batch_size int cql_page_size int cql_local_dc string cql_username string cql_password string cql_max_concurrent_reads int cql_retrypolicy_numretries int cql_retrypolicy_min string \"1s\" cql_retrypolicy_max string \"10s\" cql_reconnectionpolicy_maxretries int cql_reconnectionpolicy_initialinterval string \"1s\" cql_reconnectionpolicy_maxinterval string \"10s\" cql_metrics_cache_max_size_mb int64 in MB cql_metrics_cache_max_items int64 in MB cql_read_consistency string ( controls the consistency of read operations, defaults to LOCAL_ONE ) cql_write_consistency string ( controls the consistency of write operations, defaults to LOCAL_ONE ) cql_lvl1_compaction_window_size int ( used for the table named 'metrics5' when you let axonserver managing the tables automatically ) cql_lvl2_compaction_window_size int ( used for the table named 'metrics60' when you let axonserver managing the tables automatically ) cql_lvl3_compaction_window_size int ( used for the table named 'metrics720' when you let axonserver managing the tables automatically ) cql_lvl4_compaction_window_size int ( used for the table named 'metrics7200' when you let axonserver managing the tables automatically ) cql_lvl5_compaction_window_size int ( used for the table named 'metrics86400' when you let axonserver managing the tables automatically ) The CQL for the default tables are the following: CREATE TABLE IF NOT EXISTS axonops.metrics5 ( orgid text, metricid int, time int, value float, PRIMARY KEY (( orgid, metricid ) , time ) ) WITH CLUSTERING ORDER BY ( time DESC ) AND caching = { 'keys' : 'ALL' , 'rows_per_partition' : '256' } AND compaction = { 'class' : 'TimeWindowCompactionStrategy' , 'compaction_window_size' : '1' , 'compaction_window_unit' : 'DAYS' , 'max_threshold' : '32' , 'min_threshold' : '4' } AND default_time_to_live = 604800 AND comment = '7 days retention for 5 seconds resolution metrics' ; CREATE TABLE IF NOT EXISTS axonops.metrics60 ( orgid text, metricid int, time int, value float, PRIMARY KEY (( orgid, metricid ) , time ) ) WITH CLUSTERING ORDER BY ( time DESC ) AND caching = { 'keys' : 'ALL' , 'rows_per_partition' : '256' } AND compaction = { 'class' : 'TimeWindowCompactionStrategy' , 'compaction_window_size' : '1' , 'compaction_window_unit' : 'DAYS' , 'max_threshold' : '32' , 'min_threshold' : '4' } AND default_time_to_live = 2592000 AND comment = '30 days retention for 60 seconds resolution metrics' ; CREATE TABLE IF NOT EXISTS axonops.metrics720 ( orgid text, metricid int, time int, value float, PRIMARY KEY (( orgid, metricid ) , time ) ) WITH CLUSTERING ORDER BY ( time DESC ) AND caching = { 'keys' : 'ALL' , 'rows_per_partition' : '256' } AND compaction = { 'class' : 'TimeWindowCompactionStrategy' , 'compaction_window_size' : '4' , 'compaction_window_unit' : 'DAYS' , 'max_threshold' : '32' , 'min_threshold' : '4' } AND default_time_to_live = 5184000 AND comment = '60 days retention for 720 seconds resolution metrics' ; CREATE TABLE IF NOT EXISTS axonops.metrics7200 ( orgid text, metricid int, time int, value float, PRIMARY KEY (( orgid, metricid ) , time ) ) WITH CLUSTERING ORDER BY ( time DESC ) AND caching = { 'keys' : 'ALL' , 'rows_per_partition' : '256' } AND compaction = { 'class' : 'TimeWindowCompactionStrategy' , 'compaction_window_size' : '30' , 'compaction_window_unit' : 'DAYS' , 'max_threshold' : '32' , 'min_threshold' : '4' } AND default_time_to_live = 15552000 AND comment = '180 days retention for 7200 seconds resolution metrics' ; CREATE TABLE IF NOT EXISTS axonops.metrics86400 ( orgid text, metricid int, time int, value float, PRIMARY KEY (( orgid, metricid ) , time ) ) WITH CLUSTERING ORDER BY ( time DESC ) AND caching = { 'keys' : 'ALL' , 'rows_per_partition' : '365' } AND compaction = { 'class' : 'TimeWindowCompactionStrategy' , 'compaction_window_size' : '60' , 'compaction_window_unit' : 'DAYS' , 'max_threshold' : '32' , 'min_threshold' : '4' } AND default_time_to_live = 31536000 AND comment = '365 days retention for 86400 seconds resolution metrics' ;","title":"Metrics database"},{"location":"installation/axon-server/metricsdatabase/#axon-server-metrics-database","text":"","title":"axon-server metrics database"},{"location":"installation/axon-server/metricsdatabase/#using-cassandra-as-a-metrics-store","text":"You can use Cassandra as a metrics store instead of Elasticsearch for better performances. To start using a CQL store you just have to specify CQL hosts in axon-server.yml : cql_hosts : - 192 .168.0.1:9042 - 192 .168.0.2:9042 ... By default, the AxonOps server automatically creates the necessary keyspace and tables. You can override this behavior by specifying the following field in axon-server.yml : cql_autocreate_tables : false cql_keyspace : \"axonops\" cql_keyspace_replication : \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\" We recommend setting up at least a 3 nodes cluster with NetworkTopologyStrategy and a replication_factor of 3 .","title":"Using Cassandra as a metrics store"},{"location":"installation/axon-server/metricsdatabase/#connecting-to-encrypted-cassandra-metrics-store","text":"We recommend setting up a Secured Socket Layer connection to Cassandra with the following fields in axon-server.yml : cql_ssl: true cql_skip_verify: false cql_ca_file: '/path/to/ca_cert' cql_cert_file: '/path/to/cert_file' cql_key_file: '/path/to/key_file'","title":"Connecting to encrypted Cassandra metrics store"},{"location":"installation/axon-server/metricsdatabase/#metrics-cache-recommendations","text":"When using Cassandra as a metrics store the AxonOps server can cache metrics data in memory to further improve performance. This is configured using the cql_metrics_cache_max_items and cql_metrics_cache_max_size_mb options in axon-server.yml . The default values are shown here: cql_metrics_cache_max_items : 5000000 cql_metrics_cache_max_size_mb : 400 The recommended settings for these options are as follows: Cassandra Nodes cql_metrics_cache_max_items cql_metrics_cache_max_size_mb <10 5000000 1000 <50 5000000 4000 100 10000000 10000 200 10000000 20000 These sizes can be tuned to balance memory use in AxonOps against the read workload on Cassandra. When tuning these parameters it is recommended to set cql_metrics_cache_max_items to a high value and limit the cache size with cql_metrics_cache_max_size_mb .","title":"Metrics cache recommendations"},{"location":"installation/axon-server/metricsdatabase/#other-cql-fields","text":"You can also specify the following fields: cql_proto_version int cql_batch_size int cql_page_size int cql_local_dc string cql_username string cql_password string cql_max_concurrent_reads int cql_retrypolicy_numretries int cql_retrypolicy_min string \"1s\" cql_retrypolicy_max string \"10s\" cql_reconnectionpolicy_maxretries int cql_reconnectionpolicy_initialinterval string \"1s\" cql_reconnectionpolicy_maxinterval string \"10s\" cql_metrics_cache_max_size_mb int64 in MB cql_metrics_cache_max_items int64 in MB cql_read_consistency string ( controls the consistency of read operations, defaults to LOCAL_ONE ) cql_write_consistency string ( controls the consistency of write operations, defaults to LOCAL_ONE ) cql_lvl1_compaction_window_size int ( used for the table named 'metrics5' when you let axonserver managing the tables automatically ) cql_lvl2_compaction_window_size int ( used for the table named 'metrics60' when you let axonserver managing the tables automatically ) cql_lvl3_compaction_window_size int ( used for the table named 'metrics720' when you let axonserver managing the tables automatically ) cql_lvl4_compaction_window_size int ( used for the table named 'metrics7200' when you let axonserver managing the tables automatically ) cql_lvl5_compaction_window_size int ( used for the table named 'metrics86400' when you let axonserver managing the tables automatically ) The CQL for the default tables are the following: CREATE TABLE IF NOT EXISTS axonops.metrics5 ( orgid text, metricid int, time int, value float, PRIMARY KEY (( orgid, metricid ) , time ) ) WITH CLUSTERING ORDER BY ( time DESC ) AND caching = { 'keys' : 'ALL' , 'rows_per_partition' : '256' } AND compaction = { 'class' : 'TimeWindowCompactionStrategy' , 'compaction_window_size' : '1' , 'compaction_window_unit' : 'DAYS' , 'max_threshold' : '32' , 'min_threshold' : '4' } AND default_time_to_live = 604800 AND comment = '7 days retention for 5 seconds resolution metrics' ; CREATE TABLE IF NOT EXISTS axonops.metrics60 ( orgid text, metricid int, time int, value float, PRIMARY KEY (( orgid, metricid ) , time ) ) WITH CLUSTERING ORDER BY ( time DESC ) AND caching = { 'keys' : 'ALL' , 'rows_per_partition' : '256' } AND compaction = { 'class' : 'TimeWindowCompactionStrategy' , 'compaction_window_size' : '1' , 'compaction_window_unit' : 'DAYS' , 'max_threshold' : '32' , 'min_threshold' : '4' } AND default_time_to_live = 2592000 AND comment = '30 days retention for 60 seconds resolution metrics' ; CREATE TABLE IF NOT EXISTS axonops.metrics720 ( orgid text, metricid int, time int, value float, PRIMARY KEY (( orgid, metricid ) , time ) ) WITH CLUSTERING ORDER BY ( time DESC ) AND caching = { 'keys' : 'ALL' , 'rows_per_partition' : '256' } AND compaction = { 'class' : 'TimeWindowCompactionStrategy' , 'compaction_window_size' : '4' , 'compaction_window_unit' : 'DAYS' , 'max_threshold' : '32' , 'min_threshold' : '4' } AND default_time_to_live = 5184000 AND comment = '60 days retention for 720 seconds resolution metrics' ; CREATE TABLE IF NOT EXISTS axonops.metrics7200 ( orgid text, metricid int, time int, value float, PRIMARY KEY (( orgid, metricid ) , time ) ) WITH CLUSTERING ORDER BY ( time DESC ) AND caching = { 'keys' : 'ALL' , 'rows_per_partition' : '256' } AND compaction = { 'class' : 'TimeWindowCompactionStrategy' , 'compaction_window_size' : '30' , 'compaction_window_unit' : 'DAYS' , 'max_threshold' : '32' , 'min_threshold' : '4' } AND default_time_to_live = 15552000 AND comment = '180 days retention for 7200 seconds resolution metrics' ; CREATE TABLE IF NOT EXISTS axonops.metrics86400 ( orgid text, metricid int, time int, value float, PRIMARY KEY (( orgid, metricid ) , time ) ) WITH CLUSTERING ORDER BY ( time DESC ) AND caching = { 'keys' : 'ALL' , 'rows_per_partition' : '365' } AND compaction = { 'class' : 'TimeWindowCompactionStrategy' , 'compaction_window_size' : '60' , 'compaction_window_unit' : 'DAYS' , 'max_threshold' : '32' , 'min_threshold' : '4' } AND default_time_to_live = 31536000 AND comment = '365 days retention for 86400 seconds resolution metrics' ;","title":"Other CQL fields"},{"location":"installation/axon-server/ubuntu/","text":"axon-server installation (Debian / Ubuntu) \u00b6 Step 1 - Prerequisites \u00b6 Elasticsearch stores the data collected by axon-server. AxonOps is currently only compatible with Elasticsearch 7.x, we recommend installing the latest available 7.x release. Installing Elasticsearch \u00b6 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.16-amd64.deb wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.16-amd64.deb.sha512 shasum -a 512 -c elasticsearch-7.17.16-amd64.deb.sha512 sudo dpkg -i elasticsearch-7.17.16-amd64.deb The shasum command above verifies the downloaded package and should show this output: elasticsearch-7.17.16-amd64.deb: OK Increase the bulk queue size of Elasticsearch by running the following command: sudo echo 'thread_pool.write.queue_size: 2000' >> /etc/elasticsearch/elasticsearch.yml Increase the default heap size of elasticsearch by editing /etc/elasticsearch/jvm.options . From: -Xms1g -Xmx1g To: -Xms8g -Xmx8g This will set the minimum and maximum heap size to 8 GB. Set Xmx and Xms to no more than 50% of your physical RAM. Elasticsearch requires memory for purposes other than the JVM heap and it is important to leave space for this. Set the following index codec by running the following command: sudo echo 'index.codec: best_compression' >> /etc/elasticsearch/elasticsearch.yml Elasticsearch uses an mmapfs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions. You can increase the limits by running the following command: sudo sysctl -w vm.max_map_count = 262144 To make this change persist across reboots run this command: echo \"vm.max_map_count = 262144\" | sudo tee /etc/sysctl.d/10-elasticsearch.conf >dev/null Also, Elasticsearch needs max file descriptors system settings at least to 65536. echo 'elasticsearch - nofile 65536' | sudo tee --append /etc/security/limits.conf > /dev/null Start Elasticsearch \u00b6 sudo systemctl start elasticsearch.service After a short period of time, you can verify that your Elasticsearch node is running by sending an HTTP request to port 9200 on localhost: curl \"localhost:9200\" Step 2 - axon-server \u00b6 sudo apt-get update sudo apt-get install curl gnupg ca-certificates curl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add - echo \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-server For new versions of Debian (>= bookworm) and Ubuntu (>= 22.04) the process of setting up the apt repository has changed. See below: sudo apt-get update sudo apt-get install -y curl gnupg ca-certificates curl -L https://packages.axonops.com/apt/repo-signing-key.gpg | gpg --dearmor -o /usr/share/keyrings/axonops.gpg echo \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-server Step 3 - axon-server configurations \u00b6 Make sure elastic_host and elastic_port are corresponding to your Elasticsearch instance. /etc/axonops/axon-server.yml host : 0.0.0.0 # axon-server listening address (used by axon-dash and axon-agent) (env variable: AXONSERVER_HOST) api_port : 8080 # axon-server HTTP API listening port (used by axon-dash) (AXONSERVER_PORT) agents_port : 1888 # axon-server listening port for agent connections elastic_hosts : # Elasticsearch endpoint (env variable:ELASTIC_HOSTS, comma separated list) - http://localhost #integrations_proxy: # proxy endpoint for integrations. (INTEGRATIONS_PROXY) # For better performance on large clusters, you can use a CQL store for the metrics. # To opt-in for CQL metrics storage, just specify at least one CQL host. # We do recommend to specify a NetworkTopologyStrategy for cql_keyspace_replication #cql_hosts: # (CQL_HOSTS, comma separated list) # - 192.168.0.10:9042 # - 192.168.0.11:9042 #cql_username: \"cassandra\" # (CQL_USERNAME) #cql_password: \"cassandra\" # (CQL_PASSWORD) #cql_local_dc: datacenter1 # (CQL_LOCAL_DC) #cql_ssl: false # (CQL_SSL) #cql_skip_verify: false # (CQL_SSL_SKIP_VERIFY) #cql_ca_file: /path/to/ca_file # (CQL_CA_FILE) #cql_cert_file: /path/to/cert_file # (CQL_CERT_FILE) #cql_key_file: /path/to/key_file # (CQL_KEY_FILE) #cql_proto_version: 4 # (CQL_PROTO_VERSION) #cql_max_concurrent_reads: 1000 # (CQL_MAX_CONCURRENT_READS) #cql_batch_size: 1 # (CQL_BATCH_SIZE) #cql_page_size: 10 # (CQL_PAGE_SIZE) #cql_autocreate_tables: true # (CQL_AUTO_CREATE_TABLES) this will tell axon-server to automatically create the metrics tables (true is recommended) #cql_keyspace_replication: \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\" # (CQL_KS_REPLICATION) keyspace replication for the metrics tables #cql_retrypolicy_numretries: 3 # (CQL_RETRY_POLICY_NUM_RETRIES) #cql_retrypolicy_min: 1s # (CQL_RETRY_POLICY_MIN) #cql_retrypolicy_max: 10s # (CQL_RETRY_POLICY_MAX) #cql_reconnectionpolicy_maxretries: 10 # (CQL_RECONNECTION_POLICY_MAX_RETRIES) #cql_reconnectionpolicy_initialinterval: 1s # (CQL_RECONNECTION_POLICY_INITIAL_INTERVAL) #cql_reconnectionpolicy_maxinterval: 10s # (CQL_RECONNECTION_POLICY_MAX_INTERVAL) #cql_metrics_cache_max_size_mb: 100 #MB # (CQL_METRICS_CACHE_MAX_SIZE_MB) #cql_read_consistency: \"LOCAL_ONE\" # (CQL_READ_CONSISTENCY) #One of the following: ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE #cql_write_consistency: \"LOCAL_ONE\" # (CQL_WRITE_CONSISTENCY) #One of the following: ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE #cql_lvl1_compaction_window_size: 12 # (CQL_LVL1_COMPACTION_WINDOW_SIZE) #cql_lvl2_compaction_window_size: 1 # (CQL_LVL2_COMPACTION_WINDOW_SIZE) #cql_lvl3_compaction_window_size: 1 # (CQL_LVL3_COMPACTION_WINDOW_SIZE) #cql_lvl4_compaction_window_size: 10 # (CQL_LVL4_COMPACTION_WINDOW_SIZE) #cql_lvl5_compaction_window_size: 120 # (CQL_LVL5_COMPACTION_WINDOW_SIZE) axon-dash : # This must point to the axon-dash address accessible from axon-server host : 127.0.0.1 port : 3000 https : false alerting : # How long to wait before sending a notification again if it has already # been sent successfully for an alert. (Usually ~3h or more). notification_interval : 3h # Default retention settings, most can be overridden from the frontend retention : events : 8w # logs and events retention. Must be expressed in weeks (w) metrics : high_resolution : 14d # High frequency metrics. Must be expressed in days (d) med_resolution : 12w # Must be expressed in weeks (w) low_resolution : 12M # Must be expressed in months (M) super_low_resolution : 2y # Must be expressed in years (y) backups : # Those are use as defaults but can be overridden from the UI local : 10d remote : 30d # Storage options for PDF reports # Override the default local path of /var/lib/axonops/reports #report_storage_path: /my/reports/storage/directory # Alternatively store PDF reports in an object store by providing report_storage_config #report_storage_path: my-reports-s3-bucket/reports-folder #report_storage_config: # type: s3 # provider: AWS # access_key_id: MY_ACCESS_KEY_ID # secret_access_key: MY_SECRET_ACCESS_KEY # region: us-east-1 # acl: private # server_side_encryption: AES256 # storage_class: STANDARD For better performances on large clusters (100+ nodes), you can use a CQL store for the metrics such as Cassandra . To opt-in for CQL metrics storage, specify at least one CQL host with axon-server configuration. Step 4 - Start the server \u00b6 sudo systemctl daemon-reload sudo systemctl start axon-server sudo systemctl status axon-server This will start the axon-server process as the axonops user, which was created during the package installation. The default listening address is 0.0.0.0:8080 . Package details \u00b6 Configuration: /etc/axonops/axon-server.yml Binary: /usr/share/axonops/axon-server Logs: /var/log/axonops/axon-server.log Systemd service: /usr/lib/systemd/system/axon-server.service Copyright : /usr/share/doc/axonops/axon-server/copyright Licenses : /usr/share/axonops/licenses/axon-server/ Step 5 - Installing axon-dash \u00b6 Now axon-server is installed, you can start installing the GUI for it: axon-dash","title":"Installing on Ubuntu/Debian"},{"location":"installation/axon-server/ubuntu/#axon-server-installation-debian-ubuntu","text":"","title":"axon-server installation (Debian / Ubuntu)"},{"location":"installation/axon-server/ubuntu/#step-1-prerequisites","text":"Elasticsearch stores the data collected by axon-server. AxonOps is currently only compatible with Elasticsearch 7.x, we recommend installing the latest available 7.x release.","title":"Step 1 - Prerequisites"},{"location":"installation/axon-server/ubuntu/#installing-elasticsearch","text":"wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.16-amd64.deb wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.16-amd64.deb.sha512 shasum -a 512 -c elasticsearch-7.17.16-amd64.deb.sha512 sudo dpkg -i elasticsearch-7.17.16-amd64.deb The shasum command above verifies the downloaded package and should show this output: elasticsearch-7.17.16-amd64.deb: OK Increase the bulk queue size of Elasticsearch by running the following command: sudo echo 'thread_pool.write.queue_size: 2000' >> /etc/elasticsearch/elasticsearch.yml Increase the default heap size of elasticsearch by editing /etc/elasticsearch/jvm.options . From: -Xms1g -Xmx1g To: -Xms8g -Xmx8g This will set the minimum and maximum heap size to 8 GB. Set Xmx and Xms to no more than 50% of your physical RAM. Elasticsearch requires memory for purposes other than the JVM heap and it is important to leave space for this. Set the following index codec by running the following command: sudo echo 'index.codec: best_compression' >> /etc/elasticsearch/elasticsearch.yml Elasticsearch uses an mmapfs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions. You can increase the limits by running the following command: sudo sysctl -w vm.max_map_count = 262144 To make this change persist across reboots run this command: echo \"vm.max_map_count = 262144\" | sudo tee /etc/sysctl.d/10-elasticsearch.conf >dev/null Also, Elasticsearch needs max file descriptors system settings at least to 65536. echo 'elasticsearch - nofile 65536' | sudo tee --append /etc/security/limits.conf > /dev/null","title":"Installing Elasticsearch"},{"location":"installation/axon-server/ubuntu/#start-elasticsearch","text":"sudo systemctl start elasticsearch.service After a short period of time, you can verify that your Elasticsearch node is running by sending an HTTP request to port 9200 on localhost: curl \"localhost:9200\"","title":"Start Elasticsearch"},{"location":"installation/axon-server/ubuntu/#step-2-axon-server","text":"sudo apt-get update sudo apt-get install curl gnupg ca-certificates curl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add - echo \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-server For new versions of Debian (>= bookworm) and Ubuntu (>= 22.04) the process of setting up the apt repository has changed. See below: sudo apt-get update sudo apt-get install -y curl gnupg ca-certificates curl -L https://packages.axonops.com/apt/repo-signing-key.gpg | gpg --dearmor -o /usr/share/keyrings/axonops.gpg echo \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-server","title":"Step 2 - axon-server"},{"location":"installation/axon-server/ubuntu/#step-3-axon-server-configurations","text":"Make sure elastic_host and elastic_port are corresponding to your Elasticsearch instance. /etc/axonops/axon-server.yml host : 0.0.0.0 # axon-server listening address (used by axon-dash and axon-agent) (env variable: AXONSERVER_HOST) api_port : 8080 # axon-server HTTP API listening port (used by axon-dash) (AXONSERVER_PORT) agents_port : 1888 # axon-server listening port for agent connections elastic_hosts : # Elasticsearch endpoint (env variable:ELASTIC_HOSTS, comma separated list) - http://localhost #integrations_proxy: # proxy endpoint for integrations. (INTEGRATIONS_PROXY) # For better performance on large clusters, you can use a CQL store for the metrics. # To opt-in for CQL metrics storage, just specify at least one CQL host. # We do recommend to specify a NetworkTopologyStrategy for cql_keyspace_replication #cql_hosts: # (CQL_HOSTS, comma separated list) # - 192.168.0.10:9042 # - 192.168.0.11:9042 #cql_username: \"cassandra\" # (CQL_USERNAME) #cql_password: \"cassandra\" # (CQL_PASSWORD) #cql_local_dc: datacenter1 # (CQL_LOCAL_DC) #cql_ssl: false # (CQL_SSL) #cql_skip_verify: false # (CQL_SSL_SKIP_VERIFY) #cql_ca_file: /path/to/ca_file # (CQL_CA_FILE) #cql_cert_file: /path/to/cert_file # (CQL_CERT_FILE) #cql_key_file: /path/to/key_file # (CQL_KEY_FILE) #cql_proto_version: 4 # (CQL_PROTO_VERSION) #cql_max_concurrent_reads: 1000 # (CQL_MAX_CONCURRENT_READS) #cql_batch_size: 1 # (CQL_BATCH_SIZE) #cql_page_size: 10 # (CQL_PAGE_SIZE) #cql_autocreate_tables: true # (CQL_AUTO_CREATE_TABLES) this will tell axon-server to automatically create the metrics tables (true is recommended) #cql_keyspace_replication: \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\" # (CQL_KS_REPLICATION) keyspace replication for the metrics tables #cql_retrypolicy_numretries: 3 # (CQL_RETRY_POLICY_NUM_RETRIES) #cql_retrypolicy_min: 1s # (CQL_RETRY_POLICY_MIN) #cql_retrypolicy_max: 10s # (CQL_RETRY_POLICY_MAX) #cql_reconnectionpolicy_maxretries: 10 # (CQL_RECONNECTION_POLICY_MAX_RETRIES) #cql_reconnectionpolicy_initialinterval: 1s # (CQL_RECONNECTION_POLICY_INITIAL_INTERVAL) #cql_reconnectionpolicy_maxinterval: 10s # (CQL_RECONNECTION_POLICY_MAX_INTERVAL) #cql_metrics_cache_max_size_mb: 100 #MB # (CQL_METRICS_CACHE_MAX_SIZE_MB) #cql_read_consistency: \"LOCAL_ONE\" # (CQL_READ_CONSISTENCY) #One of the following: ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE #cql_write_consistency: \"LOCAL_ONE\" # (CQL_WRITE_CONSISTENCY) #One of the following: ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE #cql_lvl1_compaction_window_size: 12 # (CQL_LVL1_COMPACTION_WINDOW_SIZE) #cql_lvl2_compaction_window_size: 1 # (CQL_LVL2_COMPACTION_WINDOW_SIZE) #cql_lvl3_compaction_window_size: 1 # (CQL_LVL3_COMPACTION_WINDOW_SIZE) #cql_lvl4_compaction_window_size: 10 # (CQL_LVL4_COMPACTION_WINDOW_SIZE) #cql_lvl5_compaction_window_size: 120 # (CQL_LVL5_COMPACTION_WINDOW_SIZE) axon-dash : # This must point to the axon-dash address accessible from axon-server host : 127.0.0.1 port : 3000 https : false alerting : # How long to wait before sending a notification again if it has already # been sent successfully for an alert. (Usually ~3h or more). notification_interval : 3h # Default retention settings, most can be overridden from the frontend retention : events : 8w # logs and events retention. Must be expressed in weeks (w) metrics : high_resolution : 14d # High frequency metrics. Must be expressed in days (d) med_resolution : 12w # Must be expressed in weeks (w) low_resolution : 12M # Must be expressed in months (M) super_low_resolution : 2y # Must be expressed in years (y) backups : # Those are use as defaults but can be overridden from the UI local : 10d remote : 30d # Storage options for PDF reports # Override the default local path of /var/lib/axonops/reports #report_storage_path: /my/reports/storage/directory # Alternatively store PDF reports in an object store by providing report_storage_config #report_storage_path: my-reports-s3-bucket/reports-folder #report_storage_config: # type: s3 # provider: AWS # access_key_id: MY_ACCESS_KEY_ID # secret_access_key: MY_SECRET_ACCESS_KEY # region: us-east-1 # acl: private # server_side_encryption: AES256 # storage_class: STANDARD For better performances on large clusters (100+ nodes), you can use a CQL store for the metrics such as Cassandra . To opt-in for CQL metrics storage, specify at least one CQL host with axon-server configuration.","title":"Step 3 - axon-server configurations"},{"location":"installation/axon-server/ubuntu/#step-4-start-the-server","text":"sudo systemctl daemon-reload sudo systemctl start axon-server sudo systemctl status axon-server This will start the axon-server process as the axonops user, which was created during the package installation. The default listening address is 0.0.0.0:8080 .","title":"Step 4 - Start the server"},{"location":"installation/axon-server/ubuntu/#package-details","text":"Configuration: /etc/axonops/axon-server.yml Binary: /usr/share/axonops/axon-server Logs: /var/log/axonops/axon-server.log Systemd service: /usr/lib/systemd/system/axon-server.service Copyright : /usr/share/doc/axonops/axon-server/copyright Licenses : /usr/share/axonops/licenses/axon-server/","title":"Package details"},{"location":"installation/axon-server/ubuntu/#step-5-installing-axon-dash","text":"Now axon-server is installed, you can start installing the GUI for it: axon-dash","title":"Step 5 - Installing axon-dash"},{"location":"installation/cassandra-agent/docker/","text":"Installing axon-agent for Cassandra in Docker \u00b6 Caveats Cassandra logs cannot normally be collected by AxonOps as they are sent to stdout and handled by the Docker logging driver If axon-agent is running under Docker it assumes that the Cassandra user's GID is 999 as it is in the official Cassandra images. If this is not the case then AxonOps may not be able to backup the Cassandra data. To enable the full functionality of the AxonOps agent some directories must be accessible to both the Cassandra and AxonOps Agent processes. Directory Required Description /var/lib/axonops Required Contains UNIX domain sockets, Cassandra agent jars and local data stored by the agent. This directory must be readable and writable by Cassandra and AxonOps /etc/axonops Required Contains the configuration for AxonOps. This directory must be readable by Cassandra and AxonOps /var/log/axonops Required The Cassandra agent will write logs to this directory where they will be buffered and sent to the AxonOps server This directory must be writable by Cassandra and readable by AxonOps /var/lib/cassandra Optional For the backups feature to function correctly the Cassandra data directory must be readable by the AxonOps agent When running Cassandra under Docker it is possible to run the AxonOps agent either on the host or in another Docker container. When installing on the host follow the instructions under AxonOps Cassandra agent installation to install the agent and ensure that the appropriate directories are mapped into the Cassandra container. Example with Docker Compose \u00b6 This example shows running a single Cassandra node and the AxonOps agent under Docker Compose using host volumes to share data between the containers. version : \"3\" services : cassandra : image : cassandra:4.0 container_name : cassandra restart : always volumes : - ./cassandra:/var/lib/cassandra - ./axonops/var:/var/lib/axonops - ./axonops/etc:/etc/axonops - ./axonops/log:/var/log/axonops ports : - \"9042:9042\" environment : - JVM_EXTRA_OPTS=-javaagent:/var/lib/axonops/axon-cassandra4.0-agent.jar=/etc/axonops/axon-agent.yml - CASSANDRA_CLUSTER_NAME=my-cluster axon-agent : image : registry.axonops.com/axonops-public/axonops-docker/axon-agent:latest restart : always environment : # Enter the hostname or IP address of your AxonOps server here - AXON_AGENT_SERVER_HOST=axonops-server.example.com volumes : - ./cassandra:/var/lib/cassandra - ./axonops/var:/var/lib/axonops - ./axonops/etc:/etc/axonops - ./axonops/log:/var/log/axonops","title":"Installing axon-agent for Cassandra in Docker"},{"location":"installation/cassandra-agent/docker/#installing-axon-agent-for-cassandra-in-docker","text":"Caveats Cassandra logs cannot normally be collected by AxonOps as they are sent to stdout and handled by the Docker logging driver If axon-agent is running under Docker it assumes that the Cassandra user's GID is 999 as it is in the official Cassandra images. If this is not the case then AxonOps may not be able to backup the Cassandra data. To enable the full functionality of the AxonOps agent some directories must be accessible to both the Cassandra and AxonOps Agent processes. Directory Required Description /var/lib/axonops Required Contains UNIX domain sockets, Cassandra agent jars and local data stored by the agent. This directory must be readable and writable by Cassandra and AxonOps /etc/axonops Required Contains the configuration for AxonOps. This directory must be readable by Cassandra and AxonOps /var/log/axonops Required The Cassandra agent will write logs to this directory where they will be buffered and sent to the AxonOps server This directory must be writable by Cassandra and readable by AxonOps /var/lib/cassandra Optional For the backups feature to function correctly the Cassandra data directory must be readable by the AxonOps agent When running Cassandra under Docker it is possible to run the AxonOps agent either on the host or in another Docker container. When installing on the host follow the instructions under AxonOps Cassandra agent installation to install the agent and ensure that the appropriate directories are mapped into the Cassandra container.","title":"Installing axon-agent for Cassandra in Docker"},{"location":"installation/cassandra-agent/docker/#example-with-docker-compose","text":"This example shows running a single Cassandra node and the AxonOps agent under Docker Compose using host volumes to share data between the containers. version : \"3\" services : cassandra : image : cassandra:4.0 container_name : cassandra restart : always volumes : - ./cassandra:/var/lib/cassandra - ./axonops/var:/var/lib/axonops - ./axonops/etc:/etc/axonops - ./axonops/log:/var/log/axonops ports : - \"9042:9042\" environment : - JVM_EXTRA_OPTS=-javaagent:/var/lib/axonops/axon-cassandra4.0-agent.jar=/etc/axonops/axon-agent.yml - CASSANDRA_CLUSTER_NAME=my-cluster axon-agent : image : registry.axonops.com/axonops-public/axonops-docker/axon-agent:latest restart : always environment : # Enter the hostname or IP address of your AxonOps server here - AXON_AGENT_SERVER_HOST=axonops-server.example.com volumes : - ./cassandra:/var/lib/cassandra - ./axonops/var:/var/lib/axonops - ./axonops/etc:/etc/axonops - ./axonops/log:/var/log/axonops","title":"Example with Docker Compose"},{"location":"installation/cassandra-agent/install/","text":"AxonOps Cassandra agent installation \u00b6 This agent will enable metrics, logs and events collection with adaptive repairs and backups for Cassandra. See Installing axon-agent for Cassandra in Docker if you are running Cassandra under Docker. Available versions \u00b6 Apache Cassandra 4.1.x Apache Cassandra 4.0.x Apache Cassandra 3.11.x Apache Cassandra 3.0.x Step 1 - Installation \u00b6 Make sure that the {version} of your Cassandra and Cassandra agent are compatible from the compatibility matrix . CentOS / RedHat \u00b6 sudo tee /etc/yum.repos.d/axonops-yum.repo << EOL [axonops-yum] name=axonops-yum baseurl=https://packages.axonops.com/yum/ enabled=1 repo_gpgcheck=0 gpgcheck=0 EOL sudo yum install axon-cassandra { version } -agent Debian / Ubuntu \u00b6 sudo apt-get update sudo apt-get install curl gnupg ca-certificates curl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add - echo \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-cassandra { version } -agent For new versions of Debian (>= bookworm) and Ubuntu (>= 22.04) the process of setting up the apt repository has changed. See below: sudo apt-get update sudo apt-get install -y curl gnupg ca-certificates curl -L https://packages.axonops.com/apt/repo-signing-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/axonops.gpg echo \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-cassandra { version } -agent Note: This will install the AxonOps Cassandra agent and its dependency: axon-agent Step 2 - Agent Configuration \u00b6 Update the following highlighted lines from /etc/axonops/axon-agent.yml : axon-server : hosts : \"axon-server_endpoint\" # Your axon-server IP or hostname, e.g. axonops.mycompany.com port : 1888 # The default axon-server port is 1888 axon-agent : org : \"my-company\" # Your organisation name NTP : host : \"ntp.mycompany.com\" # Your NTP server IP address or hostname Step 3 - Configure Cassandra \u00b6 Edit cassandra-env.sh , which is usually located in /<Cassandra Installation Directory>/conf/cassandra-env.sh for tarball installs or /etc/cassandra/cassandra-env.sh for package installs, and append the following line at the end of the file: JVM_OPTS = \" $JVM_OPTS -javaagent:/usr/share/axonops/axon-cassandra{version}-agent.jar=/etc/axonops/axon-agent.yml\" for example with Cassandra agent version 3.11 : JVM_OPTS = \" $JVM_OPTS -javaagent:/usr/share/axonops/axon-cassandra3.11-agent.jar=/etc/axonops/axon-agent.yml\" Make sure that this configuration will not get overridden by an automation tool. Step 4 - Add axonops user to Cassandra user group and Cassandra user to axonops group \u00b6 sudo usermod -aG <your_cassandra_group> axonops sudo usermod -aG axonops <your_cassandra_user> Step 5 - Start Cassandra \u00b6 Step 6 - Start axon-agent \u00b6 sudo systemctl start axon-agent (Optional) Step 7 - Cassandra Remote Backups or Restore Prerequisites \u00b6 If you plan to use AxonOps remote backup functionality, axonops user will require read access on Cassandra data folder. As well if you plan to Restore data with AxonOps, axonops user will require write access to Cassandra data folder. We recommend to only provide temporary write access to axonops when required. Cassandra agent Package details \u00b6 Configuration: /etc/axonops/axon-agent.yml Binary: /usr/share/axonops/axon-cassandra{version}-agent.jar Version number: /usr/share/axonops/axon-cassandra{version}-agent.version Copyright : /usr/share/doc/axonops/axon-cassandra{version}-agent/copyright Licenses : /usr/share/axonops/licenses/axon-cassandra{version}-agent/ axon-agent Package details (dependency of Cassandra agent) \u00b6 Configuration: /etc/axonops/axon-agent.yml Binary: usr/share/axonops/axon-agent Logs : /var/log/axonops/axon-agent.log Systemd service: /usr/lib/systemd/system/axon-agent.service","title":"AxonOps Agent"},{"location":"installation/cassandra-agent/install/#axonops-cassandra-agent-installation","text":"This agent will enable metrics, logs and events collection with adaptive repairs and backups for Cassandra. See Installing axon-agent for Cassandra in Docker if you are running Cassandra under Docker.","title":"AxonOps Cassandra agent installation"},{"location":"installation/cassandra-agent/install/#available-versions","text":"Apache Cassandra 4.1.x Apache Cassandra 4.0.x Apache Cassandra 3.11.x Apache Cassandra 3.0.x","title":"Available versions"},{"location":"installation/cassandra-agent/install/#step-1-installation","text":"Make sure that the {version} of your Cassandra and Cassandra agent are compatible from the compatibility matrix .","title":"Step 1 - Installation"},{"location":"installation/cassandra-agent/install/#centos-redhat","text":"sudo tee /etc/yum.repos.d/axonops-yum.repo << EOL [axonops-yum] name=axonops-yum baseurl=https://packages.axonops.com/yum/ enabled=1 repo_gpgcheck=0 gpgcheck=0 EOL sudo yum install axon-cassandra { version } -agent","title":"CentOS / RedHat"},{"location":"installation/cassandra-agent/install/#debian-ubuntu","text":"sudo apt-get update sudo apt-get install curl gnupg ca-certificates curl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add - echo \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-cassandra { version } -agent For new versions of Debian (>= bookworm) and Ubuntu (>= 22.04) the process of setting up the apt repository has changed. See below: sudo apt-get update sudo apt-get install -y curl gnupg ca-certificates curl -L https://packages.axonops.com/apt/repo-signing-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/axonops.gpg echo \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-cassandra { version } -agent Note: This will install the AxonOps Cassandra agent and its dependency: axon-agent","title":"Debian / Ubuntu"},{"location":"installation/cassandra-agent/install/#step-2-agent-configuration","text":"Update the following highlighted lines from /etc/axonops/axon-agent.yml : axon-server : hosts : \"axon-server_endpoint\" # Your axon-server IP or hostname, e.g. axonops.mycompany.com port : 1888 # The default axon-server port is 1888 axon-agent : org : \"my-company\" # Your organisation name NTP : host : \"ntp.mycompany.com\" # Your NTP server IP address or hostname","title":"Step 2 - Agent Configuration"},{"location":"installation/cassandra-agent/install/#step-3-configure-cassandra","text":"Edit cassandra-env.sh , which is usually located in /<Cassandra Installation Directory>/conf/cassandra-env.sh for tarball installs or /etc/cassandra/cassandra-env.sh for package installs, and append the following line at the end of the file: JVM_OPTS = \" $JVM_OPTS -javaagent:/usr/share/axonops/axon-cassandra{version}-agent.jar=/etc/axonops/axon-agent.yml\" for example with Cassandra agent version 3.11 : JVM_OPTS = \" $JVM_OPTS -javaagent:/usr/share/axonops/axon-cassandra3.11-agent.jar=/etc/axonops/axon-agent.yml\" Make sure that this configuration will not get overridden by an automation tool.","title":"Step 3 - Configure Cassandra"},{"location":"installation/cassandra-agent/install/#step-4-add-axonops-user-to-cassandra-user-group-and-cassandra-user-to-axonops-group","text":"sudo usermod -aG <your_cassandra_group> axonops sudo usermod -aG axonops <your_cassandra_user>","title":"Step 4 - Add axonops user to Cassandra user group and Cassandra user to axonops group"},{"location":"installation/cassandra-agent/install/#step-5-start-cassandra","text":"","title":"Step 5 - Start Cassandra"},{"location":"installation/cassandra-agent/install/#step-6-start-axon-agent","text":"sudo systemctl start axon-agent","title":"Step 6 - Start axon-agent"},{"location":"installation/cassandra-agent/install/#optional-step-7-cassandra-remote-backups-or-restore-prerequisites","text":"If you plan to use AxonOps remote backup functionality, axonops user will require read access on Cassandra data folder. As well if you plan to Restore data with AxonOps, axonops user will require write access to Cassandra data folder. We recommend to only provide temporary write access to axonops when required.","title":"(Optional) Step 7 - Cassandra Remote Backups or Restore Prerequisites"},{"location":"installation/cassandra-agent/install/#cassandra-agent-package-details","text":"Configuration: /etc/axonops/axon-agent.yml Binary: /usr/share/axonops/axon-cassandra{version}-agent.jar Version number: /usr/share/axonops/axon-cassandra{version}-agent.version Copyright : /usr/share/doc/axonops/axon-cassandra{version}-agent/copyright Licenses : /usr/share/axonops/licenses/axon-cassandra{version}-agent/","title":"Cassandra agent Package details"},{"location":"installation/cassandra-agent/install/#axon-agent-package-details-dependency-of-cassandra-agent","text":"Configuration: /etc/axonops/axon-agent.yml Binary: usr/share/axonops/axon-agent Logs : /var/log/axonops/axon-agent.log Systemd service: /usr/lib/systemd/system/axon-agent.service","title":"axon-agent Package details (dependency of Cassandra agent)"},{"location":"installation/compat_matrix/compat_matrix/","text":"AxonOps Server \u00b6 Operating Systems Target Architecture RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [10,11,12] x86_64 AxonOps GUI Server \u00b6 Operating Systems Target Architecture RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [10,11,12] x86_64 AxonOps Cassandra Agent \u00b6 System AxonOps Cassandra Agent Name Java Versions Operating Systems Target Architecture Cassandra 3.0.x axon-cassandra3.0-agent Java 8 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 3.11.x axon-cassandra3.11-agent Java 8 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 4.0.x axon-cassandra4.0-agent Java 11 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 4.0.x axon-cassandra4.0-agent-jdk8 Java 8 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 4.1.x axon-cassandra4.1-agent Java 11 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 4.1.x axon-cassandra4.1-agent-jdk8 Java 8 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 5.0.x axon-cassandra5.0-agent-jdk11 Java 11 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Java Versions \u00b6 Oracle Java Standard Edition 8 Oracle Java Standard Edition 11 (Long Term Support) OpenJDK 8 OpenJDK 11","title":"Interoperability matrix"},{"location":"installation/compat_matrix/compat_matrix/#axonops-server","text":"Operating Systems Target Architecture RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [10,11,12] x86_64","title":"AxonOps Server"},{"location":"installation/compat_matrix/compat_matrix/#axonops-gui-server","text":"Operating Systems Target Architecture RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [10,11,12] x86_64","title":"AxonOps GUI Server"},{"location":"installation/compat_matrix/compat_matrix/#axonops-cassandra-agent","text":"System AxonOps Cassandra Agent Name Java Versions Operating Systems Target Architecture Cassandra 3.0.x axon-cassandra3.0-agent Java 8 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 3.11.x axon-cassandra3.11-agent Java 8 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 4.0.x axon-cassandra4.0-agent Java 11 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 4.0.x axon-cassandra4.0-agent-jdk8 Java 8 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 4.1.x axon-cassandra4.1-agent Java 11 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 4.1.x axon-cassandra4.1-agent-jdk8 Java 8 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 5.0.x axon-cassandra5.0-agent-jdk11 Java 11 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64","title":"AxonOps Cassandra Agent"},{"location":"installation/compat_matrix/compat_matrix/#java-versions","text":"Oracle Java Standard Edition 8 Oracle Java Standard Edition 11 (Long Term Support) OpenJDK 8 OpenJDK 11","title":"Java Versions"},{"location":"installation/dse-agent/install/","text":"axon-java-agent for DSE installation \u00b6 This agent will enable metrics collection from DSE and enable adaptive repairs and backups. Prerequisites \u00b6 DSE agent needs axon-agent to be installed and configured properly. If not installed already, please go to axon-agent installation page. Setup axon-agent for DSE \u00b6 You'll need the specify/update the following lines from axon-agent.yml located in /etc/axonops/axon-agent.yml : axon-server : hosts : \"axon-server_endpoint\" # Specify axon-server endpoint axon-agent : host : 0.0.0.0 # axon-agent listening address for it's OpenTSDB endpoint port : 9916 # axon-agent listening port for it's OpenTSDB endpoint org : \"your_organisation_name\" # Specify your organisation name standalone_mode : false type : \"dse\" #cluster_name: \"standalone\" # comment that line ssl : false # SSL flag for it's OpenTSDB endpoint Set standalone_mode to false Set type to dse Don't forget to comment or remove the cluster_name as it will be deduced from DSE configuration. Don't forget to specify axon-server host and port if that's not already specified. DSE agent installation \u00b6 Make sure the {version} of your DSE and DSE agent are compatible from the compatibility matrix . CentOS / RedHat installer \u00b6 sudo yum install <TODO> Debian / Ubuntu installer \u00b6 sudo apt-get install <TODO> Package details \u00b6 Configuration: /etc/axonops/axon-java-agent.yml Binary: usr/share/axonops/axon-dse{version}-agent-1.0.jar Version number: usr/share/axonops/axon-dse{version}-agent-1.0.version Configure DSE \u00b6 Edit cassandra-env.sh usually located in your dse install path such as /<path_to_DSE>/resources/cassandra/conf/cassandra-env.sh and add at the end of the file the following line: JVM_OPTS = \" $JVM_OPTS -javaagent:/usr/share/axonops/axon-dse{version}-agent-1.0.jar=/etc/axonops/axon-java-agent.yml\" example: JVM_OPTS = \" $JVM_OPTS -javaagent:/usr/share/axonops/axon-dse6.0.4-agent-1.0.jar=/etc/axonops/axon-java-agent.yml\" Start DSE \u00b6 All you need to do now is start DSE. Configuration defaults \u00b6 tier0 : # metrics collected every 5 seconds metrics : jvm_ : - \"java.lang:*\" cas_ : - \"org.apache.cassandra.metrics:*\" - \"org.apache.cassandra.net:type=FailureDetector\" - \"com.datastax.bdp:type=dsefs,*\" tier1 : frequency : 300 # metrics collected every 300 seconds (5m) metrics : cas_ : - \"org.apache.cassandra.metrics:name=EstimatedPartitionCount,*\" #tier2: # frequency: 3600 # 1h #tier3: # frequency: 86400 # 1d blacklist : # You can blacklist metrics based on MBean query pattern - \"org.apache.cassandra.metrics:type=ColumnFamily,*\" # dup of tables - \"org.apache.cassandra.metrics:name=SnapshotsSize,*\" # generally takes time free_text_blacklist : # You can blacklist metrics based on Regex pattern - \"org.apache.cassandra.metrics:type=ThreadPools,path=internal,scope=Repair#.*\" warningThresholdMillis : 100 # This will warn in logs when a MBean takes longer than the specified value. whitelisted_clients : # Whitelist for CQL connections - \"127.0.0.1\" - \"^*.*.*.*\"","title":"axon-java-agent for DSE installation"},{"location":"installation/dse-agent/install/#axon-java-agent-for-dse-installation","text":"This agent will enable metrics collection from DSE and enable adaptive repairs and backups.","title":"axon-java-agent for DSE installation"},{"location":"installation/dse-agent/install/#prerequisites","text":"DSE agent needs axon-agent to be installed and configured properly. If not installed already, please go to axon-agent installation page.","title":"Prerequisites"},{"location":"installation/dse-agent/install/#setup-axon-agent-for-dse","text":"You'll need the specify/update the following lines from axon-agent.yml located in /etc/axonops/axon-agent.yml : axon-server : hosts : \"axon-server_endpoint\" # Specify axon-server endpoint axon-agent : host : 0.0.0.0 # axon-agent listening address for it's OpenTSDB endpoint port : 9916 # axon-agent listening port for it's OpenTSDB endpoint org : \"your_organisation_name\" # Specify your organisation name standalone_mode : false type : \"dse\" #cluster_name: \"standalone\" # comment that line ssl : false # SSL flag for it's OpenTSDB endpoint Set standalone_mode to false Set type to dse Don't forget to comment or remove the cluster_name as it will be deduced from DSE configuration. Don't forget to specify axon-server host and port if that's not already specified.","title":"Setup axon-agent for DSE"},{"location":"installation/dse-agent/install/#dse-agent-installation","text":"Make sure the {version} of your DSE and DSE agent are compatible from the compatibility matrix .","title":"DSE agent installation"},{"location":"installation/dse-agent/install/#centos-redhat-installer","text":"sudo yum install <TODO>","title":"CentOS / RedHat installer"},{"location":"installation/dse-agent/install/#debian-ubuntu-installer","text":"sudo apt-get install <TODO>","title":"Debian / Ubuntu installer"},{"location":"installation/dse-agent/install/#package-details","text":"Configuration: /etc/axonops/axon-java-agent.yml Binary: usr/share/axonops/axon-dse{version}-agent-1.0.jar Version number: usr/share/axonops/axon-dse{version}-agent-1.0.version","title":"Package details"},{"location":"installation/dse-agent/install/#configure-dse","text":"Edit cassandra-env.sh usually located in your dse install path such as /<path_to_DSE>/resources/cassandra/conf/cassandra-env.sh and add at the end of the file the following line: JVM_OPTS = \" $JVM_OPTS -javaagent:/usr/share/axonops/axon-dse{version}-agent-1.0.jar=/etc/axonops/axon-java-agent.yml\" example: JVM_OPTS = \" $JVM_OPTS -javaagent:/usr/share/axonops/axon-dse6.0.4-agent-1.0.jar=/etc/axonops/axon-java-agent.yml\"","title":"Configure DSE"},{"location":"installation/dse-agent/install/#start-dse","text":"All you need to do now is start DSE.","title":"Start DSE"},{"location":"installation/dse-agent/install/#configuration-defaults","text":"tier0 : # metrics collected every 5 seconds metrics : jvm_ : - \"java.lang:*\" cas_ : - \"org.apache.cassandra.metrics:*\" - \"org.apache.cassandra.net:type=FailureDetector\" - \"com.datastax.bdp:type=dsefs,*\" tier1 : frequency : 300 # metrics collected every 300 seconds (5m) metrics : cas_ : - \"org.apache.cassandra.metrics:name=EstimatedPartitionCount,*\" #tier2: # frequency: 3600 # 1h #tier3: # frequency: 86400 # 1d blacklist : # You can blacklist metrics based on MBean query pattern - \"org.apache.cassandra.metrics:type=ColumnFamily,*\" # dup of tables - \"org.apache.cassandra.metrics:name=SnapshotsSize,*\" # generally takes time free_text_blacklist : # You can blacklist metrics based on Regex pattern - \"org.apache.cassandra.metrics:type=ThreadPools,path=internal,scope=Repair#.*\" warningThresholdMillis : 100 # This will warn in logs when a MBean takes longer than the specified value. whitelisted_clients : # Whitelist for CQL connections - \"127.0.0.1\" - \"^*.*.*.*\"","title":"Configuration defaults"},{"location":"installation/kubernetes/","text":"Running AxonOps on Kubernetes \u00b6 Introduction \u00b6 The following shows how to install AxonOps for monitoring cassandra. AxonOps requires ElasticSearch and the documentation below shows how to install both. If you already have ElasticSearch running, you can omit the installation and just ensure the AxonOps config points to it. AxonOps installation uses Helm Charts. Helm v3.8.0 or later is required in order to access the OCI repository hosting the charts. The raw charts can be downloaded from the GitHub repository . Preparing the configuration \u00b6 Resources \u00b6 Cassandra Nodes ElasticSearch CPU ElasticSearch Memory AxonOps Server CPU AxonOps Server Memory <10 1000m 4Gi 750m 1Gi <50 1000m 4Gi 2000m 6Gi 100 2000m 16Gi 4000m 12Gi 200 4000m 32Gi 8000m 24Gi ElasticSearch \u00b6 The example below is a configuration file for the official ElasticSearch helm repository. See inline comments: --- clusterName : \"axonops-elastic\" replicas : 1 esConfig : elasticsearch.yml : | thread_pool.write.queue_size: 2000 roles : master : \"true\" ingest : \"true\" data : \"true\" remote_cluster_client : \"false\" ml : \"false\" # Adjust the memory and cpu requirements to your deployment # esJavaOpts : \"-Xms2g -Xmx2g\" resources : requests : cpu : \"750m\" memory : \"2Gi\" limits : cpu : \"1500m\" memory : \"4Gi\" volumeClaimTemplate : accessModes : [ \"ReadWriteOnce\" ] storageClassName : \"\" # adjust to your storageClass if you don't want to use default resources : requests : storage : 50Gi rbac : create : true AxonOps \u00b6 The default AxonOps installation does not expose the services outside of the cluster. We recommend that you use either a LoadBalancer service or an Ingress. Below you can find an example using Ingress to expose both the dashboard and the AxonOps server. axon-dash : image : pullPolicy : IfNotPresent repository : registry.axonops.com/axonops-public/axonops-docker/axon-dash tag : latest ingress : enabled : true className : nginx annotations : external-dns.alpha.kubernetes.io/hostname : axonops.mycompany.com hosts : - host : axonops.mycompany.com path : \"/\" tls : - hosts : - axonops.mycompany.com secretName : axon-dash-tls resources : limits : cpu : 1000m memory : 1536Mi requests : cpu : 25m memory : 256Mi # If you are using an existing ElasticSearch rather than installing it # as shown above then make sure you update the elasticHost URL below axon-server : elasticHost : http://axonops-elastic-master:9200 dashboardUrl : https://axonops.mycompany.com config : # Set your organization name here. This must match the name used in your license key org_name : demo # Enter your AxonOps license key here license_key : \"...\" image : pullPolicy : IfNotPresent repository : registry.axonops.com/axonops-public/axonops-docker/axon-server tag : latest # Enable the agent ingress to allow agents to connect from outside the Kubernetes cluster agentIngress : enabled : true className : nginx annotations : external-dns.alpha.kubernetes.io/hostname : axonops-server.mycompany.com hosts : - host : axonops-server.mycompany.com path : \"/\" tls : - hosts : - axonops-server.mycompany.com secretName : axon-server-tls resources : limits : cpu : 1 memory : 1Gi requests : cpu : 100m memory : 256Mi An example values file showing all available options can be found in the GitHub repository here: values-full.yaml Installing \u00b6 ElasticSearch \u00b6 Now you can install Elasticsearch referencing the configuration file created in the previous step: helm repo add elastic https://helm.elastic.co helm update helm upgrade -n axonops --install \\ --create-namespace \\ -f \"elasticsearch.yaml\" \\ elasticsearch elastic/elasticsearch AxonOps \u00b6 Finally install the AxonOps helm chart: helm upgrade -n axonops --install \\ --create-namespace \\ -f \"axonops.yaml\" \\ axonops oci://helm.axonops.com/axonops-public/axonops-helm/axonops","title":"Kubernetes"},{"location":"installation/kubernetes/#running-axonops-on-kubernetes","text":"","title":"Running AxonOps on Kubernetes"},{"location":"installation/kubernetes/#introduction","text":"The following shows how to install AxonOps for monitoring cassandra. AxonOps requires ElasticSearch and the documentation below shows how to install both. If you already have ElasticSearch running, you can omit the installation and just ensure the AxonOps config points to it. AxonOps installation uses Helm Charts. Helm v3.8.0 or later is required in order to access the OCI repository hosting the charts. The raw charts can be downloaded from the GitHub repository .","title":"Introduction"},{"location":"installation/kubernetes/#preparing-the-configuration","text":"","title":"Preparing the configuration"},{"location":"installation/kubernetes/#resources","text":"Cassandra Nodes ElasticSearch CPU ElasticSearch Memory AxonOps Server CPU AxonOps Server Memory <10 1000m 4Gi 750m 1Gi <50 1000m 4Gi 2000m 6Gi 100 2000m 16Gi 4000m 12Gi 200 4000m 32Gi 8000m 24Gi","title":"Resources"},{"location":"installation/kubernetes/#elasticsearch","text":"The example below is a configuration file for the official ElasticSearch helm repository. See inline comments: --- clusterName : \"axonops-elastic\" replicas : 1 esConfig : elasticsearch.yml : | thread_pool.write.queue_size: 2000 roles : master : \"true\" ingest : \"true\" data : \"true\" remote_cluster_client : \"false\" ml : \"false\" # Adjust the memory and cpu requirements to your deployment # esJavaOpts : \"-Xms2g -Xmx2g\" resources : requests : cpu : \"750m\" memory : \"2Gi\" limits : cpu : \"1500m\" memory : \"4Gi\" volumeClaimTemplate : accessModes : [ \"ReadWriteOnce\" ] storageClassName : \"\" # adjust to your storageClass if you don't want to use default resources : requests : storage : 50Gi rbac : create : true","title":"ElasticSearch"},{"location":"installation/kubernetes/#axonops","text":"The default AxonOps installation does not expose the services outside of the cluster. We recommend that you use either a LoadBalancer service or an Ingress. Below you can find an example using Ingress to expose both the dashboard and the AxonOps server. axon-dash : image : pullPolicy : IfNotPresent repository : registry.axonops.com/axonops-public/axonops-docker/axon-dash tag : latest ingress : enabled : true className : nginx annotations : external-dns.alpha.kubernetes.io/hostname : axonops.mycompany.com hosts : - host : axonops.mycompany.com path : \"/\" tls : - hosts : - axonops.mycompany.com secretName : axon-dash-tls resources : limits : cpu : 1000m memory : 1536Mi requests : cpu : 25m memory : 256Mi # If you are using an existing ElasticSearch rather than installing it # as shown above then make sure you update the elasticHost URL below axon-server : elasticHost : http://axonops-elastic-master:9200 dashboardUrl : https://axonops.mycompany.com config : # Set your organization name here. This must match the name used in your license key org_name : demo # Enter your AxonOps license key here license_key : \"...\" image : pullPolicy : IfNotPresent repository : registry.axonops.com/axonops-public/axonops-docker/axon-server tag : latest # Enable the agent ingress to allow agents to connect from outside the Kubernetes cluster agentIngress : enabled : true className : nginx annotations : external-dns.alpha.kubernetes.io/hostname : axonops-server.mycompany.com hosts : - host : axonops-server.mycompany.com path : \"/\" tls : - hosts : - axonops-server.mycompany.com secretName : axon-server-tls resources : limits : cpu : 1 memory : 1Gi requests : cpu : 100m memory : 256Mi An example values file showing all available options can be found in the GitHub repository here: values-full.yaml","title":"AxonOps"},{"location":"installation/kubernetes/#installing","text":"","title":"Installing"},{"location":"installation/kubernetes/#elasticsearch_1","text":"Now you can install Elasticsearch referencing the configuration file created in the previous step: helm repo add elastic https://helm.elastic.co helm update helm upgrade -n axonops --install \\ --create-namespace \\ -f \"elasticsearch.yaml\" \\ elasticsearch elastic/elasticsearch","title":"ElasticSearch"},{"location":"installation/kubernetes/#axonops_1","text":"Finally install the AxonOps helm chart: helm upgrade -n axonops --install \\ --create-namespace \\ -f \"axonops.yaml\" \\ axonops oci://helm.axonops.com/axonops-public/axonops-helm/axonops","title":"AxonOps"},{"location":"installation/kubernetes/minikube/","text":"Cassandra with AxonOps on Kubernetes \u00b6 Introduction \u00b6 The following shows how to install AxonOps for monitoring cassandra. This process specifically requires the official cassandra helm repository . Using minikube \u00b6 The deployment should work fine on latest versions of minikube as long as you provide enough memory for it. minikube start --memory 8192 --cpus = 4 minikube addons enable storage-provisioner :warning: Make sure you use a recent version of minikube. Also check available drivers and select the most appropriate for your platform Helmfile \u00b6 Overview \u00b6 As this deployment contains multiple applications we recommend you use an automation system such as Ansible or Helmfile to put together the config. The example below uses helmfile. Install requirements \u00b6 You would need to install the following components: helm: https://helm.sh/docs/intro/install/ helmfile: https://github.com/roboll/helmfile/releases Alternatively you can consider using a dockerized version of them both such as https://hub.docker.com/r/chatwork/helmfile Config files \u00b6 The values below are set for running on a laptop with minikube , adjust accordingly for larger deployments. helmfile.yaml \u00b6 --- repositories : - name : axonops url : helm.axonops.com/axonops-public/axonops-helm/axonops oci : true - name : bitnami url : https://charts.bitnami.com/bitnami - name : ckotzbauer url : https://ckotzbauer.github.io/helm-charts releases : - name : axon-elastic namespace : {{ env \"NAMESPACE\" | default \"axonops\" }} chart : \"bitnami/elasticsearch\" version : '12.8.1' wait : true values : - fullnameOverride : axon-elastic - imageTag : \"7.8.0\" - data : replicas : 1 persistence : size : 1Gi enabled : true accessModes : [ \"ReadWriteOnce\" ] - curator : enabled : true - coordinating : replicas : 1 - master : replicas : 1 persistence : size : 1Gi enabled : true accessModes : [ \"ReadWriteOnce\" ] - name : axonops namespace : {{ env \"NAMESPACE\" | default \"axonops\" }} chart : \"digitalis/axonops\" wait : true values : - values.yaml - name : cassandra namespace : cassandra chart : \"digitalis/cassandra\" wait : true values : - values.yaml - name : cadvisor namespace : kube-system chart : ckotzbauer/cadvisor version : 1.2.0 values : - container : additionalArgs : - --housekeeping_interval=5s # kubernetes default args - --max_housekeeping_interval=10s - --event_storage_event_limit=default=0 - --event_storage_age_limit=default=0 - --disable_metrics=percpu,process,sched,tcp,udp # enable only diskIO, cpu, memory, network, disk - --docker_only - image : repository : gcr.io/cadvisor/cadvisor tag : v0.37.0 values.yaml \u00b6 --- persistence : enabled : true size : 2Gi accessMode : ReadWriteMany podSettings : terminationGracePeriodSeconds : 300 image : tag : 3.11.6 pullPolicy : IfNotPresent config : cluster_name : digitalis cluster_size : 2 dc_name : dc1 seed_size : 1 num_tokens : 256 max_heap_size : 512M heap_new_size : 512M endpoint_snitch : GossipingPropertyFileSnitch env : JVM_OPTS : \"-javaagent:/var/lib/axonops/axon-cassandra3.11-agent.jar=/etc/axonops/axon-agent.yml\" serviceAccount : create : true rules : - apiGroups : - \"\" resources : - nodes - nodes/metrics - pods verbs : - get - list - watch - nonResourceURLs : - /metrics verbs : - get extraVolumes : - name : axonops-agent-config configMap : name : axonops-agent - name : axonops-shared emptyDir : {} - name : axonops-logs emptyDir : {} extraVolumeMounts : - name : axonops-shared mountPath : /var/lib/axonops readOnly : false - name : axonops-agent-config mountPath : /etc/axonops readOnly : true - name : axonops-logs mountPath : /var/log/axonops extraContainers : - name : axonops-agent image : digitalisdocker/axon-agent:latest env : - name : AXON_AGENT_VERBOSITY value : \"1\" - name : AXON_AGENT_ARGS value : \"-v 1\" - name : DATA_FILE_DIRECTORY value : \"/var/lib/cassandra\" - name : CASSANDRA_POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : CASSANDRA_POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace - name : CASSANDRA_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName - name : CASSANDRA_POD_IP valueFrom : fieldRef : apiVersion : v1 fieldPath : status.podIP volumeMounts : - name : axonops-agent-config mountPath : /etc/axonops readOnly : true - name : axonops-shared mountPath : /var/lib/axonops readOnly : false - name : axonops-logs mountPath : /var/log/axonops - name : data mountPath : /var/lib/cassandra axon-server : global : customer : minikube baseDomain : axonops.com elasticHost : http://axon-elastic-elasticsearch-master.axonops:9200 dashboardUrl : https://axonops.axonops.com image : repository : digitalisdocker/axon-server tag : latest pullPolicy : IfNotPresent config : extraConfig : cql_hosts : - cassandra-0.cassandra.cassandra.svc.cluster.local cql_username : \"cassandra\" cql_password : \"cassandra\" cql_local_dc : dc1 cql_proto_version : 4 cql_max_searchqueriesparallelism : 100 cql_batch_size : 100 cql_page_size : 100 cql_autocreate_tables : false cql_retrypolicy_numretries : 3 cql_retrypolicy_min : 2s cql_retrypolicy_max : 10s cql_reconnectionpolicy_maxretries : 10 cql_reconnectionpolicy_initialinterval : 1s cql_reconnectionpolicy_maxinterval : 10s cql_keyspace_replication : \"{ 'class': 'NetworkTopologyStrategy', 'dc1': 1 }\" cql_metrics_cache_max_size : 128 #MB cql_metrics_cache_max_items : 500000 axon-dash : replicaCount : 1 config : axonServerUrl : http://axonops-axon-server:8080 service : type : NodePort ingress : enabled : true annotations : nginx.ingress.kubernetes.io/ssl-redirect : \"true\" hosts : - hosts : axonops.axonops.com paths : - / image : repository : digitalisdocker/axon-dash tag : latest pullPolicy : IfNotPresent autoscaling : enabled : true resources : limits : cpu : 500m memory : 512Mi requests : cpu : 50m memory : 128Mi axon-agent.yml \u00b6 axon-server : hosts : \"axonops-axon-server.axonops\" # Specify axon-server IP axon-server.mycompany. port : 1888 axon-agent : org : \"digitalis\" human_readable_identifier : \"axon_agent_ip\" # one of the following: NTP : host : \"pool.ntp.org\" # Specify a NTP to determine a NTP offset cassandra : tier0 : # metrics collected every 5 seconds metrics : jvm_ : - \"java.lang:*\" cas_ : - \"org.apache.cassandra.metrics:*\" - \"org.apache.cassandra.net:type=FailureDetector\" tier1 : frequency : 300 # metrics collected every 300 seconds (5m) metrics : cas_ : - \"org.apache.cassandra.metrics:name=EstimatedPartitionCount,*\" blacklist : # You can blacklist metrics based on Regex pattern. Hit the agent on http://agentIP:9916/metricslist to list JMX metrics it is collecting - \"org.apache.cassandra.metrics:type=ColumnFamily.*\" # duplication of table metrics - \"org.apache.cassandra.metrics:.*scope=Repair#.*\" # ignore each repair instance metrics - \"org.apache.cassandra.metrics:.*name=SnapshotsSize.*\" # Collecting SnapshotsSize metrics slows down collection - \"org.apache.cassandra.metrics:.*Max.*\" - \"org.apache.cassandra.metrics:.*Min.*\" - \".*999thPercentile|.*50thPercentile|.*FifteenMinuteRate|.*FiveMinuteRate|.*MeanRate|.*Mean|.*OneMinuteRate|.*StdDev\" JMXOperationsBlacklist : - \"getThreadInfo\" - \"getDatacenter\" - \"getRack\" DMLEventsWhitelist : # You can whitelist keyspaces / tables (list of \"keyspace\" and/or \"keyspace.table\") to log DML queries. Data is not analysed. # - \"system_distributed\" DMLEventsBlacklist : # You can blacklist keyspaces / tables from the DMLEventsWhitelist (list of \"keyspace\" and/or \"keyspace.table\") to log DML queries. Data is not analysed. # - system_distributed.parent_repair_history logSuccessfulRepairs : false # set it to true if you want to log all the successful repair events. warningThresholdMillis : 200 # This will warn in logs when a MBean takes longer than the specified value. logFormat : \"%4$s %1$tY-%1$tm-%1$td %1$tH:%1$tM:%1$tS,%1$tL %5$s%6$s%n\" Start up \u00b6 Create Axon Agent configuration \u00b6 kubectl create ns cassandra kubectl create configmap axonops-agent --from-file = axon-agent.yml -n cassandra Run helmfile \u00b6 With locally installed helm and helmfile \u00b6 cd your/config/directory hemlfile sync With docker image \u00b6 docker run --rm \\ -v ~/.kube:/root/.kube \\ -v ${ PWD } /.helm:/root/.helm \\ -v ${ PWD } /helmfile.yaml:/helmfile.yaml \\ -v ${ PWD } /values.yaml:/values.yaml \\ --net = host chatwork/helmfile sync Access \u00b6 Minikube \u00b6 If you used minikube , identify the name of the service with kubectl get svc -n monitoring and launch it with minikube service axonops-axon-dash -n monitoring LoadBalancer \u00b6 Find the DNS entry for it: kubectl get svc -n monitoring -o wide Open your browser and copy and paste the URL. Troubleshooting \u00b6 Check the status of the pods: kubectl get pod -n monitoring kubectl get pod -n cassandra Any pod which is not on state Running check it out with kubectl describe -n NAMESPACE pod POD-NAME Storage \u00b6 One common problem is regarding storage. If you have enabled persistent storage you may see an error about persistent volume claims (not found, unclaimed, etc.). If you're using minikube make sure you enable storage with minikube addons enable storage-provisioner Memory \u00b6 The second most common problem is not enough memory (OOMKilled). You will see this often if your node does not have enough memory to run the containers or if the heap settings for Cassandra are not right. kubectl describe command will be showing Error 127 when this occurs. In the values.yaml file adjust the heap options to match your hardware: max_heap_size : 512M heap_new_size : 512M Minikube \u00b6 Review the way you have started up minikube and assign more memory if you can. Also check the available drivers and select the appropriate for your platform. On macOS where I tested hyperkit or virtualbox are the best ones. minikube start --memory 10240 --cpus = 4 --driver = hyperkit Putting it all together \u00b6","title":"Cassandra with AxonOps on Kubernetes"},{"location":"installation/kubernetes/minikube/#cassandra-with-axonops-on-kubernetes","text":"","title":"Cassandra with AxonOps on Kubernetes"},{"location":"installation/kubernetes/minikube/#introduction","text":"The following shows how to install AxonOps for monitoring cassandra. This process specifically requires the official cassandra helm repository .","title":"Introduction"},{"location":"installation/kubernetes/minikube/#using-minikube","text":"The deployment should work fine on latest versions of minikube as long as you provide enough memory for it. minikube start --memory 8192 --cpus = 4 minikube addons enable storage-provisioner :warning: Make sure you use a recent version of minikube. Also check available drivers and select the most appropriate for your platform","title":"Using minikube"},{"location":"installation/kubernetes/minikube/#helmfile","text":"","title":"Helmfile"},{"location":"installation/kubernetes/minikube/#overview","text":"As this deployment contains multiple applications we recommend you use an automation system such as Ansible or Helmfile to put together the config. The example below uses helmfile.","title":"Overview"},{"location":"installation/kubernetes/minikube/#install-requirements","text":"You would need to install the following components: helm: https://helm.sh/docs/intro/install/ helmfile: https://github.com/roboll/helmfile/releases Alternatively you can consider using a dockerized version of them both such as https://hub.docker.com/r/chatwork/helmfile","title":"Install requirements"},{"location":"installation/kubernetes/minikube/#config-files","text":"The values below are set for running on a laptop with minikube , adjust accordingly for larger deployments.","title":"Config files"},{"location":"installation/kubernetes/minikube/#helmfileyaml","text":"--- repositories : - name : axonops url : helm.axonops.com/axonops-public/axonops-helm/axonops oci : true - name : bitnami url : https://charts.bitnami.com/bitnami - name : ckotzbauer url : https://ckotzbauer.github.io/helm-charts releases : - name : axon-elastic namespace : {{ env \"NAMESPACE\" | default \"axonops\" }} chart : \"bitnami/elasticsearch\" version : '12.8.1' wait : true values : - fullnameOverride : axon-elastic - imageTag : \"7.8.0\" - data : replicas : 1 persistence : size : 1Gi enabled : true accessModes : [ \"ReadWriteOnce\" ] - curator : enabled : true - coordinating : replicas : 1 - master : replicas : 1 persistence : size : 1Gi enabled : true accessModes : [ \"ReadWriteOnce\" ] - name : axonops namespace : {{ env \"NAMESPACE\" | default \"axonops\" }} chart : \"digitalis/axonops\" wait : true values : - values.yaml - name : cassandra namespace : cassandra chart : \"digitalis/cassandra\" wait : true values : - values.yaml - name : cadvisor namespace : kube-system chart : ckotzbauer/cadvisor version : 1.2.0 values : - container : additionalArgs : - --housekeeping_interval=5s # kubernetes default args - --max_housekeeping_interval=10s - --event_storage_event_limit=default=0 - --event_storage_age_limit=default=0 - --disable_metrics=percpu,process,sched,tcp,udp # enable only diskIO, cpu, memory, network, disk - --docker_only - image : repository : gcr.io/cadvisor/cadvisor tag : v0.37.0","title":"helmfile.yaml"},{"location":"installation/kubernetes/minikube/#valuesyaml","text":"--- persistence : enabled : true size : 2Gi accessMode : ReadWriteMany podSettings : terminationGracePeriodSeconds : 300 image : tag : 3.11.6 pullPolicy : IfNotPresent config : cluster_name : digitalis cluster_size : 2 dc_name : dc1 seed_size : 1 num_tokens : 256 max_heap_size : 512M heap_new_size : 512M endpoint_snitch : GossipingPropertyFileSnitch env : JVM_OPTS : \"-javaagent:/var/lib/axonops/axon-cassandra3.11-agent.jar=/etc/axonops/axon-agent.yml\" serviceAccount : create : true rules : - apiGroups : - \"\" resources : - nodes - nodes/metrics - pods verbs : - get - list - watch - nonResourceURLs : - /metrics verbs : - get extraVolumes : - name : axonops-agent-config configMap : name : axonops-agent - name : axonops-shared emptyDir : {} - name : axonops-logs emptyDir : {} extraVolumeMounts : - name : axonops-shared mountPath : /var/lib/axonops readOnly : false - name : axonops-agent-config mountPath : /etc/axonops readOnly : true - name : axonops-logs mountPath : /var/log/axonops extraContainers : - name : axonops-agent image : digitalisdocker/axon-agent:latest env : - name : AXON_AGENT_VERBOSITY value : \"1\" - name : AXON_AGENT_ARGS value : \"-v 1\" - name : DATA_FILE_DIRECTORY value : \"/var/lib/cassandra\" - name : CASSANDRA_POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : CASSANDRA_POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace - name : CASSANDRA_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName - name : CASSANDRA_POD_IP valueFrom : fieldRef : apiVersion : v1 fieldPath : status.podIP volumeMounts : - name : axonops-agent-config mountPath : /etc/axonops readOnly : true - name : axonops-shared mountPath : /var/lib/axonops readOnly : false - name : axonops-logs mountPath : /var/log/axonops - name : data mountPath : /var/lib/cassandra axon-server : global : customer : minikube baseDomain : axonops.com elasticHost : http://axon-elastic-elasticsearch-master.axonops:9200 dashboardUrl : https://axonops.axonops.com image : repository : digitalisdocker/axon-server tag : latest pullPolicy : IfNotPresent config : extraConfig : cql_hosts : - cassandra-0.cassandra.cassandra.svc.cluster.local cql_username : \"cassandra\" cql_password : \"cassandra\" cql_local_dc : dc1 cql_proto_version : 4 cql_max_searchqueriesparallelism : 100 cql_batch_size : 100 cql_page_size : 100 cql_autocreate_tables : false cql_retrypolicy_numretries : 3 cql_retrypolicy_min : 2s cql_retrypolicy_max : 10s cql_reconnectionpolicy_maxretries : 10 cql_reconnectionpolicy_initialinterval : 1s cql_reconnectionpolicy_maxinterval : 10s cql_keyspace_replication : \"{ 'class': 'NetworkTopologyStrategy', 'dc1': 1 }\" cql_metrics_cache_max_size : 128 #MB cql_metrics_cache_max_items : 500000 axon-dash : replicaCount : 1 config : axonServerUrl : http://axonops-axon-server:8080 service : type : NodePort ingress : enabled : true annotations : nginx.ingress.kubernetes.io/ssl-redirect : \"true\" hosts : - hosts : axonops.axonops.com paths : - / image : repository : digitalisdocker/axon-dash tag : latest pullPolicy : IfNotPresent autoscaling : enabled : true resources : limits : cpu : 500m memory : 512Mi requests : cpu : 50m memory : 128Mi","title":"values.yaml"},{"location":"installation/kubernetes/minikube/#axon-agentyml","text":"axon-server : hosts : \"axonops-axon-server.axonops\" # Specify axon-server IP axon-server.mycompany. port : 1888 axon-agent : org : \"digitalis\" human_readable_identifier : \"axon_agent_ip\" # one of the following: NTP : host : \"pool.ntp.org\" # Specify a NTP to determine a NTP offset cassandra : tier0 : # metrics collected every 5 seconds metrics : jvm_ : - \"java.lang:*\" cas_ : - \"org.apache.cassandra.metrics:*\" - \"org.apache.cassandra.net:type=FailureDetector\" tier1 : frequency : 300 # metrics collected every 300 seconds (5m) metrics : cas_ : - \"org.apache.cassandra.metrics:name=EstimatedPartitionCount,*\" blacklist : # You can blacklist metrics based on Regex pattern. Hit the agent on http://agentIP:9916/metricslist to list JMX metrics it is collecting - \"org.apache.cassandra.metrics:type=ColumnFamily.*\" # duplication of table metrics - \"org.apache.cassandra.metrics:.*scope=Repair#.*\" # ignore each repair instance metrics - \"org.apache.cassandra.metrics:.*name=SnapshotsSize.*\" # Collecting SnapshotsSize metrics slows down collection - \"org.apache.cassandra.metrics:.*Max.*\" - \"org.apache.cassandra.metrics:.*Min.*\" - \".*999thPercentile|.*50thPercentile|.*FifteenMinuteRate|.*FiveMinuteRate|.*MeanRate|.*Mean|.*OneMinuteRate|.*StdDev\" JMXOperationsBlacklist : - \"getThreadInfo\" - \"getDatacenter\" - \"getRack\" DMLEventsWhitelist : # You can whitelist keyspaces / tables (list of \"keyspace\" and/or \"keyspace.table\") to log DML queries. Data is not analysed. # - \"system_distributed\" DMLEventsBlacklist : # You can blacklist keyspaces / tables from the DMLEventsWhitelist (list of \"keyspace\" and/or \"keyspace.table\") to log DML queries. Data is not analysed. # - system_distributed.parent_repair_history logSuccessfulRepairs : false # set it to true if you want to log all the successful repair events. warningThresholdMillis : 200 # This will warn in logs when a MBean takes longer than the specified value. logFormat : \"%4$s %1$tY-%1$tm-%1$td %1$tH:%1$tM:%1$tS,%1$tL %5$s%6$s%n\"","title":"axon-agent.yml"},{"location":"installation/kubernetes/minikube/#start-up","text":"","title":"Start up"},{"location":"installation/kubernetes/minikube/#create-axon-agent-configuration","text":"kubectl create ns cassandra kubectl create configmap axonops-agent --from-file = axon-agent.yml -n cassandra","title":"Create Axon Agent configuration"},{"location":"installation/kubernetes/minikube/#run-helmfile","text":"","title":"Run helmfile"},{"location":"installation/kubernetes/minikube/#with-locally-installed-helm-and-helmfile","text":"cd your/config/directory hemlfile sync","title":"With locally installed helm and helmfile"},{"location":"installation/kubernetes/minikube/#with-docker-image","text":"docker run --rm \\ -v ~/.kube:/root/.kube \\ -v ${ PWD } /.helm:/root/.helm \\ -v ${ PWD } /helmfile.yaml:/helmfile.yaml \\ -v ${ PWD } /values.yaml:/values.yaml \\ --net = host chatwork/helmfile sync","title":"With docker image"},{"location":"installation/kubernetes/minikube/#access","text":"","title":"Access"},{"location":"installation/kubernetes/minikube/#minikube","text":"If you used minikube , identify the name of the service with kubectl get svc -n monitoring and launch it with minikube service axonops-axon-dash -n monitoring","title":"Minikube"},{"location":"installation/kubernetes/minikube/#loadbalancer","text":"Find the DNS entry for it: kubectl get svc -n monitoring -o wide Open your browser and copy and paste the URL.","title":"LoadBalancer"},{"location":"installation/kubernetes/minikube/#troubleshooting","text":"Check the status of the pods: kubectl get pod -n monitoring kubectl get pod -n cassandra Any pod which is not on state Running check it out with kubectl describe -n NAMESPACE pod POD-NAME","title":"Troubleshooting"},{"location":"installation/kubernetes/minikube/#storage","text":"One common problem is regarding storage. If you have enabled persistent storage you may see an error about persistent volume claims (not found, unclaimed, etc.). If you're using minikube make sure you enable storage with minikube addons enable storage-provisioner","title":"Storage"},{"location":"installation/kubernetes/minikube/#memory","text":"The second most common problem is not enough memory (OOMKilled). You will see this often if your node does not have enough memory to run the containers or if the heap settings for Cassandra are not right. kubectl describe command will be showing Error 127 when this occurs. In the values.yaml file adjust the heap options to match your hardware: max_heap_size : 512M heap_new_size : 512M","title":"Memory"},{"location":"installation/kubernetes/minikube/#minikube_1","text":"Review the way you have started up minikube and assign more memory if you can. Also check the available drivers and select the appropriate for your platform. On macOS where I tested hyperkit or virtualbox are the best ones. minikube start --memory 10240 --cpus = 4 --driver = hyperkit","title":"Minikube"},{"location":"installation/kubernetes/minikube/#putting-it-all-together","text":"","title":"Putting it all together"},{"location":"integrations/email-integration/","text":"Setup SMTP notifications \u00b6 On the Axonops application menu, select Settings -> Integrations . Click on the SMTP area. Infomy","title":"Email Integration"},{"location":"integrations/email-integration/#setup-smtp-notifications","text":"On the Axonops application menu, select Settings -> Integrations . Click on the SMTP area. Infomy","title":"Setup SMTP notifications"},{"location":"integrations/microsoft-teams-integration/","text":"Setup Microsoft Teams notifications \u00b6 Create Microsoft Teams Webhooks \u00b6 On the Microsoft Teams interface, go to Connectors Click on configure on the Incoming Webhook connector Provide a name and select Create Copy the url provided to the clipboard On the Axonops application menu, select Settings -> Integrations Click on the Microsoft Teams area. Enter a name and copy the url in the Webhook URL field and select Create","title":"Microsoft Teams"},{"location":"integrations/microsoft-teams-integration/#setup-microsoft-teams-notifications","text":"","title":"Setup Microsoft Teams notifications"},{"location":"integrations/microsoft-teams-integration/#create-microsoft-teams-webhooks","text":"On the Microsoft Teams interface, go to Connectors Click on configure on the Incoming Webhook connector Provide a name and select Create Copy the url provided to the clipboard On the Axonops application menu, select Settings -> Integrations Click on the Microsoft Teams area. Enter a name and copy the url in the Webhook URL field and select Create","title":"Create Microsoft Teams Webhooks"},{"location":"integrations/overview/","text":"AxonOps provide various integrations for the notifications. The functionality is accessible via Settings > Integrations The current integrations are: SMTP Pagerduty Slack Microsoft Teams ServiceNow Generic webhooks Infomy Routing \u00b6 AxonOps provide a rich routing mechanism for the notifications. The current routing options are: Global - this will route all the notifications Metrics - notifications about the alerts on metrics Backups - notifications about the backups / restore Service Checks - notifications about the service checks / health checks Nodes - notifications raised from the nodes Commands - notifications from generic tasks Repairs - notifications from Cassandra repairs Rolling Restart - notification from the rolling restart feature Each severity ( info, warning, error ) can be routed independently","title":"Overview"},{"location":"integrations/overview/#routing","text":"AxonOps provide a rich routing mechanism for the notifications. The current routing options are: Global - this will route all the notifications Metrics - notifications about the alerts on metrics Backups - notifications about the backups / restore Service Checks - notifications about the service checks / health checks Nodes - notifications raised from the nodes Commands - notifications from generic tasks Repairs - notifications from Cassandra repairs Rolling Restart - notification from the rolling restart feature Each severity ( info, warning, error ) can be routed independently","title":"Routing"},{"location":"integrations/pagerduy-integration/","text":"Setup Pagerduty \u00b6 Create Pagerduty Routing Key \u00b6 Using these steps . Please note down the pagerduty routing key Insert Pagerduty Routing Key \u00b6 On the Axonops application menu, select Settings -> Integrations . Click on the Pagerduty area. Infomy","title":"PagerDuty Integration"},{"location":"integrations/pagerduy-integration/#setup-pagerduty","text":"","title":"Setup Pagerduty"},{"location":"integrations/pagerduy-integration/#create-pagerduty-routing-key","text":"Using these steps . Please note down the pagerduty routing key","title":"Create Pagerduty Routing Key"},{"location":"integrations/pagerduy-integration/#insert-pagerduty-routing-key","text":"On the Axonops application menu, select Settings -> Integrations . Click on the Pagerduty area. Infomy","title":"Insert Pagerduty Routing Key"},{"location":"integrations/servicenow-integration/","text":"Navigate to Settings > Integrations and click on ServiceNow Once you have gathered your instance name , username and password from ServiceNow, you can validate the form: If you want to see the detailed description of a notification, you'll need to add the description field from ServiceNow incidents templates.","title":"ServiceNow"},{"location":"integrations/slack-integration/","text":"Setup Slack \u00b6 Create Slack Incoming Webhooks \u00b6 Go to Slack Application On the side menu click In search box type Incoming Webhook s From the App directory click Install on Incoming WebHooks App . Infomy Click Add Configuration Infomy In Post to Channel Box select an option from the choose a channel dropdown menu . Click Add Incoming WebHooks Integration Infomy Copy and make a note of the WebHook URL that appears in the Setup Instructions . Infomy Creating the Slack integration on axon-server \u00b6 On the Axonops application menu, select Settings -> Integrations . Click on the Slack area. Infomy Infomy","title":"Slack Integration"},{"location":"integrations/slack-integration/#setup-slack","text":"","title":"Setup Slack"},{"location":"integrations/slack-integration/#create-slack-incoming-webhooks","text":"Go to Slack Application On the side menu click In search box type Incoming Webhook s From the App directory click Install on Incoming WebHooks App . Infomy Click Add Configuration Infomy In Post to Channel Box select an option from the choose a channel dropdown menu . Click Add Incoming WebHooks Integration Infomy Copy and make a note of the WebHook URL that appears in the Setup Instructions . Infomy","title":"Create Slack Incoming Webhooks"},{"location":"integrations/slack-integration/#creating-the-slack-integration-on-axon-server","text":"On the Axonops application menu, select Settings -> Integrations . Click on the Slack area. Infomy Infomy","title":"Creating the Slack integration on axon-server"},{"location":"monitoring/overview/","text":"When monitoring enterprise service there are 3 categories of how the service is performing that we generally capture and monitor. These are; Performance metrics Events (logs) Service availability Performance Metrics \u00b6 Performance metrics in Cassandra is highly extensive and there is a large number that can be captured to understand how Cassandra is performing. Another key metrics that also must be captured in order to effectively understand the performance of a database is the system resource utilisation. AxonOps agent captures both Cassandra and OS metrics and pushes them to the AxonOps server. Events \u00b6 Cassandra event logs are, by default, written to log files. There are important information in the log files that allows SREs and DevOps engineers to identify issues when they occur. AxonOps agent captures the logs and pushes them to the AxonOps server. These logs are visible within AxonOps dashboard allowing quick access to them without having to log in to the individual servers. Service Availability \u00b6 Checking the momentary service availability and dashboards gives confidence that all services are running correctly as expected. Example service checks that allow engineers to gain confidence in the service availability are: System process Network open ports - e.g. CQL and storage ports Database availability - e.g. can execute CQL query AxonOps Monitoring \u00b6 AxonOps implements all three types of monitoring described above. AxonOps agent captures the information, sends them securely to AxonOps server, and the information is stored in the backend data store. AxonOps GUI provides comprehensive set of metrics dashboards combined with the event log view. It also provides separate service check status view showing the health of the cluster. This section describes how the AxonOps GUI organises the dashboards of all three types of monitoring.","title":"Monitoring Overview"},{"location":"monitoring/overview/#performance-metrics","text":"Performance metrics in Cassandra is highly extensive and there is a large number that can be captured to understand how Cassandra is performing. Another key metrics that also must be captured in order to effectively understand the performance of a database is the system resource utilisation. AxonOps agent captures both Cassandra and OS metrics and pushes them to the AxonOps server.","title":"Performance Metrics"},{"location":"monitoring/overview/#events","text":"Cassandra event logs are, by default, written to log files. There are important information in the log files that allows SREs and DevOps engineers to identify issues when they occur. AxonOps agent captures the logs and pushes them to the AxonOps server. These logs are visible within AxonOps dashboard allowing quick access to them without having to log in to the individual servers.","title":"Events"},{"location":"monitoring/overview/#service-availability","text":"Checking the momentary service availability and dashboards gives confidence that all services are running correctly as expected. Example service checks that allow engineers to gain confidence in the service availability are: System process Network open ports - e.g. CQL and storage ports Database availability - e.g. can execute CQL query","title":"Service Availability"},{"location":"monitoring/overview/#axonops-monitoring","text":"AxonOps implements all three types of monitoring described above. AxonOps agent captures the information, sends them securely to AxonOps server, and the information is stored in the backend data store. AxonOps GUI provides comprehensive set of metrics dashboards combined with the event log view. It also provides separate service check status view showing the health of the cluster. This section describes how the AxonOps GUI organises the dashboards of all three types of monitoring.","title":"AxonOps Monitoring"},{"location":"monitoring/logsandevents/logsandevents/","text":"Logs and Events \u00b6 AxonOps provides a powerful logging feature that allows you to search and filter logs based on different parameters such as DC/Rack/Node, Log Level, Event Type, Source and Log Content. The logs and events are visible within AxonOps dashboard and Logs & Events tab allowing quick access to them without having to login to the individual servers. Search by Log Level \u00b6 Filter logs based on their log levels to focus on specific severity levels. The log level indicates the importance or severity of a message from the most critical (ERROR) to less severe (DEBUG). Setting up the Debug Level \u00b6 To search logs by debug level you have to enable debug mode in cassandra by editing the logback.xml file: <appender name=\"SYSTEMLOG\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"> <filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"> <level>DEBUG</level> Search by Logs Source and Event Type \u00b6 You can filter logs based on the log source (cassandra, axon-server and axon-agent logs) and event type to narrow down search results. Search by Content \u00b6 For a free text search enter a keyword in the content input or use the /<expression>/ syntax to search by regex expression. Here are some examples: Display logs that contain a specific word or phrase: Display logs that contain a match either what is before or after the |, in this case \"Validated\" or \"Compacted\": Display logs that contain both patterns in a line, in this case \"Segment\" and \"deleted:","title":"Logs & Events"},{"location":"monitoring/logsandevents/logsandevents/#logs-and-events","text":"AxonOps provides a powerful logging feature that allows you to search and filter logs based on different parameters such as DC/Rack/Node, Log Level, Event Type, Source and Log Content. The logs and events are visible within AxonOps dashboard and Logs & Events tab allowing quick access to them without having to login to the individual servers.","title":"Logs and Events"},{"location":"monitoring/logsandevents/logsandevents/#search-by-log-level","text":"Filter logs based on their log levels to focus on specific severity levels. The log level indicates the importance or severity of a message from the most critical (ERROR) to less severe (DEBUG).","title":"Search by Log Level"},{"location":"monitoring/logsandevents/logsandevents/#setting-up-the-debug-level","text":"To search logs by debug level you have to enable debug mode in cassandra by editing the logback.xml file: <appender name=\"SYSTEMLOG\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"> <filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"> <level>DEBUG</level>","title":"Setting up the Debug Level"},{"location":"monitoring/logsandevents/logsandevents/#search-by-logs-source-and-event-type","text":"You can filter logs based on the log source (cassandra, axon-server and axon-agent logs) and event type to narrow down search results.","title":"Search by Logs Source and Event Type"},{"location":"monitoring/logsandevents/logsandevents/#search-by-content","text":"For a free text search enter a keyword in the content input or use the /<expression>/ syntax to search by regex expression. Here are some examples: Display logs that contain a specific word or phrase: Display logs that contain a match either what is before or after the |, in this case \"Validated\" or \"Compacted\": Display logs that contain both patterns in a line, in this case \"Segment\" and \"deleted:","title":"Search by Content"},{"location":"monitoring/metricsdashboards/cassandra/","text":"AxonOps dashboards provides a comprehensive set of charts with an embedded view for logs and events. You can correlate metrics with logs/events as you can zoom in the logs histogram or metrics charts to drill down both results. Alert rules can be defined graphically in each chart and Log collection is defined in the bottom part of that page. Infomy","title":"Cassandra"},{"location":"monitoring/servicechecks/configurations/","text":"Adding Service Checks \u00b6 On the Axonops application menu, click Service Checks and select Setup tab. Creating Services \u00b6 > Note that for a Cassandra node, you can use the variable `{{.listen_address}}` which will correspond to Cassandra listening address. Example: !!! infomy [![servicecheckseditor](/img/servicecheckseditor.png)](/img/servicecheckseditor.png) #### Delete Services To Delete a service `copy`/`paste` into the editor and `click` save [![save](/img/disk.png)](/img/disk.png) ``` jsonld { \"shellchecks\": [], \"httpchecks\": [], \"tcpchecks\": [] } Example: Infomy","title":"Configurations"},{"location":"monitoring/servicechecks/configurations/#adding-service-checks","text":"On the Axonops application menu, click Service Checks and select Setup tab.","title":"Adding Service Checks"},{"location":"monitoring/servicechecks/configurations/#creating-services","text":"> Note that for a Cassandra node, you can use the variable `{{.listen_address}}` which will correspond to Cassandra listening address. Example: !!! infomy [![servicecheckseditor](/img/servicecheckseditor.png)](/img/servicecheckseditor.png) #### Delete Services To Delete a service `copy`/`paste` into the editor and `click` save [![save](/img/disk.png)](/img/disk.png) ``` jsonld { \"shellchecks\": [], \"httpchecks\": [], \"tcpchecks\": [] } Example: Infomy","title":"Creating Services"},{"location":"monitoring/servicechecks/notifications/","text":"Service checks will notify with one of the three statuses: Service Statuses. Success Warning Error Depending on the status of the service an appropriate alert will be sent. The alert will be sent based on the Default Routing that has been setup via the integrations menu. Noticed: If the Default Routing has not been set up no alerts will be sent. Service Alerts will be sent using the following rules. Info \u00b6 Default routing rules will be used to send success alerts Warning \u00b6 Default routing rules will be used to send warning alerts Error \u00b6 Default routing rules will be used to send error alerts","title":"Notifications"},{"location":"monitoring/servicechecks/notifications/#info","text":"Default routing rules will be used to send success alerts","title":"Info"},{"location":"monitoring/servicechecks/notifications/#warning","text":"Default routing rules will be used to send warning alerts","title":"Warning"},{"location":"monitoring/servicechecks/notifications/#error","text":"Default routing rules will be used to send error alerts","title":"Error"},{"location":"monitoring/servicechecks/overview/","text":"Overview \u00b6 Service Checks in AxonOps allows you to configure custom checks using three types of checks: Shell Scripts HTTP endpoint checks TCP endpoint checks The functionality is accessible via the Service Checks menu You can list the service checks by node : Or by services : You can click on a row within the node view to see all the services for that given Node . The following shows a successful check: And a failing check: Configure service checks \u00b6 To set up the checks, go to Settings > Service Checks and click one of the + buttons Any changes made and saved are automatically pushed down to the agents. There is no need to deploy the check scripts to individual servers like you may do for instance with Nagios. The status will show once the check has been executed on the agent, so it might take some time depending on the interval you have specified within the Service Checks. Although the first execution of the checks will be spread across 30 seconds to prevent running all the checks at the same time. Service checks templating \u00b6 You can provide templated checks with the following pattern: {{.variable_name}} {{.comp_listen_address}} will be replace with Cassandra listen address . For instance, port 7000 in the previous example for check storage port could be replaced with {{.comp_storage_port}} on a Cassandra cluster: endpoint: {{.comp_listen_address}}:{{.comp_storage_port}} Cassandra variables \u00b6 Here is the full list of variables that can be specified in any service check: agent_version comp_PROPERTY_PREFIX comp_SENSITIVE_KEYS comp_allocate_tokens_for_keyspace comp_authenticator comp_authorizer comp_auto_bootstrap comp_auto_snapshot comp_back_pressure_enabled comp_back_pressure_strategy comp_batch_size_fail_threshold_in_kb comp_batch_size_warn_threshold_in_kb comp_batchlog_replay_throttle_in_kb comp_broadcast_address comp_broadcast_rpc_address comp_buffer_pool_use_heap_if_exhausted comp_cas_contention_timeout_in_ms comp_cdc_enabled comp_cdc_free_space_check_interval_ms comp_cdc_raw_directory comp_cdc_total_space_in_mb comp_client_encryption_options comp_cluster_name comp_column_index_cache_size_in_kb comp_column_index_size_in_kb comp_commit_failure_policy comp_commitlog_compression comp_commitlog_directory comp_commitlog_max_compression_buffers_in_pool comp_commitlog_periodic_queue_size comp_commitlog_segment_size_in_mb comp_commitlog_sync comp_commitlog_sync_batch_window_in_ms comp_commitlog_sync_period_in_ms comp_commitlog_total_space_in_mb comp_compaction_large_partition_warning_threshold_mb comp_compaction_throughput_mb_per_sec comp_concurrent_compactors comp_concurrent_counter_writes comp_concurrent_materialized_view_writes comp_concurrent_reads comp_concurrent_replicates comp_concurrent_writes comp_counter_cache_keys_to_save comp_counter_cache_save_period comp_counter_cache_size_in_mb comp_counter_write_request_timeout_in_ms comp_credentials_cache_max_entries comp_credentials_update_interval_in_ms comp_credentials_validity_in_ms comp_cross_node_timeout comp_data_file_directories comp_dc comp_disk_access_mode comp_disk_failure_policy comp_disk_optimization_estimate_percentile comp_disk_optimization_page_cross_chance comp_disk_optimization_strategy comp_dynamic_snitch comp_dynamic_snitch_badness_threshold comp_dynamic_snitch_reset_interval_in_ms comp_dynamic_snitch_update_interval_in_ms comp_enable_materialized_views comp_enable_scripted_user_defined_functions comp_enable_user_defined_functions comp_enable_user_defined_functions_threads comp_encryption_options comp_endpoint_snitch comp_file_cache_round_up comp_file_cache_size_in_mb comp_gc_log_threshold_in_ms comp_gc_warn_threshold_in_ms comp_hinted_handoff_disabled_datacenters comp_hinted_handoff_enabled comp_hinted_handoff_throttle_in_kb comp_hints_compression comp_hints_directory comp_hints_flush_period_in_ms comp_hostId comp_incremental_backups comp_index_interval comp_index_summary_capacity_in_mb comp_index_summary_resize_interval_in_minutes comp_initial_token comp_inter_dc_stream_throughput_outbound_megabits_per_sec comp_inter_dc_tcp_nodelay comp_internode_authenticator comp_internode_compression comp_internode_recv_buff_size_in_bytes comp_internode_send_buff_size_in_bytes comp_isClientMode comp_jvm_VM name comp_jvm_VM vendor comp_jvm_VM version comp_jvm_awt.toolkit comp_jvm_boot classpath comp_jvm_cassandra-foreground comp_jvm_cassandra.config comp_jvm_cassandra.jmx.local.port comp_jvm_cassandra.native.epoll.enabled comp_jvm_com.sun.management.jmxremote.ssl comp_jvm_file.encoding comp_jvm_file.encoding.pkg comp_jvm_file.separator comp_jvm_gc_G1 Old Generation_collection count comp_jvm_gc_G1 Old Generation_collection time comp_jvm_gc_G1 Old Generation_memory pool names comp_jvm_gc_G1 Young Generation_collection count comp_jvm_gc_G1 Young Generation_collection time comp_jvm_gc_G1 Young Generation_memory pool names comp_jvm_heap_heapFreeSize comp_jvm_heap_heapMaxSize comp_jvm_heap_heapSize comp_jvm_input arguments comp_jvm_io.netty.native.workdir comp_jvm_java.awt.graphicsenv comp_jvm_java.awt.printerjob comp_jvm_java.class.path comp_jvm_java.class.version comp_jvm_java.endorsed.dirs comp_jvm_java.ext.dirs comp_jvm_java.home comp_jvm_java.io.tmpdir comp_jvm_java.library.path comp_jvm_java.rmi.server.hostname comp_jvm_java.rmi.server.randomIDs comp_jvm_java.runtime.name comp_jvm_java.runtime.version comp_jvm_java.specification.name comp_jvm_java.specification.vendor comp_jvm_java.specification.version comp_jvm_java.util.logging.SimpleFormatter.format comp_jvm_java.vendor comp_jvm_java.vendor.url comp_jvm_java.vendor.url.bug comp_jvm_java.version comp_jvm_java.vm.info comp_jvm_java.vm.name comp_jvm_java.vm.specification.name comp_jvm_java.vm.specification.vendor comp_jvm_java.vm.specification.version comp_jvm_java.vm.vendor comp_jvm_java.vm.version comp_jvm_jna.loaded comp_jvm_jna.platform.library.path comp_jvm_jnidispatch.path comp_jvm_library classpath comp_jvm_line.separator comp_jvm_log4j.configuration comp_jvm_management spec version comp_jvm_name comp_jvm_os.arch comp_jvm_os.name comp_jvm_os.version comp_jvm_path.separator comp_jvm_spec name comp_jvm_spec vendor comp_jvm_start time comp_jvm_sun.arch.data.model comp_jvm_sun.boot.class.path comp_jvm_sun.boot.library.path comp_jvm_sun.cpu.endian comp_jvm_sun.cpu.isalist comp_jvm_sun.io.unicode.encoding comp_jvm_sun.java.command comp_jvm_sun.java.launcher comp_jvm_sun.jnu.encoding comp_jvm_sun.management.compiler comp_jvm_sun.nio.ch.bugLevel comp_jvm_sun.os.patch.level comp_jvm_up time comp_jvm_user.country comp_jvm_user.dir comp_jvm_user.home comp_jvm_user.language comp_jvm_user.name comp_jvm_user.timezone comp_jvm_user.variant comp_key_cache_keys_to_save comp_key_cache_save_period comp_key_cache_size_in_mb comp_listen_address comp_listen_interface comp_listen_interface_prefer_ipv6 comp_listen_on_broadcast_address comp_logger comp_max_file_descriptors comp_max_hint_window_in_ms comp_max_hints_delivery_threads comp_max_hints_file_size_in_mb comp_max_mutation_size_in_kb comp_max_streaming_retries comp_max_value_size_in_mb comp_memtable_allocation_type comp_memtable_cleanup_threshold comp_memtable_flush_writers comp_memtable_heap_space_in_mb comp_memtable_offheap_space_in_mb comp_min_free_space_per_drive_in_mb comp_mode comp_native_transport_max_concurrent_connections comp_native_transport_max_concurrent_connections_per_ip comp_native_transport_max_frame_size_in_mb comp_native_transport_max_threads comp_native_transport_port comp_native_transport_port_ssl comp_num_tokens comp_open_file_descriptors comp_otc_backlog_expiration_interval_ms comp_otc_backlog_expiration_interval_ms_default comp_otc_coalescing_enough_coalesced_messages comp_otc_coalescing_strategy comp_otc_coalescing_window_us comp_otc_coalescing_window_us_default comp_ownership comp_partitioner comp_permissions_cache_max_entries comp_permissions_update_interval_in_ms comp_permissions_validity_in_ms comp_phi_convict_threshold comp_prepared_statements_cache_size_mb comp_rack comp_range_request_timeout_in_ms comp_read_request_timeout_in_ms comp_releaseVersion comp_request_scheduler comp_request_scheduler_id comp_request_scheduler_options comp_request_timeout_in_ms comp_role_manager comp_roles_cache_max_entries comp_roles_update_interval_in_ms comp_roles_validity_in_ms comp_row_cache_class_name comp_row_cache_keys_to_save comp_row_cache_save_period comp_row_cache_size_in_mb comp_rpc_address comp_rpc_interface comp_rpc_interface_prefer_ipv6 comp_rpc_keepalive comp_rpc_listen_backlog comp_rpc_max_threads comp_rpc_min_threads comp_rpc_port comp_rpc_recv_buff_size_in_bytes comp_rpc_send_buff_size_in_bytes comp_rpc_server_type comp_saved_caches_directory comp_schemaVersion comp_seed_provider comp_server_encryption_options comp_slow_query_log_timeout_in_ms comp_snapshot_before_compaction comp_ssl_storage_port comp_sstable_preemptive_open_interval_in_mb comp_start_native_transport comp_start_rpc comp_storage_port comp_stream_throughput_outbound_megabits_per_sec comp_streaming_keep_alive_period_in_secs comp_streaming_socket_timeout_in_ms comp_thrift_framed_transport_size_in_mb comp_thrift_max_message_length_in_mb comp_thrift_prepared_statements_cache_size_mb comp_tombstone_failure_threshold comp_tombstone_warn_threshold comp_tracetype_query_ttl comp_tracetype_repair_ttl comp_transparent_data_encryption_options comp_trickle_fsync comp_trickle_fsync_interval_in_kb comp_truncate_request_timeout_in_ms comp_unlogged_batch_across_partitions_warn_threshold comp_user_defined_function_fail_timeout comp_user_defined_function_warn_timeout comp_user_function_timeout_policy comp_windows_timer_interval comp_write_request_timeout_in_ms host_BootTime host_Ctxt host_HostID host_Hostname host_KernelArch host_KernelVersion host_OS host_Platform host_PlatformFamily host_PlatformVersion host_Procs host_ProcsBlocked host_ProcsRunning host_ProcsTotal host_Uptime host_VirtualizationRole host_VirtualizationSystem host_cpu_CPU host_cpu_CacheSize host_cpu_CoreID host_cpu_Cores host_cpu_Family host_cpu_Flags host_cpu_Mhz host_cpu_Microcode host_cpu_Model host_cpu_ModelName host_cpu_PhysicalID host_cpu_Stepping host_cpu_VendorID host_disk_/_Free host_disk_/_Total host_disk_/_Used host_disk_/_fstype host_swapmem_Free host_swapmem_PgFault host_swapmem_PgIn host_swapmem_PgMajFault host_swapmem_PgOut host_swapmem_Sin host_swapmem_Sout host_swapmem_Total host_swapmem_Used host_swapmem_UsedPercent host_virtualmem_Active host_virtualmem_Available host_virtualmem_Buffers host_virtualmem_Cached host_virtualmem_CommitLimit host_virtualmem_CommittedAS host_virtualmem_Dirty host_virtualmem_Free host_virtualmem_HighFree host_virtualmem_HighTotal host_virtualmem_HugePageSize host_virtualmem_HugePagesFree host_virtualmem_HugePagesTotal host_virtualmem_Inactive host_virtualmem_Laundry host_virtualmem_LowFree host_virtualmem_LowTotal host_virtualmem_Mapped host_virtualmem_PageTables host_virtualmem_SReclaimable host_virtualmem_SUnreclaim host_virtualmem_Shared host_virtualmem_Slab host_virtualmem_SwapCached host_virtualmem_SwapFree host_virtualmem_SwapTotal host_virtualmem_Total host_virtualmem_Used host_virtualmem_UsedPercent host_virtualmem_VMallocChunk host_virtualmem_VMallocTotal host_virtualmem_VMallocUsed host_virtualmem_Wired host_virtualmem_Writeback host_virtualmem_WritebackTmp human_readable_identifier human_readable_identifier_field","title":"Service Checks"},{"location":"monitoring/servicechecks/overview/#overview","text":"Service Checks in AxonOps allows you to configure custom checks using three types of checks: Shell Scripts HTTP endpoint checks TCP endpoint checks The functionality is accessible via the Service Checks menu You can list the service checks by node : Or by services : You can click on a row within the node view to see all the services for that given Node . The following shows a successful check: And a failing check:","title":"Overview"},{"location":"monitoring/servicechecks/overview/#configure-service-checks","text":"To set up the checks, go to Settings > Service Checks and click one of the + buttons Any changes made and saved are automatically pushed down to the agents. There is no need to deploy the check scripts to individual servers like you may do for instance with Nagios. The status will show once the check has been executed on the agent, so it might take some time depending on the interval you have specified within the Service Checks. Although the first execution of the checks will be spread across 30 seconds to prevent running all the checks at the same time.","title":"Configure service checks"},{"location":"monitoring/servicechecks/overview/#service-checks-templating","text":"You can provide templated checks with the following pattern: {{.variable_name}} {{.comp_listen_address}} will be replace with Cassandra listen address . For instance, port 7000 in the previous example for check storage port could be replaced with {{.comp_storage_port}} on a Cassandra cluster: endpoint: {{.comp_listen_address}}:{{.comp_storage_port}}","title":"Service checks templating"},{"location":"monitoring/servicechecks/overview/#cassandra-variables","text":"Here is the full list of variables that can be specified in any service check: agent_version comp_PROPERTY_PREFIX comp_SENSITIVE_KEYS comp_allocate_tokens_for_keyspace comp_authenticator comp_authorizer comp_auto_bootstrap comp_auto_snapshot comp_back_pressure_enabled comp_back_pressure_strategy comp_batch_size_fail_threshold_in_kb comp_batch_size_warn_threshold_in_kb comp_batchlog_replay_throttle_in_kb comp_broadcast_address comp_broadcast_rpc_address comp_buffer_pool_use_heap_if_exhausted comp_cas_contention_timeout_in_ms comp_cdc_enabled comp_cdc_free_space_check_interval_ms comp_cdc_raw_directory comp_cdc_total_space_in_mb comp_client_encryption_options comp_cluster_name comp_column_index_cache_size_in_kb comp_column_index_size_in_kb comp_commit_failure_policy comp_commitlog_compression comp_commitlog_directory comp_commitlog_max_compression_buffers_in_pool comp_commitlog_periodic_queue_size comp_commitlog_segment_size_in_mb comp_commitlog_sync comp_commitlog_sync_batch_window_in_ms comp_commitlog_sync_period_in_ms comp_commitlog_total_space_in_mb comp_compaction_large_partition_warning_threshold_mb comp_compaction_throughput_mb_per_sec comp_concurrent_compactors comp_concurrent_counter_writes comp_concurrent_materialized_view_writes comp_concurrent_reads comp_concurrent_replicates comp_concurrent_writes comp_counter_cache_keys_to_save comp_counter_cache_save_period comp_counter_cache_size_in_mb comp_counter_write_request_timeout_in_ms comp_credentials_cache_max_entries comp_credentials_update_interval_in_ms comp_credentials_validity_in_ms comp_cross_node_timeout comp_data_file_directories comp_dc comp_disk_access_mode comp_disk_failure_policy comp_disk_optimization_estimate_percentile comp_disk_optimization_page_cross_chance comp_disk_optimization_strategy comp_dynamic_snitch comp_dynamic_snitch_badness_threshold comp_dynamic_snitch_reset_interval_in_ms comp_dynamic_snitch_update_interval_in_ms comp_enable_materialized_views comp_enable_scripted_user_defined_functions comp_enable_user_defined_functions comp_enable_user_defined_functions_threads comp_encryption_options comp_endpoint_snitch comp_file_cache_round_up comp_file_cache_size_in_mb comp_gc_log_threshold_in_ms comp_gc_warn_threshold_in_ms comp_hinted_handoff_disabled_datacenters comp_hinted_handoff_enabled comp_hinted_handoff_throttle_in_kb comp_hints_compression comp_hints_directory comp_hints_flush_period_in_ms comp_hostId comp_incremental_backups comp_index_interval comp_index_summary_capacity_in_mb comp_index_summary_resize_interval_in_minutes comp_initial_token comp_inter_dc_stream_throughput_outbound_megabits_per_sec comp_inter_dc_tcp_nodelay comp_internode_authenticator comp_internode_compression comp_internode_recv_buff_size_in_bytes comp_internode_send_buff_size_in_bytes comp_isClientMode comp_jvm_VM name comp_jvm_VM vendor comp_jvm_VM version comp_jvm_awt.toolkit comp_jvm_boot classpath comp_jvm_cassandra-foreground comp_jvm_cassandra.config comp_jvm_cassandra.jmx.local.port comp_jvm_cassandra.native.epoll.enabled comp_jvm_com.sun.management.jmxremote.ssl comp_jvm_file.encoding comp_jvm_file.encoding.pkg comp_jvm_file.separator comp_jvm_gc_G1 Old Generation_collection count comp_jvm_gc_G1 Old Generation_collection time comp_jvm_gc_G1 Old Generation_memory pool names comp_jvm_gc_G1 Young Generation_collection count comp_jvm_gc_G1 Young Generation_collection time comp_jvm_gc_G1 Young Generation_memory pool names comp_jvm_heap_heapFreeSize comp_jvm_heap_heapMaxSize comp_jvm_heap_heapSize comp_jvm_input arguments comp_jvm_io.netty.native.workdir comp_jvm_java.awt.graphicsenv comp_jvm_java.awt.printerjob comp_jvm_java.class.path comp_jvm_java.class.version comp_jvm_java.endorsed.dirs comp_jvm_java.ext.dirs comp_jvm_java.home comp_jvm_java.io.tmpdir comp_jvm_java.library.path comp_jvm_java.rmi.server.hostname comp_jvm_java.rmi.server.randomIDs comp_jvm_java.runtime.name comp_jvm_java.runtime.version comp_jvm_java.specification.name comp_jvm_java.specification.vendor comp_jvm_java.specification.version comp_jvm_java.util.logging.SimpleFormatter.format comp_jvm_java.vendor comp_jvm_java.vendor.url comp_jvm_java.vendor.url.bug comp_jvm_java.version comp_jvm_java.vm.info comp_jvm_java.vm.name comp_jvm_java.vm.specification.name comp_jvm_java.vm.specification.vendor comp_jvm_java.vm.specification.version comp_jvm_java.vm.vendor comp_jvm_java.vm.version comp_jvm_jna.loaded comp_jvm_jna.platform.library.path comp_jvm_jnidispatch.path comp_jvm_library classpath comp_jvm_line.separator comp_jvm_log4j.configuration comp_jvm_management spec version comp_jvm_name comp_jvm_os.arch comp_jvm_os.name comp_jvm_os.version comp_jvm_path.separator comp_jvm_spec name comp_jvm_spec vendor comp_jvm_start time comp_jvm_sun.arch.data.model comp_jvm_sun.boot.class.path comp_jvm_sun.boot.library.path comp_jvm_sun.cpu.endian comp_jvm_sun.cpu.isalist comp_jvm_sun.io.unicode.encoding comp_jvm_sun.java.command comp_jvm_sun.java.launcher comp_jvm_sun.jnu.encoding comp_jvm_sun.management.compiler comp_jvm_sun.nio.ch.bugLevel comp_jvm_sun.os.patch.level comp_jvm_up time comp_jvm_user.country comp_jvm_user.dir comp_jvm_user.home comp_jvm_user.language comp_jvm_user.name comp_jvm_user.timezone comp_jvm_user.variant comp_key_cache_keys_to_save comp_key_cache_save_period comp_key_cache_size_in_mb comp_listen_address comp_listen_interface comp_listen_interface_prefer_ipv6 comp_listen_on_broadcast_address comp_logger comp_max_file_descriptors comp_max_hint_window_in_ms comp_max_hints_delivery_threads comp_max_hints_file_size_in_mb comp_max_mutation_size_in_kb comp_max_streaming_retries comp_max_value_size_in_mb comp_memtable_allocation_type comp_memtable_cleanup_threshold comp_memtable_flush_writers comp_memtable_heap_space_in_mb comp_memtable_offheap_space_in_mb comp_min_free_space_per_drive_in_mb comp_mode comp_native_transport_max_concurrent_connections comp_native_transport_max_concurrent_connections_per_ip comp_native_transport_max_frame_size_in_mb comp_native_transport_max_threads comp_native_transport_port comp_native_transport_port_ssl comp_num_tokens comp_open_file_descriptors comp_otc_backlog_expiration_interval_ms comp_otc_backlog_expiration_interval_ms_default comp_otc_coalescing_enough_coalesced_messages comp_otc_coalescing_strategy comp_otc_coalescing_window_us comp_otc_coalescing_window_us_default comp_ownership comp_partitioner comp_permissions_cache_max_entries comp_permissions_update_interval_in_ms comp_permissions_validity_in_ms comp_phi_convict_threshold comp_prepared_statements_cache_size_mb comp_rack comp_range_request_timeout_in_ms comp_read_request_timeout_in_ms comp_releaseVersion comp_request_scheduler comp_request_scheduler_id comp_request_scheduler_options comp_request_timeout_in_ms comp_role_manager comp_roles_cache_max_entries comp_roles_update_interval_in_ms comp_roles_validity_in_ms comp_row_cache_class_name comp_row_cache_keys_to_save comp_row_cache_save_period comp_row_cache_size_in_mb comp_rpc_address comp_rpc_interface comp_rpc_interface_prefer_ipv6 comp_rpc_keepalive comp_rpc_listen_backlog comp_rpc_max_threads comp_rpc_min_threads comp_rpc_port comp_rpc_recv_buff_size_in_bytes comp_rpc_send_buff_size_in_bytes comp_rpc_server_type comp_saved_caches_directory comp_schemaVersion comp_seed_provider comp_server_encryption_options comp_slow_query_log_timeout_in_ms comp_snapshot_before_compaction comp_ssl_storage_port comp_sstable_preemptive_open_interval_in_mb comp_start_native_transport comp_start_rpc comp_storage_port comp_stream_throughput_outbound_megabits_per_sec comp_streaming_keep_alive_period_in_secs comp_streaming_socket_timeout_in_ms comp_thrift_framed_transport_size_in_mb comp_thrift_max_message_length_in_mb comp_thrift_prepared_statements_cache_size_mb comp_tombstone_failure_threshold comp_tombstone_warn_threshold comp_tracetype_query_ttl comp_tracetype_repair_ttl comp_transparent_data_encryption_options comp_trickle_fsync comp_trickle_fsync_interval_in_kb comp_truncate_request_timeout_in_ms comp_unlogged_batch_across_partitions_warn_threshold comp_user_defined_function_fail_timeout comp_user_defined_function_warn_timeout comp_user_function_timeout_policy comp_windows_timer_interval comp_write_request_timeout_in_ms host_BootTime host_Ctxt host_HostID host_Hostname host_KernelArch host_KernelVersion host_OS host_Platform host_PlatformFamily host_PlatformVersion host_Procs host_ProcsBlocked host_ProcsRunning host_ProcsTotal host_Uptime host_VirtualizationRole host_VirtualizationSystem host_cpu_CPU host_cpu_CacheSize host_cpu_CoreID host_cpu_Cores host_cpu_Family host_cpu_Flags host_cpu_Mhz host_cpu_Microcode host_cpu_Model host_cpu_ModelName host_cpu_PhysicalID host_cpu_Stepping host_cpu_VendorID host_disk_/_Free host_disk_/_Total host_disk_/_Used host_disk_/_fstype host_swapmem_Free host_swapmem_PgFault host_swapmem_PgIn host_swapmem_PgMajFault host_swapmem_PgOut host_swapmem_Sin host_swapmem_Sout host_swapmem_Total host_swapmem_Used host_swapmem_UsedPercent host_virtualmem_Active host_virtualmem_Available host_virtualmem_Buffers host_virtualmem_Cached host_virtualmem_CommitLimit host_virtualmem_CommittedAS host_virtualmem_Dirty host_virtualmem_Free host_virtualmem_HighFree host_virtualmem_HighTotal host_virtualmem_HugePageSize host_virtualmem_HugePagesFree host_virtualmem_HugePagesTotal host_virtualmem_Inactive host_virtualmem_Laundry host_virtualmem_LowFree host_virtualmem_LowTotal host_virtualmem_Mapped host_virtualmem_PageTables host_virtualmem_SReclaimable host_virtualmem_SUnreclaim host_virtualmem_Shared host_virtualmem_Slab host_virtualmem_SwapCached host_virtualmem_SwapFree host_virtualmem_SwapTotal host_virtualmem_Total host_virtualmem_Used host_virtualmem_UsedPercent host_virtualmem_VMallocChunk host_virtualmem_VMallocTotal host_virtualmem_VMallocUsed host_virtualmem_Wired host_virtualmem_Writeback host_virtualmem_WritebackTmp human_readable_identifier human_readable_identifier_field","title":"Cassandra variables"},{"location":"operations/cassandra/repair/","text":"Repairs must be completed regularly to maintain Cassandra nodes. AxonOps provide two mechanisms to ease Cassandra repairs: Scheduled repair Adaptive repair service Scheduled repair \u00b6 You can initiate three types of scheduled repair: Immediate scheduled repair: these will trigger immediately once Infomy Simple scheduled repair: these will trigger base on the selected schedule repeatedly Infomy Cron schedule repair: Same as simple scheduled repair but the schedule will be based on a Cron expression Infomy The following capture presents a running repair that has been initiated immediately and a scheduled repair that is scheduled for 12:00 AM UTC: Infomy Adaptive repair service \u00b6 Since AxonOps collects performance metrics and logs, we built an \u201cAdaptive\u201d repair system which regulates the velocity (parallelism and pauses between each subrange repair) based on performance trending data. The regulation of repair velocity takes input from various metrics including CPU utilisation, query latencies, Cassandra thread pools pending statistics, and IOwait percentage, while tracking the schedule of repair based on gc_grace_seconds for each table. The idea of this is to achieve the following: Completion of repair within gc_grace_seconds of each table. Repair process does not affect query performance. In essence, adaptive repair regulator slows down the repair velocity when it deems the load is going to be high based on the gradient of the rate of increase of load, and speeds up to catch up with the repair schedule when the resources are more readily available. This mechanism also doesn't require JMX access. The adaptive repair service running on AxonOps server orchestrates and issues commands to the agents over the existing connection. Infomy If you want to keep the tables as fresh as possible we recommend to increase the table parallelism to be greater than the total number of tables of your cluster and reduce the segments per VNode to generate less repair requests. From a user\u2019s point of view there is only a single switch to enable this service. Keep this enabled and AxonOps will take care of the repair of all tables for you. You can also customize the following: Blacklist some tables Specify the number of tables to repair in parallel Specify the number of segments per VNode to repair The GC grace threshold in seconds: if a table has a gc grace lesser than the specified value, it will be ignored from the adaptive repair service","title":"Repair"},{"location":"operations/cassandra/repair/#scheduled-repair","text":"You can initiate three types of scheduled repair: Immediate scheduled repair: these will trigger immediately once Infomy Simple scheduled repair: these will trigger base on the selected schedule repeatedly Infomy Cron schedule repair: Same as simple scheduled repair but the schedule will be based on a Cron expression Infomy The following capture presents a running repair that has been initiated immediately and a scheduled repair that is scheduled for 12:00 AM UTC: Infomy","title":"Scheduled repair"},{"location":"operations/cassandra/repair/#adaptive-repair-service","text":"Since AxonOps collects performance metrics and logs, we built an \u201cAdaptive\u201d repair system which regulates the velocity (parallelism and pauses between each subrange repair) based on performance trending data. The regulation of repair velocity takes input from various metrics including CPU utilisation, query latencies, Cassandra thread pools pending statistics, and IOwait percentage, while tracking the schedule of repair based on gc_grace_seconds for each table. The idea of this is to achieve the following: Completion of repair within gc_grace_seconds of each table. Repair process does not affect query performance. In essence, adaptive repair regulator slows down the repair velocity when it deems the load is going to be high based on the gradient of the rate of increase of load, and speeds up to catch up with the repair schedule when the resources are more readily available. This mechanism also doesn't require JMX access. The adaptive repair service running on AxonOps server orchestrates and issues commands to the agents over the existing connection. Infomy If you want to keep the tables as fresh as possible we recommend to increase the table parallelism to be greater than the total number of tables of your cluster and reduce the segments per VNode to generate less repair requests. From a user\u2019s point of view there is only a single switch to enable this service. Keep this enabled and AxonOps will take care of the repair of all tables for you. You can also customize the following: Blacklist some tables Specify the number of tables to repair in parallel Specify the number of segments per VNode to repair The GC grace threshold in seconds: if a table has a gc grace lesser than the specified value, it will be ignored from the adaptive repair service","title":"Adaptive repair service"},{"location":"operations/cassandra/backup/overview/","text":"AxonOps provides scheduled backup and restore functionality for your Cassandra data. The functionality is accessible via Operations > Backups Infomy Scheduled backup \u00b6 You can initiate three types of scheduled backup: Immediate scheduled backup: these will trigger immediately once Simple scheduled backup: these will trigger based on the selected schedule repeatedly Infomy Cron schedule backup: Same as simple scheduled backup but the schedule will be based on a Cron expression Infomy The following capture presents two backups, a local only and a local and remote backup: Infomy And the details of the local and remote backup: Infomy Remote backups \u00b6 Note that axonops user will need read access on Cassandra data folders to be able to perform a remote backup. The available remote options are: AWS S3 Google Cloud Storage Microsoft Azure Blob Storage SFTP/SSH local filesystem example of the AWS S3 remote interface: Infomy","title":"Backups Overview"},{"location":"operations/cassandra/backup/overview/#scheduled-backup","text":"You can initiate three types of scheduled backup: Immediate scheduled backup: these will trigger immediately once Simple scheduled backup: these will trigger based on the selected schedule repeatedly Infomy Cron schedule backup: Same as simple scheduled backup but the schedule will be based on a Cron expression Infomy The following capture presents two backups, a local only and a local and remote backup: Infomy And the details of the local and remote backup: Infomy","title":"Scheduled backup"},{"location":"operations/cassandra/backup/overview/#remote-backups","text":"Note that axonops user will need read access on Cassandra data folders to be able to perform a remote backup. The available remote options are: AWS S3 Google Cloud Storage Microsoft Azure Blob Storage SFTP/SSH local filesystem example of the AWS S3 remote interface: Infomy","title":"Remote backups"},{"location":"operations/cassandra/restore/overview/","text":"AxonOps provides the ability to restore from local snapshots and remote backups. The Restore feature is accessible via Operations > Restore Infomy Note that axonops user will need temporary write access on Cassandra data folders to be able to perform the restoration. To restore Cassandra, click on the backup you wish to restore. This will provide the details of that backup and the ability to start the restoration by clicking the LOCAL RESTORE or REMOTE RESTORE button depending on if you prefer to restore from the local snapshot or the remote backup (if remote backups were configured). Here you can also select a subset of nodes to restore via the checkboxes in the Nodes list. Infomy Follow the links below for some more detailed backup restore scenarios Restore a single node - same IP address Replace a node - different IP address Restore whole cluster - same IP addresses","title":"Overview"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/","text":"Restore a whole cluster from a remote backup with different IP addresses \u00b6 Follow this procedure when you have lost all nodes in a cluster and they have been recreated in the same topology (Cluster, DC and rack names are all the same and the same number of nodes in each) but the replacement nodes have different IP addresses from the original cluster. NOTE: This process is for disaster recovery and cannot be used to clone a backup to a different cluster Prepare the cluster for restoring a backup \u00b6 Before you start, ensure that Cassandra is stopped on all replacement nodes and that their data directories are empty sudo systemctl stop cassandra sudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/* Allow the AxonOps user to write to the Cassandra data directory on all nodes sudo chmod -R g+w /var/lib/cassandra/data The commands above assume you are storing the Cassandra data in the default location /var/lib/cassandra , you will need to change the paths shown if your data is stored at a different location As the IP addresses of the replacement Cassandra nodes are different to the old cluster you will need to update the seeds list in cassandra.yaml to point to the IPs of the nodes that are replacing the old seeds. For package-based installations (RPM or DEB) you can find this in /etc/cassandra/cassandra.yaml or for tarball installations it should be in <install_path>/conf/cassandra.yaml . Manually configure the AxonOps Agent host IDs \u00b6 AxonOps identifies nodes by a unique host ID which is assigned when the agent starts up. In order to restore a backup to a node with a different IP address you must manually assign the AxonOps host ID of the old node to its new replacement. In order to restore the whole cluster from a backup you will need to apply the old AxonOps host ID to all nodes in the replacement cluster. The host ID of the old node can be found on the Cluster Overview page of the AxonOps dashboard Infomy If you still have access to the old server or its data then its host ID can also be found in the file /var/lib/axonops/hostId Apply the old node's host ID to its replacement \u00b6 Ensure the AxonOps Agent is stopped and clear any data that it may have created on startup sudo systemctl stop axon-agent sudo rm -rf /var/lib/axonops/* Manually apply the old node's host ID on the replacement (replace the host ID shown with your host ID from the previous steps) echo '24d0cbf9-3b5a-11ed-8433-16b3c6a7bcc5' | sudo tee /var/lib/axonops/hostId sudo chown axonops.axonops /var/lib/axonops/hostId Start the AxonOps agent sudo systemctl start axon-agent In the AxonOps dashboard you should see the replacement nodes start up and take over from the old nodes after a few minutes. Restore the backup \u00b6 Open the Restore page in the AxonOps Dashboard by going to Operations > Restore Infomy Choose the backup you wish to restore from the list and click the RESTORE button This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list. Infomy Select all nodes in the checkbox list then start the restore by clicking the REMOTE RESTORE button. The restore progress will be displayed in the Backup Restorations in Progress list Infomy After the restore operation has completed successfully, fix the ownership and permissions on the Cassandra data directories on all nodes in the cluster sudo chown -R cassandra.cassandra /var/lib/cassandra/data sudo chmod -R g-w /var/lib/cassandra/data Start cassandra on the restored nodes one at a time, starting with the seeds first sudo systemctl start cassandra After the whole cluster is started up you should be able to see the replaced nodes with their new IP addresses in the output of nodetool status . You may still see the IP addresses of the old cluster nodes in the output of nodetool gossipinfo , these should clear out automatically after a few days or they can be manually tidied up by performing a rolling restart of the cluster.","title":"Whole cluster with changed IP addresses"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/#restore-a-whole-cluster-from-a-remote-backup-with-different-ip-addresses","text":"Follow this procedure when you have lost all nodes in a cluster and they have been recreated in the same topology (Cluster, DC and rack names are all the same and the same number of nodes in each) but the replacement nodes have different IP addresses from the original cluster. NOTE: This process is for disaster recovery and cannot be used to clone a backup to a different cluster","title":"Restore a whole cluster from a remote backup with different IP addresses"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/#prepare-the-cluster-for-restoring-a-backup","text":"Before you start, ensure that Cassandra is stopped on all replacement nodes and that their data directories are empty sudo systemctl stop cassandra sudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/* Allow the AxonOps user to write to the Cassandra data directory on all nodes sudo chmod -R g+w /var/lib/cassandra/data The commands above assume you are storing the Cassandra data in the default location /var/lib/cassandra , you will need to change the paths shown if your data is stored at a different location As the IP addresses of the replacement Cassandra nodes are different to the old cluster you will need to update the seeds list in cassandra.yaml to point to the IPs of the nodes that are replacing the old seeds. For package-based installations (RPM or DEB) you can find this in /etc/cassandra/cassandra.yaml or for tarball installations it should be in <install_path>/conf/cassandra.yaml .","title":"Prepare the cluster for restoring a backup"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/#manually-configure-the-axonops-agent-host-ids","text":"AxonOps identifies nodes by a unique host ID which is assigned when the agent starts up. In order to restore a backup to a node with a different IP address you must manually assign the AxonOps host ID of the old node to its new replacement. In order to restore the whole cluster from a backup you will need to apply the old AxonOps host ID to all nodes in the replacement cluster. The host ID of the old node can be found on the Cluster Overview page of the AxonOps dashboard Infomy If you still have access to the old server or its data then its host ID can also be found in the file /var/lib/axonops/hostId","title":"Manually configure the AxonOps Agent host IDs"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/#apply-the-old-nodes-host-id-to-its-replacement","text":"Ensure the AxonOps Agent is stopped and clear any data that it may have created on startup sudo systemctl stop axon-agent sudo rm -rf /var/lib/axonops/* Manually apply the old node's host ID on the replacement (replace the host ID shown with your host ID from the previous steps) echo '24d0cbf9-3b5a-11ed-8433-16b3c6a7bcc5' | sudo tee /var/lib/axonops/hostId sudo chown axonops.axonops /var/lib/axonops/hostId Start the AxonOps agent sudo systemctl start axon-agent In the AxonOps dashboard you should see the replacement nodes start up and take over from the old nodes after a few minutes.","title":"Apply the old node's host ID to its replacement"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/#restore-the-backup","text":"Open the Restore page in the AxonOps Dashboard by going to Operations > Restore Infomy Choose the backup you wish to restore from the list and click the RESTORE button This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list. Infomy Select all nodes in the checkbox list then start the restore by clicking the REMOTE RESTORE button. The restore progress will be displayed in the Backup Restorations in Progress list Infomy After the restore operation has completed successfully, fix the ownership and permissions on the Cassandra data directories on all nodes in the cluster sudo chown -R cassandra.cassandra /var/lib/cassandra/data sudo chmod -R g-w /var/lib/cassandra/data Start cassandra on the restored nodes one at a time, starting with the seeds first sudo systemctl start cassandra After the whole cluster is started up you should be able to see the replaced nodes with their new IP addresses in the output of nodetool status . You may still see the IP addresses of the old cluster nodes in the output of nodetool gossipinfo , these should clear out automatically after a few days or they can be manually tidied up by performing a rolling restart of the cluster.","title":"Restore the backup"},{"location":"operations/cassandra/restore/restore-cluster-same-ip/","text":"Restore a whole cluster from a remote backup \u00b6 Follow this procedure when you have lost all nodes in a cluster but they have been recreated in the same cluster topology (Cluster, DC and rack names are all the same and the same number of nodes in each) and the replacement nodes have the same IP addresses as the original cluster. Ensure that Cassandra is stopped on all nodes and that its data directories are empty sudo systemctl stop cassandra sudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/* Allow the AxonOps user to write to the Cassandra data directory sudo chmod -R g+w /var/lib/cassandra/data These commands assume you are storing the Cassandra data in the default location /var/lib/cassandra/ , you will need to change the paths shown if your data is stored at a different location Start the AxonOps agent on all nodes sudo systemctl start axon-agent Now open the Restore page in the AxonOps Dashboard by going to Operations > Restore Infomy Choose the backup you wish to restore from the list and click the RESTORE button This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list. Infomy Select all nodes in the checkbox list then start the restore by clicking the REMOTE RESTORE button. The restore progress will be displayed in the Backup Restorations in Progress list Infomy After the restore operation has completed successfully, fix the ownership and permissions on the Cassandra data directories on all nodes in the cluster sudo chown -R cassandra.cassandra /var/lib/cassandra/data sudo chmod -R g-w /var/lib/cassandra/data Start cassandra on the restored nodes, starting with the seeds first sudo systemctl start cassandra","title":"Whole cluster"},{"location":"operations/cassandra/restore/restore-cluster-same-ip/#restore-a-whole-cluster-from-a-remote-backup","text":"Follow this procedure when you have lost all nodes in a cluster but they have been recreated in the same cluster topology (Cluster, DC and rack names are all the same and the same number of nodes in each) and the replacement nodes have the same IP addresses as the original cluster. Ensure that Cassandra is stopped on all nodes and that its data directories are empty sudo systemctl stop cassandra sudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/* Allow the AxonOps user to write to the Cassandra data directory sudo chmod -R g+w /var/lib/cassandra/data These commands assume you are storing the Cassandra data in the default location /var/lib/cassandra/ , you will need to change the paths shown if your data is stored at a different location Start the AxonOps agent on all nodes sudo systemctl start axon-agent Now open the Restore page in the AxonOps Dashboard by going to Operations > Restore Infomy Choose the backup you wish to restore from the list and click the RESTORE button This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list. Infomy Select all nodes in the checkbox list then start the restore by clicking the REMOTE RESTORE button. The restore progress will be displayed in the Backup Restorations in Progress list Infomy After the restore operation has completed successfully, fix the ownership and permissions on the Cassandra data directories on all nodes in the cluster sudo chown -R cassandra.cassandra /var/lib/cassandra/data sudo chmod -R g-w /var/lib/cassandra/data Start cassandra on the restored nodes, starting with the seeds first sudo systemctl start cassandra","title":"Restore a whole cluster from a remote backup"},{"location":"operations/cassandra/restore/restore-different-cluster/","text":"Restore a backup to a different cluster \u00b6 Follow this procedure to restore an AxonOps backup from remote storage onto a different cluster NOTE: This facility is only available for backups created using AxonOps Agent version 1.0.60 or later AxonOps Agent version 1.0.60 and later includes a command-line tool which can be used to restore a backup created by AxonOps from remote storage (e.g. S3, GCS). This tool connects directly to your remote storage and does not require an AxonOps server or an active AxonOps Cloud account in order to function. Installing the Cassandra Restore Tool \u00b6 The AxonOps Cassandra Restore tool is included in the AxonOps Agent package. Installing on Debian / Ubuntu \u00b6 sudo apt-get install -y curl gnupg ca-certificates curl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add - echo \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-agent For new versions of Debian (>= bookworm) and Ubuntu (>= 22.04) the process of setting up the apt repository has changed. See below: sudo apt-get update sudo apt-get install -y curl gnupg ca-certificates curl -L https://packages.axonops.com/apt/repo-signing-key.gpg | gpg --dearmor -o /usr/share/keyrings/axonops.gpg echo \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-agent Installing on CentOS / RedHat \u00b6 sudo tee /etc/yum.repos.d/axonops-yum.repo << EOL [axonops-yum] name=axonops-yum baseurl=https://packages.axonops.com/yum/ enabled=1 repo_gpgcheck=0 gpgcheck=0 EOL sudo yum install axon-agent After the package has been installed you can find the Cassandra Restore Tool at /usr/share/axonops/axon-cassandra-restore . Run the tool with --help to see the available options: ~# /usr/share/axonops/axon-cassandra-restore --help Usage of /usr/share/axonops/axon-cassandra-restore: -i, --backup-id string UUID of the backup to restore --cassandra-bin-dir string Where the Cassandra binary files are stored (e.g. /opt/cassandra/bin) --cqlsh-options string Options to pass to cqlsh when restoring a table schema --dest-table string The name of the destination table for the restore in keyspace.table format. Requires --tables with a single source table. (Added in v1.0.61) -h, --help Show command-line help -l, --list List backups available in remote storage -d, --local-sstable-dir string A local directory in which to store sstables downloaded from backup storage --org-id string ID of the AxonOps organisation from which the backup was created -r, --restore Restore a backup from remote storage --restore-schema Set this when using --use-sstable-loader to restore the CQL schema for each table. Keyspaces must already exist. -e, --skip-existing-files Don't download files that already exist in the local destination path (Added in v1.0.61) -c, --source-cluster string The name of the cluster from which to restore -s, --source-hosts string Comma-separated list containing host IDs for which to restore backups --sstable-loader-options string Options to pass to sstableloader when restoring a backup --storage-config string JSON-formatted remote storage configuration -t, --tables string Comma-separated list of keyspace.table to restore. Defaults to all tables if omitted. --use-sstable-loader Use sstableloader to restore the backup. Requires --sstable-loader-options and --cassandra-bin-dir. -v, --verbose Show verbose output when listing backups --version Show version information and exit Listing the available backups \u00b6 NOTE: The host IDs used in this tool are the ID given to each host by AxonOps and do not relate to the Cassandra host ID. You can find the AxonOps host ID by selecting the node on the Cluster Overview page of the AxonOps dashboard and looking at the Agent ID field. To list the backups available in the remote storage bucket you can run the tool with the --list option. For example to list the backups in an Amazon S3 bucket you could use a command similar to this: /usr/share/axonops/axon-cassandra-restore --list \\ --org-id myaxonopsorg \\ --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' The restore tool will then scan the specified S3 bucket for AxonOps backups and it will display the date and backup ID for any backups it finds: Org ID: myaxonopsorg Cluster: testcluster Time Backup ID 2023 -09-14 14 :30 UTC c67cea2a-5310-11ee-b686-bed50b9335ec 2023 -09-15 14 :31 UTC 2c1d9aca-5312-11ee-b686-bed50b9335ec 2023 -09-16 14 :30 UTC 91be5007-5313-11ee-b686-bed50b9335ec 2023 -09-17 14 :31 UTC f75f13d9-5314-11ee-b686-bed50b9335ec 2023 -09-18 14 :30 UTC 5cffc1e6-5316-11ee-b686-bed50b9335ec If you pass the --verbose option when listing backups it will show the list of nodes and tables in each backup, for example: Org ID: myaxonopsorg Cluster: testcluster Time: 2023 -09-14 14 :30 UTC Backup ID: c67cea2a-5310-11ee-b686-bed50b9335ec Host: 026346a0-dc89-4235-ae34-552fcd453b42 Tables: system.prepared_statements, system.transferred_ranges_v2, system_distributed.repair_history, system_schema.types, system_traces.sessions, system.compaction_history, system.available_ranges_v2, system.batches, system.size_estimates, system_schema.aggregates, system.IndexInfo, system_auth.resource_role_permissons_index, system_schema.views, test.test, system.paxos, system.local, system.peers, system.peers_v2, system.table_estimates, system_auth.network_permissions, system_auth.roles, system_distributed.view_build_status, system.built_views, system_schema.triggers, system.peer_events, system.repairs, keyspace1.table1, system.sstable_activity, keyspace1.table2, system.transferred_ranges, system_auth.role_permissions, system_distributed.parent_repair_history, system.available_ranges, system_schema.dropped_columns, system_schema.columns, system_schema.keyspaces, system.view_builds_in_progress, system_auth.role_members, system_schema.functions, system_schema.indexes, system_schema.tables, system_traces.events, system.peer_events_v2 Host: 84759df0-8a19-497e-965f-200bdb4c1c9b Tables: system_traces.events, system.available_ranges_v2, system.peer_events, system_auth.resource_role_permissons_index, system_auth.role_members, system_schema.types, system.IndexInfo, system.sstable_activity, system_distributed.view_build_status, system_schema.indexes, system.batches, system.transferred_ranges, system_schema.keyspaces, system_schema.tables, system_traces.sessions, system_distributed.repair_history, system_schema.aggregates, system.available_ranges, system.compaction_history, system.paxos, system.peers_v2, system.view_builds_in_progress, system.size_estimates, keyspace1.table1, system_auth.roles, system_schema.dropped_columns, test.test, system_auth.role_permissions, system_distributed.parent_repair_history, system.local, system.peer_events_v2, system.repairs, system.table_estimates, system_auth.network_permissions, system.peers, system_schema.triggers, system_schema.views, system.built_views, system.prepared_statements, system.transferred_ranges_v2, system_schema.columns, system_schema.functions, keyspace1.table2 Host: 94ed3811-12ce-487f-ac49-ae31299efa31 Tables: system.peers_v2, system.view_builds_in_progress, system_auth.resource_role_permissons_index, system_auth.role_permissions, system_schema.aggregates, system_schema.indexes, test.test, system.available_ranges_v2, system_distributed.parent_repair_history, system_schema.keyspaces, system_traces.sessions, system_auth.role_members, system_auth.network_permissions, system_schema.dropped_columns, system_schema.types, system.repairs, system.size_estimates, system_auth.roles, system_schema.tables, system_schema.views, system.paxos, system.table_estimates, system.transferred_ranges, system.peers, system.prepared_statements, system.sstable_activity, system.peer_events_v2, system.batches, system.built_views, system.compaction_history, system_traces.events, system.IndexInfo, system.local, keyspace1.table2, system.peer_events, system.transferred_ranges_v2, system_distributed.repair_history, system_distributed.view_build_status, system_schema.columns, system_schema.functions, system.available_ranges, system_schema.triggers, keyspace1.table1 Scanning for backups can take a long time depending on the storage type and the amount of data, so you can use command-line options to restrict the search. For example this will restrict the search to a specific backup, cluster, hosts and tables: /usr/share/axonops/axon-cassandra-restore --list \\ --verbose \\ --org-id myaxonopsorg \\ --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\ --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\ --source-cluster testcluster \\ --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b --tables keyspace1.table1,keyspace1.table2 Restoring a Backup \u00b6 The axon-cassandra-restore tool can perform the following operations to restore a backup from remote storage: 1. Download the sstable files from the bucket 2. Create table schemas in the target cluster 3. Import the downloaded sstable files into the target cluster using sstableloader The default behaviour is to only download the sstable files to a local directory. Downloading a backup to a local directory \u00b6 This command will download the backup with ID 2c1d9aca-5312-11ee-b686-bed50b9335ec for the 3 hosts listed in the --list output above into the local directory /opt/cassandra/axonops-restore /usr/share/axonops/axon-cassandra-restore \\ --restore \\ --org-id myaxonopsorg \\ --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\ --source-cluster testcluster \\ --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\ --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\ --local-sstable-dir /opt/cassandra/axonops-restore The sstable files will be restored into directories named {local-sstable-dir}/{host-id}/keyspace/table/ and from here you can copy/move the files to another location or import them into a cluster using sstableloader . Download and import a backup in a single operation \u00b6 The above example shows how to download the backed up files into a local directory but it does not import them into a new cluster. You can make the axon-cassandra-restore tool do this for you after it downloads the files by passing the --use-sstable-loader , --cassandra-bin-dir and --sstable-loader-options command-line arguments. For example this command will download the same backup files as the previous example but it will also run sstableloader to import the downloaded files into a new cluster with contact points 10.0.0.1, 10.0.0.2 and 10.0.0.3: /usr/share/axonops/axon-cassandra-restore \\ --restore \\ --org-id myaxonopsorg \\ --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\ --source-cluster testcluster \\ --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\ --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\ --local-sstable-dir /opt/cassandra/axonops-restore \\ --use-sstable-loader \\ --cassandra-bin-dir /opt/cassandra/bin \\ --sstable-loader-options \"-d 10.0.0.1,10.0.0.2,10.0.0.3 -u cassandra -pw cassandra\" Importing CQL schemas during the restore \u00b6 When a backup is imported to a cluster using sstableloader it assumes that the destination tables already exist and will skip the import for any that are missing. AxonOps stores the current table schema with each backup, so it is possible to create any missing tables as part of the restore operation. This can be enabled with the --restore-schema and --cqlsh-options arguments to axon-cassandra-restore . Building on the example above this command will download the files from the backup, create the schema for any missing tables, and import the downloaded data with sstableloader : /usr/share/axonops/axon-cassandra-restore \\ --restore \\ --org-id myaxonopsorg \\ --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\ --source-cluster testcluster \\ --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\ --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\ --local-sstable-dir /opt/cassandra/axonops-restore \\ --use-sstable-loader \\ --cassandra-bin-dir /opt/cassandra/bin \\ --sstable-loader-options \"-d 10.0.0.1,10.0.0.2,10.0.0.3 -u cassandra -pw cassandra\" \\ --restore-schema \\ --cqlsh-options \"-u cassandra -p cassandra 10.0.0.1\" NOTE: This will not create missing keyspaces. You must ensure that the target keyspaces already exist in the destination cluster before running the restore command. Storage Config Examples \u00b6 The AxonOps Cassandra restore tool can restore backups from any remote storage supported by AxonOps for backups. The --storage-config command-line option configures the type of remote storage and the credentials required for access. Here are some examples of the most common storage types: Local filesystem \u00b6 --storage-config '{\"type\":\"local\",\"path\":\"/backups/cassandra\"}' Amazon S3 \u00b6 --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' Azure Blob Storage \u00b6 --storage-config '{\"type\":\"azureblob\",\"account\":\"MY_AZURE_ACCOUNT_NAME\",\"key\":\"MY_AZURE_STORAGE_KEY\"}' Google Cloud Storage \u00b6 --storage-config '{\"type\":\"googlecloudstorage\",\"location\":\"us\",\"service_account_credentials\":\"ESCAPED_JSON_PRIVATE_KEY\"}' Restore to a different table \u00b6 This feature is available in AxonOps Agent v1.0.61 or later When restoring a single table from a backup it is possible to use the --dest-table option on the command-line to load the restored data into table with a different name and/or keyspace to the original table. If you also supply the --restore-schema option then the new table will be created as part of the restore process. NOTE: The destination keyspace must already exist before running the restore command. This example shows restoring the table keyspace1.table1 into a table named table1_restored in keysace restoreks : /usr/share/axonops/axon-cassandra-restore \\ --restore \\ --org-id myaxonopsorg \\ --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\ --source-cluster testcluster \\ --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\ --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\ --local-sstable-dir /opt/cassandra/axonops-restore \\ --use-sstable-loader \\ --cassandra-bin-dir /opt/cassandra/bin \\ --sstable-loader-options \"-d 10.0.0.1,10.0.0.2,10.0.0.3 -u cassandra -pw cassandra\" \\ --restore-schema \\ --cqlsh-options \"-u cassandra -p cassandra 10.0.0.1\" \\ --tables keyspace1.table1 \\ --dest-table restoreks.table1_restored","title":"Restore to a different cluster"},{"location":"operations/cassandra/restore/restore-different-cluster/#restore-a-backup-to-a-different-cluster","text":"Follow this procedure to restore an AxonOps backup from remote storage onto a different cluster NOTE: This facility is only available for backups created using AxonOps Agent version 1.0.60 or later AxonOps Agent version 1.0.60 and later includes a command-line tool which can be used to restore a backup created by AxonOps from remote storage (e.g. S3, GCS). This tool connects directly to your remote storage and does not require an AxonOps server or an active AxonOps Cloud account in order to function.","title":"Restore a backup to a different cluster"},{"location":"operations/cassandra/restore/restore-different-cluster/#installing-the-cassandra-restore-tool","text":"The AxonOps Cassandra Restore tool is included in the AxonOps Agent package.","title":"Installing the Cassandra Restore Tool"},{"location":"operations/cassandra/restore/restore-different-cluster/#installing-on-debian-ubuntu","text":"sudo apt-get install -y curl gnupg ca-certificates curl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add - echo \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-agent For new versions of Debian (>= bookworm) and Ubuntu (>= 22.04) the process of setting up the apt repository has changed. See below: sudo apt-get update sudo apt-get install -y curl gnupg ca-certificates curl -L https://packages.axonops.com/apt/repo-signing-key.gpg | gpg --dearmor -o /usr/share/keyrings/axonops.gpg echo \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list sudo apt-get update sudo apt-get install axon-agent","title":"Installing on Debian / Ubuntu"},{"location":"operations/cassandra/restore/restore-different-cluster/#installing-on-centos-redhat","text":"sudo tee /etc/yum.repos.d/axonops-yum.repo << EOL [axonops-yum] name=axonops-yum baseurl=https://packages.axonops.com/yum/ enabled=1 repo_gpgcheck=0 gpgcheck=0 EOL sudo yum install axon-agent After the package has been installed you can find the Cassandra Restore Tool at /usr/share/axonops/axon-cassandra-restore . Run the tool with --help to see the available options: ~# /usr/share/axonops/axon-cassandra-restore --help Usage of /usr/share/axonops/axon-cassandra-restore: -i, --backup-id string UUID of the backup to restore --cassandra-bin-dir string Where the Cassandra binary files are stored (e.g. /opt/cassandra/bin) --cqlsh-options string Options to pass to cqlsh when restoring a table schema --dest-table string The name of the destination table for the restore in keyspace.table format. Requires --tables with a single source table. (Added in v1.0.61) -h, --help Show command-line help -l, --list List backups available in remote storage -d, --local-sstable-dir string A local directory in which to store sstables downloaded from backup storage --org-id string ID of the AxonOps organisation from which the backup was created -r, --restore Restore a backup from remote storage --restore-schema Set this when using --use-sstable-loader to restore the CQL schema for each table. Keyspaces must already exist. -e, --skip-existing-files Don't download files that already exist in the local destination path (Added in v1.0.61) -c, --source-cluster string The name of the cluster from which to restore -s, --source-hosts string Comma-separated list containing host IDs for which to restore backups --sstable-loader-options string Options to pass to sstableloader when restoring a backup --storage-config string JSON-formatted remote storage configuration -t, --tables string Comma-separated list of keyspace.table to restore. Defaults to all tables if omitted. --use-sstable-loader Use sstableloader to restore the backup. Requires --sstable-loader-options and --cassandra-bin-dir. -v, --verbose Show verbose output when listing backups --version Show version information and exit","title":"Installing on CentOS / RedHat"},{"location":"operations/cassandra/restore/restore-different-cluster/#listing-the-available-backups","text":"NOTE: The host IDs used in this tool are the ID given to each host by AxonOps and do not relate to the Cassandra host ID. You can find the AxonOps host ID by selecting the node on the Cluster Overview page of the AxonOps dashboard and looking at the Agent ID field. To list the backups available in the remote storage bucket you can run the tool with the --list option. For example to list the backups in an Amazon S3 bucket you could use a command similar to this: /usr/share/axonops/axon-cassandra-restore --list \\ --org-id myaxonopsorg \\ --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' The restore tool will then scan the specified S3 bucket for AxonOps backups and it will display the date and backup ID for any backups it finds: Org ID: myaxonopsorg Cluster: testcluster Time Backup ID 2023 -09-14 14 :30 UTC c67cea2a-5310-11ee-b686-bed50b9335ec 2023 -09-15 14 :31 UTC 2c1d9aca-5312-11ee-b686-bed50b9335ec 2023 -09-16 14 :30 UTC 91be5007-5313-11ee-b686-bed50b9335ec 2023 -09-17 14 :31 UTC f75f13d9-5314-11ee-b686-bed50b9335ec 2023 -09-18 14 :30 UTC 5cffc1e6-5316-11ee-b686-bed50b9335ec If you pass the --verbose option when listing backups it will show the list of nodes and tables in each backup, for example: Org ID: myaxonopsorg Cluster: testcluster Time: 2023 -09-14 14 :30 UTC Backup ID: c67cea2a-5310-11ee-b686-bed50b9335ec Host: 026346a0-dc89-4235-ae34-552fcd453b42 Tables: system.prepared_statements, system.transferred_ranges_v2, system_distributed.repair_history, system_schema.types, system_traces.sessions, system.compaction_history, system.available_ranges_v2, system.batches, system.size_estimates, system_schema.aggregates, system.IndexInfo, system_auth.resource_role_permissons_index, system_schema.views, test.test, system.paxos, system.local, system.peers, system.peers_v2, system.table_estimates, system_auth.network_permissions, system_auth.roles, system_distributed.view_build_status, system.built_views, system_schema.triggers, system.peer_events, system.repairs, keyspace1.table1, system.sstable_activity, keyspace1.table2, system.transferred_ranges, system_auth.role_permissions, system_distributed.parent_repair_history, system.available_ranges, system_schema.dropped_columns, system_schema.columns, system_schema.keyspaces, system.view_builds_in_progress, system_auth.role_members, system_schema.functions, system_schema.indexes, system_schema.tables, system_traces.events, system.peer_events_v2 Host: 84759df0-8a19-497e-965f-200bdb4c1c9b Tables: system_traces.events, system.available_ranges_v2, system.peer_events, system_auth.resource_role_permissons_index, system_auth.role_members, system_schema.types, system.IndexInfo, system.sstable_activity, system_distributed.view_build_status, system_schema.indexes, system.batches, system.transferred_ranges, system_schema.keyspaces, system_schema.tables, system_traces.sessions, system_distributed.repair_history, system_schema.aggregates, system.available_ranges, system.compaction_history, system.paxos, system.peers_v2, system.view_builds_in_progress, system.size_estimates, keyspace1.table1, system_auth.roles, system_schema.dropped_columns, test.test, system_auth.role_permissions, system_distributed.parent_repair_history, system.local, system.peer_events_v2, system.repairs, system.table_estimates, system_auth.network_permissions, system.peers, system_schema.triggers, system_schema.views, system.built_views, system.prepared_statements, system.transferred_ranges_v2, system_schema.columns, system_schema.functions, keyspace1.table2 Host: 94ed3811-12ce-487f-ac49-ae31299efa31 Tables: system.peers_v2, system.view_builds_in_progress, system_auth.resource_role_permissons_index, system_auth.role_permissions, system_schema.aggregates, system_schema.indexes, test.test, system.available_ranges_v2, system_distributed.parent_repair_history, system_schema.keyspaces, system_traces.sessions, system_auth.role_members, system_auth.network_permissions, system_schema.dropped_columns, system_schema.types, system.repairs, system.size_estimates, system_auth.roles, system_schema.tables, system_schema.views, system.paxos, system.table_estimates, system.transferred_ranges, system.peers, system.prepared_statements, system.sstable_activity, system.peer_events_v2, system.batches, system.built_views, system.compaction_history, system_traces.events, system.IndexInfo, system.local, keyspace1.table2, system.peer_events, system.transferred_ranges_v2, system_distributed.repair_history, system_distributed.view_build_status, system_schema.columns, system_schema.functions, system.available_ranges, system_schema.triggers, keyspace1.table1 Scanning for backups can take a long time depending on the storage type and the amount of data, so you can use command-line options to restrict the search. For example this will restrict the search to a specific backup, cluster, hosts and tables: /usr/share/axonops/axon-cassandra-restore --list \\ --verbose \\ --org-id myaxonopsorg \\ --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\ --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\ --source-cluster testcluster \\ --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b --tables keyspace1.table1,keyspace1.table2","title":"Listing the available backups"},{"location":"operations/cassandra/restore/restore-different-cluster/#restoring-a-backup","text":"The axon-cassandra-restore tool can perform the following operations to restore a backup from remote storage: 1. Download the sstable files from the bucket 2. Create table schemas in the target cluster 3. Import the downloaded sstable files into the target cluster using sstableloader The default behaviour is to only download the sstable files to a local directory.","title":"Restoring a Backup"},{"location":"operations/cassandra/restore/restore-different-cluster/#downloading-a-backup-to-a-local-directory","text":"This command will download the backup with ID 2c1d9aca-5312-11ee-b686-bed50b9335ec for the 3 hosts listed in the --list output above into the local directory /opt/cassandra/axonops-restore /usr/share/axonops/axon-cassandra-restore \\ --restore \\ --org-id myaxonopsorg \\ --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\ --source-cluster testcluster \\ --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\ --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\ --local-sstable-dir /opt/cassandra/axonops-restore The sstable files will be restored into directories named {local-sstable-dir}/{host-id}/keyspace/table/ and from here you can copy/move the files to another location or import them into a cluster using sstableloader .","title":"Downloading a backup to a local directory"},{"location":"operations/cassandra/restore/restore-different-cluster/#download-and-import-a-backup-in-a-single-operation","text":"The above example shows how to download the backed up files into a local directory but it does not import them into a new cluster. You can make the axon-cassandra-restore tool do this for you after it downloads the files by passing the --use-sstable-loader , --cassandra-bin-dir and --sstable-loader-options command-line arguments. For example this command will download the same backup files as the previous example but it will also run sstableloader to import the downloaded files into a new cluster with contact points 10.0.0.1, 10.0.0.2 and 10.0.0.3: /usr/share/axonops/axon-cassandra-restore \\ --restore \\ --org-id myaxonopsorg \\ --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\ --source-cluster testcluster \\ --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\ --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\ --local-sstable-dir /opt/cassandra/axonops-restore \\ --use-sstable-loader \\ --cassandra-bin-dir /opt/cassandra/bin \\ --sstable-loader-options \"-d 10.0.0.1,10.0.0.2,10.0.0.3 -u cassandra -pw cassandra\"","title":"Download and import a backup in a single operation"},{"location":"operations/cassandra/restore/restore-different-cluster/#importing-cql-schemas-during-the-restore","text":"When a backup is imported to a cluster using sstableloader it assumes that the destination tables already exist and will skip the import for any that are missing. AxonOps stores the current table schema with each backup, so it is possible to create any missing tables as part of the restore operation. This can be enabled with the --restore-schema and --cqlsh-options arguments to axon-cassandra-restore . Building on the example above this command will download the files from the backup, create the schema for any missing tables, and import the downloaded data with sstableloader : /usr/share/axonops/axon-cassandra-restore \\ --restore \\ --org-id myaxonopsorg \\ --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\ --source-cluster testcluster \\ --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\ --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\ --local-sstable-dir /opt/cassandra/axonops-restore \\ --use-sstable-loader \\ --cassandra-bin-dir /opt/cassandra/bin \\ --sstable-loader-options \"-d 10.0.0.1,10.0.0.2,10.0.0.3 -u cassandra -pw cassandra\" \\ --restore-schema \\ --cqlsh-options \"-u cassandra -p cassandra 10.0.0.1\" NOTE: This will not create missing keyspaces. You must ensure that the target keyspaces already exist in the destination cluster before running the restore command.","title":"Importing CQL schemas during the restore"},{"location":"operations/cassandra/restore/restore-different-cluster/#storage-config-examples","text":"The AxonOps Cassandra restore tool can restore backups from any remote storage supported by AxonOps for backups. The --storage-config command-line option configures the type of remote storage and the credentials required for access. Here are some examples of the most common storage types:","title":"Storage Config Examples"},{"location":"operations/cassandra/restore/restore-different-cluster/#local-filesystem","text":"--storage-config '{\"type\":\"local\",\"path\":\"/backups/cassandra\"}'","title":"Local filesystem"},{"location":"operations/cassandra/restore/restore-different-cluster/#amazon-s3","text":"--storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}'","title":"Amazon S3"},{"location":"operations/cassandra/restore/restore-different-cluster/#azure-blob-storage","text":"--storage-config '{\"type\":\"azureblob\",\"account\":\"MY_AZURE_ACCOUNT_NAME\",\"key\":\"MY_AZURE_STORAGE_KEY\"}'","title":"Azure Blob Storage"},{"location":"operations/cassandra/restore/restore-different-cluster/#google-cloud-storage","text":"--storage-config '{\"type\":\"googlecloudstorage\",\"location\":\"us\",\"service_account_credentials\":\"ESCAPED_JSON_PRIVATE_KEY\"}'","title":"Google Cloud Storage"},{"location":"operations/cassandra/restore/restore-different-cluster/#restore-to-a-different-table","text":"This feature is available in AxonOps Agent v1.0.61 or later When restoring a single table from a backup it is possible to use the --dest-table option on the command-line to load the restored data into table with a different name and/or keyspace to the original table. If you also supply the --restore-schema option then the new table will be created as part of the restore process. NOTE: The destination keyspace must already exist before running the restore command. This example shows restoring the table keyspace1.table1 into a table named table1_restored in keysace restoreks : /usr/share/axonops/axon-cassandra-restore \\ --restore \\ --org-id myaxonopsorg \\ --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\ --source-cluster testcluster \\ --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\ --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\ --local-sstable-dir /opt/cassandra/axonops-restore \\ --use-sstable-loader \\ --cassandra-bin-dir /opt/cassandra/bin \\ --sstable-loader-options \"-d 10.0.0.1,10.0.0.2,10.0.0.3 -u cassandra -pw cassandra\" \\ --restore-schema \\ --cqlsh-options \"-u cassandra -p cassandra 10.0.0.1\" \\ --tables keyspace1.table1 \\ --dest-table restoreks.table1_restored","title":"Restore to a different table"},{"location":"operations/cassandra/restore/restore-node-different-ip/","text":"Replace a node from a remote backup \u00b6 Follow this procedure to restore a single Cassandra node from a total loss of all data where the replacement node has a different IP address from the original. NOTE: Restoring a node from a total loss can only be performed from a remote backup AxonOps identifies nodes by a unique host ID which is assigned when the agent starts up. In order to restore a backup to a node with a different IP address you must manually assign the AxonOps host ID of the old node to its new replacement. You can find the host ID of the old node on the Cluster Overview page of the AxonOps dashboard Infomy If you still have access to the old server or its data then its host ID can also be found in the file /var/lib/axonops/hostId Manually configure the AxonOps Agent host ID \u00b6 Ensure the AxonOps Agent is stopped and clear any data that it may have created on startup sudo systemctl stop axon-agent sudo rm -rf /var/lib/axonops/* Manually apply the old node's host ID on the replacement (replace the host ID shown with your host ID from the previous steps) echo '24d0cbf9-3b5a-11ed-8433-16b3c6a7bcc5' | sudo tee /var/lib/axonops/hostId sudo chown axonops.axonops /var/lib/axonops/hostId Start the AxonOps agent sudo systemctl start axon-agent Restore a backup to the replacement node \u00b6 Ensure Cassandra is stopped on the new node and that its data directories are all empty sudo systemctl stop cassandra sudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/* Allow the AxonOps user to write to the Cassandra data directory sudo chmod -R g+w /var/lib/cassandra/data These commands assume you are storing the Cassandra data in the default location /var/lib/cassandra/ , you will need to change the paths shown if your data is stored at a different location Now open the Restore page in the AxonOps Dashboard by going to Operations > Restore Infomy Choose the backup you wish to restore from the list and click the RESTORE button This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list. Infomy Ensure only the node you wish to restore is selected in the checkbox list and start the restore by clicking the REMOTE RESTORE button. The restore progress will be displayed in the Backup Restorations in Progress list Infomy After the restore has completed successfully, fix the ownership and permissions on the Cassandra data directories sudo chown -R cassandra.cassandra /var/lib/cassandra/data sudo chmod -R g-w /var/lib/cassandra/data Now you can start cassandra on the restored node sudo systemctl start cassandra After the replacement node has started up the old IP address may still be visible in Gossip. It should clear out automatically after a day or two, or you can perform a rolling restart of the cluster to make sure everything is up-to-date.","title":"Single node with a different IP address"},{"location":"operations/cassandra/restore/restore-node-different-ip/#replace-a-node-from-a-remote-backup","text":"Follow this procedure to restore a single Cassandra node from a total loss of all data where the replacement node has a different IP address from the original. NOTE: Restoring a node from a total loss can only be performed from a remote backup AxonOps identifies nodes by a unique host ID which is assigned when the agent starts up. In order to restore a backup to a node with a different IP address you must manually assign the AxonOps host ID of the old node to its new replacement. You can find the host ID of the old node on the Cluster Overview page of the AxonOps dashboard Infomy If you still have access to the old server or its data then its host ID can also be found in the file /var/lib/axonops/hostId","title":"Replace a node from a remote backup"},{"location":"operations/cassandra/restore/restore-node-different-ip/#manually-configure-the-axonops-agent-host-id","text":"Ensure the AxonOps Agent is stopped and clear any data that it may have created on startup sudo systemctl stop axon-agent sudo rm -rf /var/lib/axonops/* Manually apply the old node's host ID on the replacement (replace the host ID shown with your host ID from the previous steps) echo '24d0cbf9-3b5a-11ed-8433-16b3c6a7bcc5' | sudo tee /var/lib/axonops/hostId sudo chown axonops.axonops /var/lib/axonops/hostId Start the AxonOps agent sudo systemctl start axon-agent","title":"Manually configure the AxonOps Agent host ID"},{"location":"operations/cassandra/restore/restore-node-different-ip/#restore-a-backup-to-the-replacement-node","text":"Ensure Cassandra is stopped on the new node and that its data directories are all empty sudo systemctl stop cassandra sudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/* Allow the AxonOps user to write to the Cassandra data directory sudo chmod -R g+w /var/lib/cassandra/data These commands assume you are storing the Cassandra data in the default location /var/lib/cassandra/ , you will need to change the paths shown if your data is stored at a different location Now open the Restore page in the AxonOps Dashboard by going to Operations > Restore Infomy Choose the backup you wish to restore from the list and click the RESTORE button This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list. Infomy Ensure only the node you wish to restore is selected in the checkbox list and start the restore by clicking the REMOTE RESTORE button. The restore progress will be displayed in the Backup Restorations in Progress list Infomy After the restore has completed successfully, fix the ownership and permissions on the Cassandra data directories sudo chown -R cassandra.cassandra /var/lib/cassandra/data sudo chmod -R g-w /var/lib/cassandra/data Now you can start cassandra on the restored node sudo systemctl start cassandra After the replacement node has started up the old IP address may still be visible in Gossip. It should clear out automatically after a day or two, or you can perform a rolling restart of the cluster to make sure everything is up-to-date.","title":"Restore a backup to the replacement node"},{"location":"operations/cassandra/restore/restore-node-same-ip/","text":"Restore a single node from a remote backup \u00b6 Follow this procedure to restore a single Cassandra node from a total loss of all data where the replacement node has the same IP address as the original. NOTE: Restoring a node from a total loss can only be performed from a remote backup Ensure Cassandra is stopped on the new node and that its data directories are all empty sudo systemctl stop cassandra sudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/* Allow the AxonOps user to write to the Cassandra data directory sudo chmod -R g+w /var/lib/cassandra/data These commands assume you are storing the Cassandra data in the default location /var/lib/cassandra/ , you will need to change the paths shown if your data is stored at a different location Start axon-agent if it is not already running sudo systemctl start axon-agent Now open the Restore page in the AxonOps Dashboard by going to Operations > Restore Infomy Choose the backup you wish to restore from the list and click the RESTORE button This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list. Infomy Ensure only the node you wish to restore is selected in the checkbox list and start the restore by clicking the REMOTE RESTORE button. The restore progress will be displayed in the Backup Restorations in Progress list Infomy After the restore has completed successfully, fix the ownership and permissions on the Cassandra data directories sudo chown -R cassandra.cassandra /var/lib/cassandra/data sudo chmod -R g-w /var/lib/cassandra/data Start cassandra on the restored node sudo systemctl start cassandra","title":"Single node"},{"location":"operations/cassandra/restore/restore-node-same-ip/#restore-a-single-node-from-a-remote-backup","text":"Follow this procedure to restore a single Cassandra node from a total loss of all data where the replacement node has the same IP address as the original. NOTE: Restoring a node from a total loss can only be performed from a remote backup Ensure Cassandra is stopped on the new node and that its data directories are all empty sudo systemctl stop cassandra sudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/* Allow the AxonOps user to write to the Cassandra data directory sudo chmod -R g+w /var/lib/cassandra/data These commands assume you are storing the Cassandra data in the default location /var/lib/cassandra/ , you will need to change the paths shown if your data is stored at a different location Start axon-agent if it is not already running sudo systemctl start axon-agent Now open the Restore page in the AxonOps Dashboard by going to Operations > Restore Infomy Choose the backup you wish to restore from the list and click the RESTORE button This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list. Infomy Ensure only the node you wish to restore is selected in the checkbox list and start the restore by clicking the REMOTE RESTORE button. The restore progress will be displayed in the Backup Restorations in Progress list Infomy After the restore has completed successfully, fix the ownership and permissions on the Cassandra data directories sudo chown -R cassandra.cassandra /var/lib/cassandra/data sudo chmod -R g-w /var/lib/cassandra/data Start cassandra on the restored node sudo systemctl start cassandra","title":"Restore a single node from a remote backup"},{"location":"operations/cassandra/rollingrestart/overview/","text":"AxonOps provides a rolling restart functionality for Cassandra. The feature is accessible via Operations > Rolling Restart Infomy axonops user will require permissions to be able to stop and start Cassandra service. To do so you will add axonops user in the sudoers with for instance the following permissions: #/etc/sudoers.d/axonops axonops ALL = NOPASSWD: /sbin/service cassandra *, /usr/bin/systemctl * cassandra* You can start an immediate rolling restart or schedule it. The script field let you able to tweak the predefined script executed by axon-agents during the restart process. You can also specify different degree of parallelism for the restart: DC , Rack and Node . For instance, to restart one entire rack at once across the cluster, you can set a large Node parallelism (greater than the number of nodes the rack has, ie 999). DC parallelism: 1 Rack parallelism: 1 Node parallelism: 999 To restart one entire rack across each DC : DC parallelism: 999 Rack parallelism: 1 Node parallelism: 999","title":"Rolling Restart"},{"location":"overview/architecture/","text":"Architecture \u00b6 Before \u00b6 Our deployment model with the use of open source tools Infomy AxonOps Deployment Model \u00b6 As you can see from the diagram below, we have massively simplified the stack with AxonOps. Infomy You can also use a CQL datastore such as Cassandra , Elassandra , or Scylla to store the metrics. We recommend storing metrics on a CQL store on 100+ nodes clusters to improve your experience navigating the metrics dashboards.","title":"Architecture"},{"location":"overview/architecture/#architecture","text":"","title":"Architecture"},{"location":"overview/architecture/#before","text":"Our deployment model with the use of open source tools Infomy","title":"Before"},{"location":"overview/architecture/#axonops-deployment-model","text":"As you can see from the diagram below, we have massively simplified the stack with AxonOps. Infomy You can also use a CQL datastore such as Cassandra , Elassandra , or Scylla to store the metrics. We recommend storing metrics on a CQL store on 100+ nodes clusters to improve your experience navigating the metrics dashboards.","title":"AxonOps Deployment Model"},{"location":"overview/axonops-cloud/","text":"","title":"Axonops cloud"},{"location":"overview/axonops-enterprise/","text":"","title":"Axonops enterprise"},{"location":"overview/motivation/","text":"What is AxonOps? \u00b6 AxonOps is a One-Stop Operations Platform for Apache Cassandra\u00ae Built by Cassandra experts, the only cloud native solution to monitor, maintain and backup any Cassandra cluster anywhere Why ? \u00b6 Frustrated by the lack of tooling available to manage the next generation open source data platforms, Digitalis engineers decided to build a one-stop monitoring and operations platform to ensure they could provide the highest quality of services. Proven in some of the most demanding environments, AxonOps is now available as a standalone platform supporting Apache Cassandra today with Apache Kafka coming soon. Features \u00b6 Metric Dashboard - The AxonOps dashboard is pre-configured and well laid out in order for you to easily visualise the performance of your multiple Cassandra clusters across all of your data centres, Logs and Events - AxonOps agents collect logs from log files, as well as internal Cassandra events such as \u201crepair\u201d and JMX calls. Service Checks - As a site reliability engineer, service checks and the RAG status dashboard gives you great confidence in how your platform is operating. Regular checks against your processes, open ports, service health can be quickly implemented and deployed with minimum setup. Alert Integrations - Alerts can be configured for multiple services including Slack, Pagerduty, SMTP, and generic webhooks. Cassandra Repairs - Cassandra repairs are essential for maintaining the data integrity across all replicas. Backup and Restore - There are very few Cassandra tools that allow to setup Cassandra data backups as easily as AxonOps. Company Timeline \u00b6 To view the journey we have embarked on since 2017 have a look here Motivation \u00b6 AxonOps has been developed and actively maintained by digitalis.io , a company providing managed services for Apache Cassandra\u2122 and other modern distributed data technologies. digitalis.io used a variety of modern and popular open source tools to manage its customer's data platforms to gain quick insight into how the clusters are working, and be alerted when there are issues. The open source tools we used are: Grafana - metrics dashboarding Prometheus - time series database for metrics Prometheus Alertmanager - metrics alerting ELK - Log capture and visualisation elastalert - Alerting on logs Consul - Service Discovery and Health Checks consul-alerts - Alerting on service health check failures Rundeck - Job Scheduler Ansible - Provisioning automation Problems \u00b6 The tools listed above served us well. They gave us the confidence to manage enterprise deployment of distributed data platforms \u2013 alerting us when there are problems, ability to diagnose issues quickly, automatically performing routine scheduled tasks etc. However, using these tools and their problems were realised over time. Too many components - There are many components including the agents that need to be installed. Takes a lot of effort to integrate all components for each customer's on-premises environment, even with fully automated implementation using Ansible. Steep learning curve - The learning curve of deploying and configuring all the components is high. Patching hell - Patching schedule became a nightmare because of the sheer number of components. Imagine having to raise change requests for patching all above components! Enterprise hell - Firewall configurations became big for enterprise on-premises customers, often required many hours of tracing which change requests were unsuccessfully executed. Multiple dashboards - Multiple dashboards for metrics, logs and service availability. Complex alerting configurations - Alert notification configurations were all over the place. Fine-tuning alerts and updating them takes a lot of work. Wish List \u00b6 With the above problems in mind, we needed to become more efficient as a company deploying the tools we need to manage our customers. After promoting the above tools to our customers, we ate the humble pie, and went back to the drawing board with the aim of reducing the efforts needed to on-board new customers. On-premises / cloud deployment Single dashboard for metrics / logs / service health Simple alert rules configurations Capture all metrics at high resolution (with Cassandra there are well over 20,000 metrics!) Capture logs and internal events like authentication, DDL, DML etc Scheduled backup / restore feature Performs domain specific administrative tasks, including Cassandra repair Manages the following products; Apache Cassandra Apache Kafka DataStax Enterprise Confluent Enterprise Elasticsearch Apache Spark etc Simplified deployment model Single agent for collecting metrics, logs, event, configs The same agent performs execution of health checks, backup, restore No JMX to capture the metrics, and must be push from the JVM and not pull Single socket connection initiated by agent to management server requiring only simple firewall rules Bidirectional communication between agent and management server over the single socket Modern snappy GUI","title":"What is AxonOps?"},{"location":"overview/motivation/#what-is-axonops","text":"AxonOps is a One-Stop Operations Platform for Apache Cassandra\u00ae Built by Cassandra experts, the only cloud native solution to monitor, maintain and backup any Cassandra cluster anywhere","title":"What is AxonOps?"},{"location":"overview/motivation/#why","text":"Frustrated by the lack of tooling available to manage the next generation open source data platforms, Digitalis engineers decided to build a one-stop monitoring and operations platform to ensure they could provide the highest quality of services. Proven in some of the most demanding environments, AxonOps is now available as a standalone platform supporting Apache Cassandra today with Apache Kafka coming soon.","title":"Why ?"},{"location":"overview/motivation/#features","text":"Metric Dashboard - The AxonOps dashboard is pre-configured and well laid out in order for you to easily visualise the performance of your multiple Cassandra clusters across all of your data centres, Logs and Events - AxonOps agents collect logs from log files, as well as internal Cassandra events such as \u201crepair\u201d and JMX calls. Service Checks - As a site reliability engineer, service checks and the RAG status dashboard gives you great confidence in how your platform is operating. Regular checks against your processes, open ports, service health can be quickly implemented and deployed with minimum setup. Alert Integrations - Alerts can be configured for multiple services including Slack, Pagerduty, SMTP, and generic webhooks. Cassandra Repairs - Cassandra repairs are essential for maintaining the data integrity across all replicas. Backup and Restore - There are very few Cassandra tools that allow to setup Cassandra data backups as easily as AxonOps.","title":"Features"},{"location":"overview/motivation/#company-timeline","text":"To view the journey we have embarked on since 2017 have a look here","title":"Company Timeline"},{"location":"overview/motivation/#motivation","text":"AxonOps has been developed and actively maintained by digitalis.io , a company providing managed services for Apache Cassandra\u2122 and other modern distributed data technologies. digitalis.io used a variety of modern and popular open source tools to manage its customer's data platforms to gain quick insight into how the clusters are working, and be alerted when there are issues. The open source tools we used are: Grafana - metrics dashboarding Prometheus - time series database for metrics Prometheus Alertmanager - metrics alerting ELK - Log capture and visualisation elastalert - Alerting on logs Consul - Service Discovery and Health Checks consul-alerts - Alerting on service health check failures Rundeck - Job Scheduler Ansible - Provisioning automation","title":"Motivation"},{"location":"overview/motivation/#problems","text":"The tools listed above served us well. They gave us the confidence to manage enterprise deployment of distributed data platforms \u2013 alerting us when there are problems, ability to diagnose issues quickly, automatically performing routine scheduled tasks etc. However, using these tools and their problems were realised over time. Too many components - There are many components including the agents that need to be installed. Takes a lot of effort to integrate all components for each customer's on-premises environment, even with fully automated implementation using Ansible. Steep learning curve - The learning curve of deploying and configuring all the components is high. Patching hell - Patching schedule became a nightmare because of the sheer number of components. Imagine having to raise change requests for patching all above components! Enterprise hell - Firewall configurations became big for enterprise on-premises customers, often required many hours of tracing which change requests were unsuccessfully executed. Multiple dashboards - Multiple dashboards for metrics, logs and service availability. Complex alerting configurations - Alert notification configurations were all over the place. Fine-tuning alerts and updating them takes a lot of work.","title":"Problems"},{"location":"overview/motivation/#wish-list","text":"With the above problems in mind, we needed to become more efficient as a company deploying the tools we need to manage our customers. After promoting the above tools to our customers, we ate the humble pie, and went back to the drawing board with the aim of reducing the efforts needed to on-board new customers. On-premises / cloud deployment Single dashboard for metrics / logs / service health Simple alert rules configurations Capture all metrics at high resolution (with Cassandra there are well over 20,000 metrics!) Capture logs and internal events like authentication, DDL, DML etc Scheduled backup / restore feature Performs domain specific administrative tasks, including Cassandra repair Manages the following products; Apache Cassandra Apache Kafka DataStax Enterprise Confluent Enterprise Elasticsearch Apache Spark etc Simplified deployment model Single agent for collecting metrics, logs, event, configs The same agent performs execution of health checks, backup, restore No JMX to capture the metrics, and must be push from the JVM and not pull Single socket connection initiated by agent to management server requiring only simple firewall rules Bidirectional communication between agent and management server over the single socket Modern snappy GUI","title":"Wish List"},{"location":"quickstart/docker/","text":"","title":"Docker"},{"location":"quickstart/saas/","text":"","title":"Saas"},{"location":"release_notes/releases/","text":"","title":"Releases"},{"location":"server/api/overview/","text":"","title":"Overview"}]}