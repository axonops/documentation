{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AxonOps Documentation","text":""},{"location":"#introduction","title":"Introduction","text":"<p>AxonOps is an operational management tool built for Apache Cassandra (https://cassandra.apache.org).</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Inventory information overview</li> <li>Dashboarding metrics, logs, and health checks</li> <li>Highly efficient metrics collection and storage from the agents</li> <li>Integrates with ChatOps and alerting tools - Slack and PagerDuty etc. for notifications and alerts</li> <li>Domain aware functionalities, including Cassandra repairs and backups schedulers.</li> </ul>"},{"location":"#components","title":"Components","text":"<p>AxonOps has four main components:</p> <ul> <li>axon-server - The main server of axonops that collect metrics, logs, events and more.</li> <li>axon-dash - The UI to interact with axon-server (dash for AxonOps Dashboards).</li> <li>axon-agents - An to small agent binary deployed onto the managed nodes.</li> <li>Elasticsearch - A distributed search engine which stores the collected data.</li> </ul>"},{"location":"cluster/cluster-overview/","title":"Overview","text":"<p>Cluster Overview is the home page and provides an overview of your cluster health, including OS, Cassandra and JVM details.</p> <p>The information is automatically extracted by the AxonOps agent and pushed to AxonOps server. There is no need to configure anything on the agent or the server side for this information to be populated in the Cluster Overview dashboard.</p> <p>On the Axonops application menu, select <code>Cluster Overview</code>.</p> <p></p> <p></p> <p>You can select a node to view some details on the OS, Cassandra or the JVM.</p> <p>Infomy</p> <p></p>"},{"location":"cluster/cluster-overview/#os-details","title":"OS Details","text":"<p>Operating System Details section shows general information including:</p> <ul> <li>General Information</li> <li>CPU</li> <li>Memory</li> <li>Swap</li> <li>Disk volumes</li> </ul> <p>Infomy</p> <p></p>"},{"location":"cluster/cluster-overview/#cassandra-details","title":"Cassandra Details","text":"<p>Cassandra Details view shows the details from cassandra.yaml loaded into Cassandra. There is a search field available near the top to filter the parameters.</p> <p>Infomy</p> <p></p>"},{"location":"cluster/cluster-overview/#jvm-details","title":"JVM Details","text":"<p>JVM Details section shows the general information about the JVM, including the version and some configurations such as the heap and Garbage Collection settings.</p> <p>Infomy</p> <p></p>"},{"location":"configuration/agent-configuration/","title":"Configuring AxonOps Agent","text":"<p>Work in progress</p>"},{"location":"configuration/axondash/","title":"Configuring AxonOps Agent","text":"<p>Work in progress</p>"},{"location":"configuration/server-configuration/","title":"Configuring AxonOps Server","text":"<p>Work in progress</p>"},{"location":"how-to/backup-restore-notifications/","title":"Setup Backup - Restore Notifications","text":"<p>On the AxonOps application menu, click <code>Operations -&gt; Backups -&gt; Setup</code> and select <code>Notifications</code> tab. </p>"},{"location":"how-to/backup-restore-notifications/#notification-severities","title":"Notification Severities.","text":"<p>Notification Severities.</p> <p>For each notifications severity   Info     Warning     Error you can either use the slider  to use the default routing or use the   icon to customize the notification integrations.</p> <p>Notice:  not available when default routing   selected</p> <p>Infomy</p> <p></p>"},{"location":"how-to/backup-restore-notifications/#customize-notifications","title":"Customize Notifications.","text":"<p>To customize notifications <code>click</code> on  select the integrations that you require and click <code>Close</code>.</p> <p>Infomy</p> <p></p> <p>Noticed: The<code>Warning Integration</code> were customized. You can remove these by clicking the .</p> <p>If you want to remove default routing groups from a severity and create custom groups , use the slider bar to remove default routing <code>click</code> the  and follow this steps</p> <p>If you do not require any notifications <code>ensure</code> the <code>default routing</code> is off  and delete any previously created custom notification.</p> <p>Infomy</p> <p></p>"},{"location":"how-to/default-routing/","title":"Setup Default Routing","text":"<p>Default Routing.</p> <p>Allows you to set up the channels though which alerts &amp; notifications will be received and the specific groups that will receive the alerts &amp; notifications</p>"},{"location":"how-to/default-routing/#setup-default-routing_1","title":"Setup Default Routing","text":"<p>On the Axonops application menu, select <code>Alert &amp; Notifications</code> -&gt; Integration and select<code>Default Routing</code> tab.</p> <ul> <li>Alert &amp; Notification types can be set up</li> </ul> <p>Infomy</p> <p>  Info    Warning    Error </p>"},{"location":"how-to/default-routing/#info","title":"Info","text":"<p>To Setup Default Routing For   Info click On  </p> <ul> <li>Select the desired group(s) from the dropdown menu for the desired integrations(s) and  click`   to confirm selections</li> </ul> <p>Infomy</p> <p> </p> <p>The group should now appear in the   Info  Info box on the <code>Default Routing Tab</code></p> <p>Infomy</p> <p></p>"},{"location":"how-to/default-routing/#warning-error","title":"Warning - Error","text":"<p>Repeat these steps to setup the Default Routing for   Warning    Error </p>"},{"location":"how-to/default-routing/#edit-default-routing","title":"Edit Default Routing","text":"<p>To Edit <code>Default Routing</code> click on the    icon on either    Add or <code>Remove</code> existing integrations using the <code>dropdown</code> menus."},{"location":"how-to/default-routing/#delete-default-routing","title":"Delete Default Routing","text":"<p>To Remove a <code>group</code> <code>click</code> on the <code>Delete</code> icon</p> <p>Infomy</p> <p></p>"},{"location":"how-to/reuse-host-id/","title":"Re-using an existing Host ID","text":"<p>Each agent connected to the AxonOps server is assigned a unique host ID that is used internally to associate metrics and events with the node. If a Cassandra host dies and is replaced by another one with the same IP and token range then normally a new host ID will be generated and the replacement server will appear as a new machine in AxonOps. In this situation it is possible to re-use the same host ID so AxonOps sees the replacement server as the same as the original one.</p> <p>You can find the host ID of a node in the AxonOps GUI by going to Cluster Overview and selecting a node. In Graph View the host ID is shown next to the hostname in the details panel and in List View it is shown as Agent ID at the top of the details popup. If the old server's filesystem is still accessible you can also find the host ID stored in <code>/var/lib/axonops/hostId</code>.</p> <p>Follow these steps to start up a replacement server using the old host ID:</p> <ol> <li>Install axon-agent on the replacement server</li> <li>Stop axon-agent if it is running then delete these files if they exist: <code>/var/lib/axonops/hostId</code>, <code>/var/lib/axonops/local.db</code></li> <li>Create a new file at <code>/var/lib/axonops/hostId</code> containing the host ID you wish to use</li> <li>Start axon-agent</li> </ol>"},{"location":"how-to/setup-alert-rules/","title":"Setup alert rules","text":""},{"location":"how-to/setup-alert-rules/#insert-alert-rules-credentials","title":"Insert Alert Rules Credentials","text":"<p>On the Axonops application menu, click <code>Dashboards</code> and <code>select</code> required Dashboard. eg. <code>System</code></p> <p><code>Hover over</code> the desired Chart <code>click</code> on the   button.</p> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-alert-rules/#complete-the-fields-in-form","title":"Complete The Fields In Form","text":"<ul> <li>Below the chart <code>click</code> on the <code>alert</code> tab.</li> </ul> <p>Infomy</p> <p></p> <ul> <li>A form will appear</li> </ul> <p>Infomy</p> <p></p> <ul> <li>Complete Alert settings in <code>Comparator Warning value</code> or <code>Critical value</code> or Both and the <code>Duration</code> ==&gt; (how often to check) In</li> </ul> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-alert-rules/#annotations","title":"Annotations","text":"<p>In the <code>Summary</code> box you can include free text &amp; type one <code>or</code> many of the following <code>$labels</code></p> <pre><code>$labels:\n- cluster\n- dc\n- hostname\n- org\n- rack\n- type\n- keyspace\n$value:\n</code></pre> <p>In the <code>Description</code> box you can include free along with one <code>or</code> many of the above  <code>$labels</code></p> <p>Example</p> <p><code>CPU usage per DC Alerts usage on {{ $labels.hostname }} and cluster {{$labels.cluster}}</code></p> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-alert-rules/#integrations","title":"Integrations","text":"<ul> <li> <p>Using the slider bar   you can select any Integrations.</p> <p>Then <code>click</code> on the <code>Info</code>, <code>Warning</code>, <code>Error</code> icons, to select the group(s) of Integrations for the alert.</p> </li> </ul> <p>Infomy</p> <p> </p> <p>Not selecting integrations</p> <p>If you do not select any specific Integrations the Alert will automatically follow the <code>Global Dashboard Routing</code> or if this has not been setup the Default Routing Integrations.</p>"},{"location":"how-to/setup-alert-rules/#edit-alert-rule","title":"Edit Alert Rule","text":"<p>On the Axonops application menu, click <code>Alerts &amp; Notifications</code> and <code>click</code> Active. <code>Select</code> the <code>Alert Rules</code> tab and click  </p> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-alert-rules/#delete-alert-rules","title":"Delete Alert Rule(s)","text":"<p>To Delete An Alert Either...</p> <ul> <li>On the Axonops application menu, click <code>Dashboards</code> and <code>select</code> required Dashboard. <code>eg. System</code> <code>Hover over</code> the desired Chart click on the   button. Below the chart <code>click</code> on the <code>alert</code> tab and <code>click</code> on the   of the alert rule you want to remove.</li> </ul> <p>OR...</p> <ul> <li>On the Axonops application menu, click <code>Alerts &amp; Notifications</code> and <code>click</code> Active. <code>Select</code> the Alert Rules tab and click   </li> </ul> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-dashboards-global-integrations/","title":"Setup Dashboards Global Integrations","text":"<p>On the Axonops application menu, click <code>Alerts &amp; Notifications -&gt; Active</code> and select <code>Dashboards Global Routing</code> tab.</p>"},{"location":"how-to/setup-dashboards-global-integrations/#notification-severities","title":"Notification Severities.","text":"<p>Notification Severities.</p> <p>For each notifications severity    Info      Warning      Error you can either use the slider   to use the default routing or use the    icon to customize the notification integrations.</p> <p>Notice:   not available when default routing  selected</p> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-dashboards-global-integrations/#customize-notifications","title":"Customize Notifications.","text":"<p>To customize notifications <code>click</code> on  select the integrations that you require and click <code>Close</code>.</p> <p>Infomy</p> <p></p> <p>Noticed: The<code>Warning Integration</code> were customized. You can remove these by clicking the .</p> <p>If you want to remove default routing groups from a severity and create custom groups , use the slider bar to remove default routing <code>click</code> the   and follow this steps</p> <p>If you do not require any notifications <code>ensure</code> the <code>default routing</code> is off  and delete any previously created custom notification.</p> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-log-collection/","title":"Setup Log Collection","text":"<p>AxonOps dashboards provides a comprehensive set of charts with an embedded view for logs and events. The goal is to correlate metrics with logs/events as you can zoom in the logs histogram or metrics charts to drill down both results. </p> <p>Log and event view is located in the bottom part of that page that you can expand/collapse with the horizontal splitter.</p> <p></p> <p>The setup for log collection is accessible via Settings &gt; logs</p> <p>Infomy</p> <p></p> <p>newlineregex is used by the log collector to handle multilines logs. Default newlineregex for Cassandra should be ok unless you've customized it.</p>"},{"location":"how-to/setup-servicechecks/","title":"Setup Service Checks","text":""},{"location":"how-to/setup-servicechecks/#add-service-checks","title":"Add Service Checks","text":"<p>On the Axonops application menu, click <code>Service Checks</code> and select <code>Setup</code> tab.</p> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-servicechecks/#create-services","title":"Create Services","text":"<p>Below there few examples <code>copy</code> and <code>Paste</code> inside. and click <code>save</code> </p> <pre><code>{\n\"shellchecks\": [\n{\n\"name\" : \"check_elastic\",\n\"shell\" :  \"/bin/bash\",\n\"script\":  \"if [ 'ps auxwww | grep elastic | wc -l' -lt 1 ] then exit 2 else exit 0  fi\",\n\"interval\": \"5m\" ,\n\"timeout\": \"1m\" }\n],\n\n\"httpchecks\": [],\n\"tcpchecks\": [\n{\n\"name\": \"elastic_tcp_endpoint_check\",\n\"interval\": \"5s\",\n\"timeout\": \"1m\",\n\"tcp\": \"localhost:9200\"\n}\n]\n\n}\n</code></pre> <p>Example:</p> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-servicechecks/#delete-services","title":"Delete Services","text":"<p>To Delete a service <code>copy</code> and <code>Paste</code> inside. and <code>click</code> save  </p> <pre><code>{\n\"shellchecks\": [],\n\"httpchecks\": [],\n\"tcpchecks\": []\n\n}\n</code></pre> <p>Example:</p> <p>Infomy</p> <p></p>"},{"location":"installation/axon-agent/install/","title":"axon-agent installation","text":"<p>There 2 elements to the AxonOps agent. The first is the axon-agent, which is a native application for Linux running as a standalone daemon process. The second is the Java agent which is added to the Java process. Two components communicate with each other using the Unix domain socket. The reason for this approach are the following requirements we have on the agent process.</p> <ul> <li>No JMX</li> <li>Metrics must push metrics from Cassandra all the way to the AxonOps server - never pull.</li> </ul> <p>AxonOps Java agent will push the metrics to the AxonOps native agent, which in turn pushes them to the AxonOps server. Scraping a large volume of metrics against the JMX is slow. We also wanted to avoid exposing an HTTP endpoint within Cassandra like the Prometheus JMX exporter does.</p> <p>The messaging between native agent and Java agent is bidirectional, i.e. AxonOps server sends control messages to Cassandra for operations such as repair and backups without the use of JMX.</p> <p>This section describes how to install and configure both the native agent and Java agent.</p>"},{"location":"installation/axon-agent/install/#centos-redhat","title":"CentOS / RedHat","text":"<pre><code>sudo tee /etc/yum.repos.d/axonops-yum.repo &lt;&lt; EOL\n[axonops-yum]\nname=axonops-yum\nbaseurl=https://packages.axonops.com/yum/\nenabled=1\nrepo_gpgcheck=0\ngpgcheck=0\nEOL\n\nsudo yum install axon-agent\n</code></pre>"},{"location":"installation/axon-agent/install/#debian-ubuntu","title":"Debian / Ubuntu","text":"<pre><code>apt-get install curl gnupg\ncurl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add -\necho \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list\nsudo apt-get update\nsudo apt-get install axon-agent\n</code></pre>"},{"location":"installation/axon-agent/install/#package-details","title":"Package details","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-agent.yml</code></li> <li>Binary: <code>usr/share/axonops/axon-agent</code></li> <li>Logs : <code>/var/log/axonops/axon-agent.log</code></li> <li>Systemd service: <code>/usr/lib/systemd/system/axon-agent.service</code></li> <li>certificate file used for it's OpenTSDB endpoint when SSL is active: <code>/etc/axonops/agent.crt</code></li> <li>key file used for it's OpenTSDB endpoint when SSL is active: <code>/etc/axonops/agent.key</code></li> </ul>"},{"location":"installation/axon-agent/install/#configuration","title":"Configuration","text":"<p>Make sure axon-agent configuration points to the correct axon-server address and your organisation name is specified:</p> <pre><code>axon-server:\nhosts: \"axon-server_endpoint\" # Specify axon-server IP axon-server.mycompany.com\naxon-agent:\norg: \"my-company-test\" # Specify your organisation name\ntype: \"cassandra\"\nNTP:\nhost: \"ntp.mycompany.com\" # Specify you NTP server IP address or hostname\n</code></pre>"},{"location":"installation/axon-agent/install/#start-axon-agent","title":"Start axon-agent","text":"<pre><code>systemctl daemon-reload\nsystemctl start axon-agent\nsystemctl status axon-agent\n</code></pre> <p>This will start the axon-agent process as the axonops user, which was created during the package installation.</p> <ul> <li>Note that you will have to refresh axon-dash page to show the newly connected node.</li> </ul>"},{"location":"installation/axon-agent/install/#next-steps","title":"Next Steps","text":"<p>To complete your agent installation you will need to follow the steps in the link below:</p> <ul> <li>cassandra</li> </ul>"},{"location":"installation/axon-dash/install/","title":"AxonOps GUI installation","text":"<p>AxonOps GUI service is installed as a separate service to AxonOps Server. The GUI service (axon-dash) can be co-hosted on the same server as the AxonOps Server process, or they can be running on 2 separate servers.</p> <p>This section describes the installation process for the GUI service.</p>"},{"location":"installation/axon-dash/install/#step-1-installation","title":"Step 1 - Installation","text":""},{"location":"installation/axon-dash/install/#centos-redhat","title":"CentOS / RedHat","text":"<pre><code>sudo tee /etc/yum.repos.d/axonops-yum.repo &lt;&lt; EOL\n[axonops-yum]\nname=axonops-yum\nbaseurl=https://packages.axonops.com/yum/\nenabled=1\nrepo_gpgcheck=0\ngpgcheck=0\nEOL\n\nsudo yum install axon-dash\n</code></pre>"},{"location":"installation/axon-dash/install/#debian-ubuntu","title":"Debian / Ubuntu","text":"<pre><code>apt-get install curl gnupg\ncurl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add -\necho \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list\nsudo apt-get update\nsudo apt-get install axon-dash\n</code></pre>"},{"location":"installation/axon-dash/install/#step-2-configuration","title":"Step 2 - Configuration","text":"<p>Change axon-dash configuration to specify axon-server listening address.</p> <ul> <li><code>/etc/axonops/axon-dash.yml</code></li> </ul> <pre><code>axon-dash: # The listening address of axon-dash\nhost: 0.0.0.0\nport: 3000\nline_charts_max_results: 256\n\naxon-server:\npublic_endpoints: \"http://axon-server.public:8080, https://axon-server.public\" # Public HTTP endpoint to axon-server API. This can be a list with comma separator. http://127.0.0.1 or http://locahost are always wrong.\ncontext_path: \"\" # example: \"/gui\"\n</code></pre> <p>axon-server default listening port is 8080</p>"},{"location":"installation/axon-dash/install/#step-3-axon-server-configuration-update","title":"Step 3 - axon-server configuration update","text":"<p>Update axon-server configuration by setting the correct axon-dash host and port:</p> <ul> <li><code>/etc/axonops/axon-server.yml</code></li> </ul> <pre><code>host: 0.0.0.0  # axon-server listening address \nport: 8080 # axon-server listening port \nelastic_host: http://localhost # Elasticsearch endpoint\nelastic_port: 9200 # Elasticsearch port\n\naxon-dash: # This must point to axon-dash address\nhost: 127.0.0.1\nport: 3000\nhttps: false\n\nalerting:\n# How long to wait before sending a notification again if it has already\n# been sent successfully for an alert. (Usually ~3h or more).\nnotification_interval: 3h\n\nretention:\nevents: 8w # logs and events retention. Must be expressed in weeks (w)\nmetrics:\nhigh_resolution: 14d # High frequency metrics. Must be expressed in days (d)\nmed_resolution: 12w # Must be expressed in weeks (w)\nlow_resolution: 12M # Must be expressed in months (M)\nsuper_low_resolution: 2y # Must be expressed in years (y)\nbackups: # Those are use as defaults but can be overridden from the UI\nlocal: 10d\nremote: 30d\n</code></pre>"},{"location":"installation/axon-dash/install/#step-4-restart-axon-server-after-updating-its-configuration","title":"Step 4 - Restart axon-server after updating its configuration","text":"<pre><code>sudo systemctl restart axon-server\n</code></pre>"},{"location":"installation/axon-dash/install/#step-5-start-axon-dash","title":"Step 5 - Start axon-dash","text":"<pre><code>sudo systemctl daemon-reload\nsudo systemctl start axon-dash\nsudo systemctl status axon-dash\n</code></pre> <p>This will start the axon-dash process as the axonops user, which was created during the package installation. The default listening address is <code>0.0.0.0:3000</code>.</p>"},{"location":"installation/axon-dash/install/#package-details","title":"Package details","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-dash.yml</code></li> <li>Binary: <code>/usr/share/axonops/axon-dash</code></li> <li>Logs: <code>/var/log/axonops/axon-dash.log</code> </li> <li>Systemd service: <code>/usr/lib/systemd/system/axon-dash.service</code></li> <li>Copyright : <code>/usr/share/doc/axonops/axon-dash/copyright</code></li> <li>Licenses : <code>/usr/share/axonops/licenses/axon-dash/</code></li> </ul>"},{"location":"installation/axon-dash/install/#step-6-installing-agents","title":"Step 6 - Installing agents","text":"<p>Now axon-dash is installed, you can start installing cassandra-agent</p>"},{"location":"installation/axon-server/centos/","title":"axon-server installation (CentOS / RedHat)","text":""},{"location":"installation/axon-server/centos/#step-1-prerequisites","title":"Step 1 - Prerequisites","text":"<p>Elasticsearch stores all of the data collected by axon-server. Let's install Java 8 and Elasticsearch first.</p>"},{"location":"installation/axon-server/centos/#installing-jdk","title":"Installing JDK","text":"<p>Elasticsearch supports either OpenJDK or Oracle JDK. Since Oracle has changed the licensing model as of January 2019 we suggest using OpenJDK.</p> <p>Run the following commands for OpenJDK:</p> <pre><code>sudo yum install java-1.8.0-openjdk-devel\n</code></pre> <p>Run the following commands for Oracle JDK:</p> <pre><code>wget -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.rpm\n</code></pre> <pre><code>sudo rpm -i jdk-8u131-linux-x64.rpm\n</code></pre> <p>Increase the bulk queue size of Elasticsearch by running the following command:</p>"},{"location":"installation/axon-server/centos/#elastic-7","title":"Elastic 7+:","text":"<pre><code>sudo echo 'thread_pool.write.queue_size: 2000' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre>"},{"location":"installation/axon-server/centos/#elastic-6x","title":"Elastic 6.x:","text":"<pre><code>sudo echo 'thread_pool.bulk.queue_size: 2000' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre> <p>Increase the default heap size of elasticsearch by editing <code>/etc/elasticsearch/jvm.options</code>. From:</p> <pre><code>-Xms1g\n-Xmx1g </code></pre> <p>To: </p> <pre><code>-Xms8g\n-Xmx8g </code></pre> <p>This will set the minimum and maximum heap size to 8 GB. Set Xmx and Xms to no more than 50% of your physical RAM. Elasticsearch requires memory for purposes other than the JVM heap and it is important to leave space for this.</p> <p>Set the following index codec by running the following command:</p> <pre><code>sudo echo 'index.codec: best_compression' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre> <p>Elasticsearch uses an mmapfs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions.</p> <p>You can increase the limits by running the following command:</p> <pre><code>sudo sysctl -w vm.max_map_count=262144\n</code></pre> <p>Also, Elasticsearch needs <code>max file descriptors</code> system settings at least to 65536.</p> <pre><code>echo 'elasticsearch  -  nofile  65536' | sudo tee --append /etc/security/limits.conf &gt; /dev/null\n</code></pre> <p>And set <code>LimitNOFILE=65536</code> in <code>/etc/systemd/system/elasticsearch.services</code> </p>"},{"location":"installation/axon-server/centos/#start-elasticsearch","title":"Start Elasticsearch","text":"<pre><code>sudo systemctl start elasticsearch.service\n</code></pre> <p>After a short period of time, you can verify that your Elasticsearch node is running by sending an HTTP request to port 9200 on localhost:</p> <pre><code>curl -X GET \"localhost:9200/\"\n</code></pre>"},{"location":"installation/axon-server/centos/#step-2-axon-server","title":"Step 2 - axon-server","text":"<pre><code>sudo tee /etc/yum.repos.d/axonops-yum.repo &lt;&lt; EOL\n[axonops-yum]\nname=axonops-yum\nbaseurl=https://packages.axonops.com/yum/\nenabled=1\nrepo_gpgcheck=0\ngpgcheck=0\nEOL\n\nsudo yum install axon-server\n</code></pre>"},{"location":"installation/axon-server/centos/#step-3-axon-server-configurations","title":"Step 3 - axon-server configurations","text":"<p>Make sure elastic_host and elastic_port are corresponding to your Elasticsearch instance.</p> <ul> <li><code>/etc/axonops/axon-server.yml</code></li> </ul> <pre><code>host: 0.0.0.0  # axon-server listening address (used by axon-dash and axon-agent) (env variable: AXONSERVER_HOST)\nport: 8080 # axon-server HTTP API listening port (used by axon-dash) (AXONSERVER_PORT)\nelastic_hosts: #\u00a0Elasticsearch endpoint (env variable:ELASTIC_HOSTS, comma separated list)\n-http://localhost #integrations_proxy: # proxy endpoint for integrations. (INTEGRATIONS_PROXY)\n\n# For better performance on large clusters, you can use a CQL store for the metrics.\n# To opt-in for CQL metrics storage, just specify at least one CQL host.\n# We do recommend to specify a NetworkTopologyStrategy for cql_keyspace_replication\n#cql_hosts: #  (CQL_HOSTS, comma separated list)\n#  - 192.168.0.10:9042\n#  - 192.168.0.11:9042\n#cql_username: \"cassandra\" # (CQL_USERNAME)\n#cql_password: \"cassandra\" # (CQL_PASSWORD)\n#cql_local_dc: datacenter1 # (CQL_LOCAL_DC)\n#cql_ssl: false # (CQL_SSL)\n#cql_skip_verify: false  # (CQL_SSL_SKIP_VERIFY)\n#cql_ca_file: /path/to/ca_file  # (CQL_CA_FILE)\n#cql_cert_file: /path/to/cert_file  # (CQL_CERT_FILE)\n#cql_key_file: /path/to/key_file  # (CQL_KEY_FILE)\n#cql_proto_version: 4  # (CQL_PROTO_VERSION)\n#cql_max_concurrent_reads: 1000  # (CQL_MAX_CONCURRENT_READS)\n#cql_batch_size: 1  # (CQL_BATCH_SIZE)\n#cql_page_size: 10  # (CQL_PAGE_SIZE)\n#cql_autocreate_tables: true # (CQL_AUTO_CREATE_TABLES) this will tell axon-server to automatically create the metrics tables (true is recommended)\n#cql_keyspace_replication: \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\" # (CQL_KS_REPLICATION) keyspace replication for the metrics tables\n#cql_retrypolicy_numretries: 3  # (CQL_RETRY_POLICY_NUM_RETRIES)\n#cql_retrypolicy_min: 1s  # (CQL_RETRY_POLICY_MIN)\n#cql_retrypolicy_max: 10s  # (CQL_RETRY_POLICY_MAX)\n#cql_reconnectionpolicy_maxretries: 10 # (CQL_RECONNECTION_POLICY_MAX_RETRIES)\n#cql_reconnectionpolicy_initialinterval: 1s # (CQL_RECONNECTION_POLICY_INITIAL_INTERVAL)\n#cql_reconnectionpolicy_maxinterval: 10s # (CQL_RECONNECTION_POLICY_MAX_INTERVAL)\n#cql_metrics_cache_max_size_mb: 100  #MB # (CQL_METRICS_CACHE_MAX_SIZE_MB)\n#cql_read_consistency: \"LOCAL_ONE\" # (CQL_READ_CONSISTENCY) #One of the following:  ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE\n#cql_write_consistency: \"LOCAL_ONE\" # (CQL_WRITE_CONSISTENCY) #One of the following:    ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE\n#cql_lvl1_compaction_window_size: 12 # (CQL_LVL1_COMPACTION_WINDOW_SIZE)\n#cql_lvl2_compaction_window_size: 1 # (CQL_LVL2_COMPACTION_WINDOW_SIZE)\n#cql_lvl3_compaction_window_size: 1 # (CQL_LVL3_COMPACTION_WINDOW_SIZE)\n#cql_lvl4_compaction_window_size: 10 # (CQL_LVL4_COMPACTION_WINDOW_SIZE)\n#cql_lvl5_compaction_window_size: 120 # (CQL_LVL5_COMPACTION_WINDOW_SIZE)\n\naxon-dash: # This must point to axon-dash address\nhost: 127.0.0.1\nport: 3000\nhttps: false\n\nalerting:\n# How long to wait before sending a notification again if it has already\n# been sent successfully for an alert. (Usually ~3h or more).\nnotification_interval: 3h\n\nretention:\nevents: 8w # logs and events retention. Must be expressed in weeks (w)\nmetrics:\nhigh_resolution: 14d # High frequency metrics. Must be expressed in days (d)\nmed_resolution: 12w # Must be expressed in weeks (w)\nlow_resolution: 12M # Must be expressed in months (M)\nsuper_low_resolution: 2y # Must be expressed in years (y)\nbackups: # Those are use as defaults but can be overridden from the UI\nlocal: 10d\nremote: 30d\n\n\n# Storage options for PDF reports\n# Override the default local path of /var/lib/axonops/reports\n#report_storage_path: /my/reports/storage/directory\n\n# Alternatively store PDF reports in an object store by providing report_storage_config\n#report_storage_path: my-reports-s3-bucket/reports-folder\n#report_storage_config:\n#  type: s3\n#  provider: AWS\n#  access_key_id: MY_ACCESS_KEY_ID\n#  secret_access_key: MY_SECRET_ACCESS_KEY\n#  region: us-east-1\n#  acl: private\n#  server_side_encryption: AES256\n#  storage_class: STANDARD\n</code></pre> <p>For better performances on large clusters (100+ nodes), you can use a CQL store for the metrics such as Cassandra, Scylla or Elassandra. To opt-in for CQL metrics storage, just specify at least one CQL host with axon-server configuration.</p>"},{"location":"installation/axon-server/centos/#step-4-start-the-server","title":"Step 4 - Start the server","text":"<pre><code>sudo systemctl daemon-reload\nsudo systemctl start axon-server\nsudo systemctl status axon-server\n</code></pre> <p>This will start the <code>axon-server</code> process as the <code>axonops</code> user, which was created during the package installation.  The default listening address is <code>0.0.0.0:8080</code>.</p>"},{"location":"installation/axon-server/centos/#package-details","title":"Package details","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-server.yml</code></li> <li>Binary: <code>/usr/share/axonops/axon-server</code></li> <li>Logs: <code>/var/log/axonops/axon-server.log</code> </li> <li>Systemd service: <code>/usr/lib/systemd/system/axon-server.service</code></li> <li>Copyright : <code>/usr/share/doc/axonops/axon-server/copyright</code></li> <li>Licenses : <code>/usr/share/axonops/licenses/axon-server/</code></li> </ul>"},{"location":"installation/axon-server/centos/#step-5-installing-axon-dash","title":"Step 5 - Installing axon-dash","text":"<p>Now axon-server is installed, you can start installing the GUI for it: axon-dash</p>"},{"location":"installation/axon-server/elastic/","title":"Elastic","text":"<p>Increase the bulk queue size of Elasticsearch by running the following command:</p>"},{"location":"installation/axon-server/elastic/#elastic-7","title":"Elastic 7+:","text":"<pre><code>sudo echo 'thread_pool.write.queue_size: 2000' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre>"},{"location":"installation/axon-server/elastic/#elastic-6x","title":"Elastic 6.x:","text":"<pre><code>sudo echo 'thread_pool.bulk.queue_size: 2000' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre> <p>Increase the default heap size of elasticsearch by editing <code>/etc/elasticsearch/jvm.options</code>. From:</p> <pre><code>-Xms1g\n-Xmx1g </code></pre> <p>To: </p> <pre><code>-Xms8g\n-Xmx8g </code></pre> <p>This will set the minimum and maximum heap size to 8 GB. Set Xmx and Xms to no more than 50% of your physical RAM. Elasticsearch requires memory for purposes other than the JVM heap and it is important to leave space for this.</p> <p>Set the following index codec by running the following command:</p> <pre><code>sudo echo 'index.codec: best_compression' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre> <p>Elasticsearch uses an mmapfs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions.</p> <p>You can increase the limits by running the following command:</p> <pre><code>sudo sysctl -w vm.max_map_count=262144\n</code></pre> <p>Also, Elasticsearch needs <code>max file descriptors</code> system settings at least to 65536.</p> <pre><code>echo 'elasticsearch  -  nofile  65536' | sudo tee --append /etc/security/limits.conf &gt; /dev/null\n</code></pre> <p>And set <code>LimitNOFILE=65536</code> in <code>/etc/systemd/system/elasticsearch.services</code> </p>"},{"location":"installation/axon-server/elastic/#start-elasticsearch","title":"Start Elasticsearch","text":"<pre><code>sudo systemctl start elasticsearch.service\n</code></pre> <p>After a short period of time, you can verify that your Elasticsearch node is running by sending an HTTP request to port 9200 on localhost:</p> <pre><code>curl -X GET \"localhost:9200/\"\n</code></pre>"},{"location":"installation/axon-server/install/","title":"Install","text":""},{"location":"installation/axon-server/install/#step-3-axon-server-configurations","title":"Step 3 - axon-server configurations","text":"<p>Make sure elastic_host and elastic_port are corresponding to your Elasticsearch instance.</p> <ul> <li><code>/etc/axonops/axon-server.yml</code></li> </ul> <pre><code>host: 0.0.0.0  # axon-server listening address (used by axon-dash and axon-agent) (env variable: AXONSERVER_HOST)\nport: 8080 # axon-server HTTP API listening port (used by axon-dash) (AXONSERVER_PORT)\nelastic_hosts: #\u00a0Elasticsearch endpoint (env variable:ELASTIC_HOSTS, comma separated list)\n-http://localhost #integrations_proxy: # proxy endpoint for integrations. (INTEGRATIONS_PROXY)\n\n# For better performance on large clusters, you can use a CQL store for the metrics.\n# To opt-in for CQL metrics storage, just specify at least one CQL host.\n# We do recommend to specify a NetworkTopologyStrategy for cql_keyspace_replication\n#cql_hosts: #  (CQL_HOSTS, comma separated list)\n#  - 192.168.0.10:9042\n#  - 192.168.0.11:9042\n#cql_username: \"cassandra\" # (CQL_USERNAME)\n#cql_password: \"cassandra\" # (CQL_PASSWORD)\n#cql_local_dc: datacenter1 # (CQL_LOCAL_DC)\n#cql_ssl: false # (CQL_SSL)\n#cql_skip_verify: false  # (CQL_SSL_SKIP_VERIFY)\n#cql_ca_file: /path/to/ca_file  # (CQL_CA_FILE)\n#cql_cert_file: /path/to/cert_file  # (CQL_CERT_FILE)\n#cql_key_file: /path/to/key_file  # (CQL_KEY_FILE)\n#cql_proto_version: 4  # (CQL_PROTO_VERSION)\n#cql_max_concurrent_reads: 1000  # (CQL_MAX_CONCURRENT_READS)\n#cql_batch_size: 1  # (CQL_BATCH_SIZE)\n#cql_page_size: 10  # (CQL_PAGE_SIZE)\n#cql_autocreate_tables: true # (CQL_AUTO_CREATE_TABLES) this will tell axon-server to automatically create the metrics tables (true is recommended)\n#cql_keyspace_replication: \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\" # (CQL_KS_REPLICATION) keyspace replication for the metrics tables\n#cql_retrypolicy_numretries: 3  # (CQL_RETRY_POLICY_NUM_RETRIES)\n#cql_retrypolicy_min: 1s  # (CQL_RETRY_POLICY_MIN)\n#cql_retrypolicy_max: 10s  # (CQL_RETRY_POLICY_MAX)\n#cql_reconnectionpolicy_maxretries: 10 # (CQL_RECONNECTION_POLICY_MAX_RETRIES)\n#cql_reconnectionpolicy_initialinterval: 1s # (CQL_RECONNECTION_POLICY_INITIAL_INTERVAL)\n#cql_reconnectionpolicy_maxinterval: 10s # (CQL_RECONNECTION_POLICY_MAX_INTERVAL)\n#cql_metrics_cache_max_size_mb: 100  #MB # (CQL_METRICS_CACHE_MAX_SIZE_MB)\n#cql_read_consistency: \"LOCAL_ONE\" # (CQL_READ_CONSISTENCY) #One of the following:  ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE\n#cql_write_consistency: \"LOCAL_ONE\" # (CQL_WRITE_CONSISTENCY) #One of the following:    ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE\n#cql_lvl1_compaction_window_size: 12 # (CQL_LVL1_COMPACTION_WINDOW_SIZE)\n#cql_lvl2_compaction_window_size: 1 # (CQL_LVL2_COMPACTION_WINDOW_SIZE)\n#cql_lvl3_compaction_window_size: 1 # (CQL_LVL3_COMPACTION_WINDOW_SIZE)\n#cql_lvl4_compaction_window_size: 10 # (CQL_LVL4_COMPACTION_WINDOW_SIZE)\n#cql_lvl5_compaction_window_size: 120 # (CQL_LVL5_COMPACTION_WINDOW_SIZE)\n\naxon-dash: # This must point to axon-dash address\nhost: 127.0.0.1\nport: 3000\nhttps: false\n\nalerting:\n# How long to wait before sending a notification again if it has already\n# been sent successfully for an alert. (Usually ~3h or more).\nnotification_interval: 3h\n\nretention:\nevents: 8w # logs and events retention. Must be expressed in weeks (w)\nmetrics:\nhigh_resolution: 14d # High frequency metrics. Must be expressed in days (d)\nmed_resolution: 12w # Must be expressed in weeks (w)\nlow_resolution: 12M # Must be expressed in months (M)\nsuper_low_resolution: 2y # Must be expressed in years (y)\nbackups: # Those are use as defaults but can be overridden from the UI\nlocal: 10d\nremote: 30d\n\n\n# Storage options for PDF reports\n# Override the default local path of /var/lib/axonops/reports\n#report_storage_path: /my/reports/storage/directory\n\n# Alternatively store PDF reports in an object store by providing report_storage_config\n#report_storage_path: my-reports-s3-bucket/reports-folder\n#report_storage_config:\n#  type: s3\n#  provider: AWS\n#  access_key_id: MY_ACCESS_KEY_ID\n#  secret_access_key: MY_SECRET_ACCESS_KEY\n#  region: us-east-1\n#  acl: private\n#  server_side_encryption: AES256\n#  storage_class: STANDARD\n</code></pre> <p>For better performances on large clusters (100+ nodes), you can use a CQL store for the metrics such as Cassandra, Scylla or Elassandra. To opt-in for CQL metrics storage, just specify at least one CQL host with axon-server configuration.</p>"},{"location":"installation/axon-server/install/#step-4-start-the-server","title":"Step 4 - Start the server","text":"<pre><code>sudo systemctl daemon-reload\nsudo systemctl start axon-server\nsudo systemctl status axon-server\n</code></pre> <p>This will start the <code>axon-server</code> process as the <code>axonops</code> user, which was created during the package installation.  The default listening address is <code>0.0.0.0:8080</code>.</p>"},{"location":"installation/axon-server/install/#package-details","title":"Package details","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-server.yml</code></li> <li>Binary: <code>/usr/share/axonops/axon-server</code></li> <li>Logs: <code>/var/log/axonops/axon-server.log</code> </li> <li>Systemd service: <code>/usr/lib/systemd/system/axon-server.service</code></li> <li>Copyright : <code>/usr/share/doc/axonops/axon-server/copyright</code></li> <li>Licenses : <code>/usr/share/axonops/licenses/axon-server/</code></li> </ul>"},{"location":"installation/axon-server/install/#step-5-installing-axon-dash","title":"Step 5 - Installing axon-dash","text":"<p>Now axon-server is installed, you can start installing the GUI for it: axon-dash</p>"},{"location":"installation/axon-server/metricsdatabase/","title":"axon-server metrics database","text":""},{"location":"installation/axon-server/metricsdatabase/#using-cassandra-as-a-metrics-store","title":"Using Cassandra as a metrics store","text":"<p>You can use Cassandra as a metrics store instead of Elasticsearch for better performances. To start using a CQL store you just have to specify CQL hosts in axon-server.yml:</p> <pre><code>cql_hosts :\n    - 192.168.0.1:9042\n    - 192.168.0.2:9042\n    ...\n</code></pre> <p>By default, the AxonOps server automatically creates the necessary keyspace and tables. You can override this behavior by specifying the following field in axon-server.yml:</p> <pre><code>cql_autocreate_tables : false\ncql_keyspace : \"axonops\"\ncql_keyspace_replication : \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\n</code></pre> <p>We recommend setting up at least a 3 nodes cluster with NetworkTopologyStrategy and a replication_factor of 3.</p>"},{"location":"installation/axon-server/metricsdatabase/#connecting-to-encrypted-cassandra-metrics-store","title":"Connecting to encrypted Cassandra metrics store","text":"<p>We recommend setting up a Secured Socket Layer connection to Cassandra with the following fields in axon-server.yml:</p> <pre><code>cql_ssl: true\ncql_skip_verify: false\ncql_ca_file: '/path/to/ca_cert'\ncql_cert_file: '/path/to/cert_file'\ncql_key_file: '/path/to/key_file'\n</code></pre>"},{"location":"installation/axon-server/metricsdatabase/#metrics-cache-recommendations","title":"Metrics cache recommendations","text":"<p>When using Cassandra as a metrics store the AxonOps server can cache metrics data in memory to further improve performance. This is configured using the <code>cql_metrics_cache_max_items</code> and <code>cql_metrics_cache_max_size_mb</code> options  in axon-server.yml. The default values are shown here:</p> <pre><code>cql_metrics_cache_max_items: 5000000\ncql_metrics_cache_max_size_mb: 400\n</code></pre> <p>The recommended settings for these options are as follows:</p> Cassandra Nodes cql_metrics_cache_max_items cql_metrics_cache_max_size_mb &lt;10 5000000 1000 &lt;50 5000000 4000 100 10000000 10000 200 10000000 20000 <p>These sizes can be tuned to balance memory use in AxonOps against the read workload on Cassandra. When tuning these parameters it is recommended to set <code>cql_metrics_cache_max_items</code> to a high value and limit the cache size with <code>cql_metrics_cache_max_size_mb</code>.</p>"},{"location":"installation/axon-server/metricsdatabase/#other-cql-fields","title":"Other CQL fields","text":"<p>You can also specify the following fields:</p> <pre><code>cql_proto_version int                   \ncql_batch_size  int                   \ncql_page_size int                   \ncql_local_dc string                \ncql_username string                \ncql_password string                \ncql_max_concurrent_reads int                   \ncql_retrypolicy_numretries int                   \ncql_retrypolicy_min string \"1s\"\ncql_retrypolicy_max string \"10s\"\ncql_reconnectionpolicy_maxretries int                   \ncql_reconnectionpolicy_initialinterval string \"1s\"\ncql_reconnectionpolicy_maxinterval string  \"10s\"\ncql_metrics_cache_max_size_mb int64 in MB               \ncql_metrics_cache_max_items  int64 in MB                        \ncql_read_consistency string (controls the consistency of read operations, defaults to LOCAL_ONE)              \ncql_write_consistency string (controls the consistency of write operations, defaults to LOCAL_ONE)               \ncql_lvl1_compaction_window_size int (used for the table named 'metrics5' when you let axonserver managing the tables automatically)                  \ncql_lvl2_compaction_window_size int (used for the table named 'metrics60' when you let axonserver managing the tables automatically)                  \ncql_lvl3_compaction_window_size int (used for the table named 'metrics720' when you let axonserver managing the tables automatically)                  \ncql_lvl4_compaction_window_size int (used for the table named 'metrics7200' when you let axonserver managing the tables automatically)                  \ncql_lvl5_compaction_window_size int (used for the table named 'metrics86400' when you let axonserver managing the tables automatically)                  </code></pre> <p>The CQL for the default tables are the following:</p> <pre><code>CREATE TABLE IF NOT EXISTS axonops.metrics5 (\norgid text,\n    metricid int,\n    time int,\n    value float,\n    PRIMARY KEY ((orgid, metricid), time)\n) WITH CLUSTERING ORDER BY (time DESC)\nAND caching = {'keys': 'ALL', 'rows_per_partition': '256'}\nAND compaction = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_size': '1', 'compaction_window_unit': 'DAYS', 'max_threshold': '32', 'min_threshold': '4'}\nAND default_time_to_live = 604800\nAND comment = '7 days retention for 5 seconds resolution metrics';\n\nCREATE TABLE IF NOT EXISTS axonops.metrics60 (\norgid text,\n    metricid int,\n    time int,\n    value float,\n    PRIMARY KEY ((orgid, metricid), time)\n) WITH CLUSTERING ORDER BY (time DESC)\nAND caching = {'keys': 'ALL', 'rows_per_partition': '256'}\nAND compaction = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_size': '1', 'compaction_window_unit': 'DAYS', 'max_threshold': '32', 'min_threshold': '4'}\nAND default_time_to_live = 2592000\nAND comment = '30 days retention for 60 seconds resolution metrics';\n\nCREATE TABLE IF NOT EXISTS axonops.metrics720 (\norgid text,\n    metricid int,\n    time int,\n    value float,\n    PRIMARY KEY ((orgid, metricid), time)\n) WITH CLUSTERING ORDER BY (time DESC)\nAND caching = {'keys': 'ALL', 'rows_per_partition': '256'}\nAND compaction = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_size': '4', 'compaction_window_unit': 'DAYS', 'max_threshold': '32', 'min_threshold': '4'}\nAND default_time_to_live = 5184000\nAND comment = '60 days retention for 720 seconds resolution metrics';\n\nCREATE TABLE IF NOT EXISTS axonops.metrics7200 (\norgid text,\n    metricid int,\n    time int,\n    value float,\n    PRIMARY KEY ((orgid, metricid), time)\n) WITH CLUSTERING ORDER BY (time DESC)\nAND caching = {'keys': 'ALL', 'rows_per_partition': '256'}\nAND compaction = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_size': '30', 'compaction_window_unit': 'DAYS', 'max_threshold': '32', 'min_threshold': '4'}\nAND default_time_to_live = 15552000\nAND comment = '180 days retention for 7200 seconds resolution metrics';\n\nCREATE TABLE IF NOT EXISTS axonops.metrics86400 (\norgid text,\n    metricid int,\n    time int,\n    value float,\n    PRIMARY KEY ((orgid, metricid), time)\n) WITH CLUSTERING ORDER BY (time DESC)\nAND caching = {'keys': 'ALL', 'rows_per_partition': '365'}\nAND compaction = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_size': '60', 'compaction_window_unit': 'DAYS', 'max_threshold': '32', 'min_threshold': '4'}\nAND default_time_to_live = 31536000\nAND comment = '365 days retention for 86400 seconds resolution metrics';\n</code></pre>"},{"location":"installation/axon-server/ubuntu/","title":"axon-server installation (Debian / Ubuntu)","text":""},{"location":"installation/axon-server/ubuntu/#step-1-prerequisites","title":"Step 1 - Prerequisites","text":"<p>Elasticsearch stores the data collected by axon-server. Let's install Java 8 and Elasticsearch first.</p>"},{"location":"installation/axon-server/ubuntu/#installing-jdk","title":"Installing JDK","text":"<p>Elasticsearch supports either OpenJDK or Oracle JDK. Since Oracle has changed the licensing model as of January 2019 we suggest using OpenJDK.</p> <p>Run the following commands for OpenJDK:</p> <pre><code>sudo apt-get update\nsudo apt-get install default-jdk\n</code></pre> <p>Run the following commands for Oracle JDK:</p> <pre><code>sudo apt-get update\nsudo apt-get install dirmngr\nsudo cp /etc/apt/sources.list /etc/apt/sources.list_backup\necho \"deb http://ppa.launchpad.net/webupd8team/java/ubuntu xenial main\" | sudo tee /etc/apt/sources.list.d/webupd8team-java.list\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys EEA14886\nsudo apt-get update\nsudo apt-get install oracle-java8-installer\n</code></pre> <p>Once you've accepted the license agreement the JDK will install.</p>"},{"location":"installation/axon-server/ubuntu/#installing-elasticsearch","title":"Installing Elasticsearch","text":"<pre><code>wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.deb\nsudo dpkg -i elasticsearch-6.6.1.deb\n</code></pre> <p>Increase the bulk queue size of Elasticsearch by running the following command:</p>"},{"location":"installation/axon-server/ubuntu/#elastic-7","title":"Elastic 7+:","text":"<pre><code>sudo echo 'thread_pool.write.queue_size: 2000' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre>"},{"location":"installation/axon-server/ubuntu/#elastic-6x","title":"Elastic 6.x:","text":"<pre><code>sudo echo 'thread_pool.bulk.queue_size: 2000' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre> <p>Increase the default heap size of elasticsearch by editing <code>/etc/elasticsearch/jvm.options</code>. From:</p> <pre><code>-Xms1g\n-Xmx1g </code></pre> <p>To: </p> <pre><code>-Xms8g\n-Xmx8g </code></pre> <p>This will set the minimum and maximum heap size to 8 GB. Set Xmx and Xms to no more than 50% of your physical RAM. Elasticsearch requires memory for purposes other than the JVM heap and it is important to leave space for this.</p> <p>Set the following index codec by running the following command:</p> <pre><code>sudo echo 'index.codec: best_compression' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre> <p>Elasticsearch uses an mmapfs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions.</p> <p>You can increase the limits by running the following command:</p> <pre><code>sudo sysctl -w vm.max_map_count=262144\n</code></pre> <p>Also, Elasticsearch needs <code>max file descriptors</code> system settings at least to 65536.</p> <pre><code>echo 'elasticsearch  -  nofile  65536' | sudo tee --append /etc/security/limits.conf &gt; /dev/null\n</code></pre> <p>And set <code>LimitNOFILE=65536</code> in <code>/etc/systemd/system/elasticsearch.services</code> </p>"},{"location":"installation/axon-server/ubuntu/#start-elasticsearch","title":"Start Elasticsearch","text":"<pre><code>sudo systemctl start elasticsearch.service\n</code></pre> <p>After a short period of time, you can verify that your Elasticsearch node is running by sending an HTTP request to port 9200 on localhost:</p> <pre><code>curl -X GET \"localhost:9200/\"\n</code></pre>"},{"location":"installation/axon-server/ubuntu/#step-2-axon-server","title":"Step 2 - axon-server","text":"<pre><code>apt-get install curl gnupg\ncurl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add -\necho \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list\nsudo apt-get update\nsudo apt-get install axon-server\n</code></pre>"},{"location":"installation/axon-server/ubuntu/#step-3-axon-server-configurations","title":"Step 3 - axon-server configurations","text":"<p>Make sure elastic_host and elastic_port are corresponding to your Elasticsearch instance.</p> <ul> <li><code>/etc/axonops/axon-server.yml</code></li> </ul> <pre><code>host: 0.0.0.0  # axon-server listening address (used by axon-dash and axon-agent) (env variable: AXONSERVER_HOST)\nport: 8080 # axon-server HTTP API listening port (used by axon-dash) (AXONSERVER_PORT)\nelastic_hosts: #\u00a0Elasticsearch endpoint (env variable:ELASTIC_HOSTS, comma separated list)\n-http://localhost #integrations_proxy: # proxy endpoint for integrations. (INTEGRATIONS_PROXY)\n\n# For better performance on large clusters, you can use a CQL store for the metrics.\n# To opt-in for CQL metrics storage, just specify at least one CQL host.\n# We do recommend to specify a NetworkTopologyStrategy for cql_keyspace_replication\n#cql_hosts: #  (CQL_HOSTS, comma separated list)\n#  - 192.168.0.10:9042\n#  - 192.168.0.11:9042\n#cql_username: \"cassandra\" # (CQL_USERNAME)\n#cql_password: \"cassandra\" # (CQL_PASSWORD)\n#cql_local_dc: datacenter1 # (CQL_LOCAL_DC)\n#cql_ssl: false # (CQL_SSL)\n#cql_skip_verify: false  # (CQL_SSL_SKIP_VERIFY)\n#cql_ca_file: /path/to/ca_file  # (CQL_CA_FILE)\n#cql_cert_file: /path/to/cert_file  # (CQL_CERT_FILE)\n#cql_key_file: /path/to/key_file  # (CQL_KEY_FILE)\n#cql_proto_version: 4  # (CQL_PROTO_VERSION)\n#cql_max_concurrent_reads: 1000  # (CQL_MAX_CONCURRENT_READS)\n#cql_batch_size: 1  # (CQL_BATCH_SIZE)\n#cql_page_size: 10  # (CQL_PAGE_SIZE)\n#cql_autocreate_tables: true # (CQL_AUTO_CREATE_TABLES) this will tell axon-server to automatically create the metrics tables (true is recommended)\n#cql_keyspace_replication: \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\" # (CQL_KS_REPLICATION) keyspace replication for the metrics tables\n#cql_retrypolicy_numretries: 3  # (CQL_RETRY_POLICY_NUM_RETRIES)\n#cql_retrypolicy_min: 1s  # (CQL_RETRY_POLICY_MIN)\n#cql_retrypolicy_max: 10s  # (CQL_RETRY_POLICY_MAX)\n#cql_reconnectionpolicy_maxretries: 10 # (CQL_RECONNECTION_POLICY_MAX_RETRIES)\n#cql_reconnectionpolicy_initialinterval: 1s # (CQL_RECONNECTION_POLICY_INITIAL_INTERVAL)\n#cql_reconnectionpolicy_maxinterval: 10s # (CQL_RECONNECTION_POLICY_MAX_INTERVAL)\n#cql_metrics_cache_max_size_mb: 100  #MB # (CQL_METRICS_CACHE_MAX_SIZE_MB)\n#cql_read_consistency: \"LOCAL_ONE\" # (CQL_READ_CONSISTENCY) #One of the following:  ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE\n#cql_write_consistency: \"LOCAL_ONE\" # (CQL_WRITE_CONSISTENCY) #One of the following:    ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE\n#cql_lvl1_compaction_window_size: 12 # (CQL_LVL1_COMPACTION_WINDOW_SIZE)\n#cql_lvl2_compaction_window_size: 1 # (CQL_LVL2_COMPACTION_WINDOW_SIZE)\n#cql_lvl3_compaction_window_size: 1 # (CQL_LVL3_COMPACTION_WINDOW_SIZE)\n#cql_lvl4_compaction_window_size: 10 # (CQL_LVL4_COMPACTION_WINDOW_SIZE)\n#cql_lvl5_compaction_window_size: 120 # (CQL_LVL5_COMPACTION_WINDOW_SIZE)\n\naxon-dash: # This must point to axon-dash address\nhost: 127.0.0.1\nport: 3000\nhttps: false\n\nalerting:\n# How long to wait before sending a notification again if it has already\n# been sent successfully for an alert. (Usually ~3h or more).\nnotification_interval: 3h\n\nretention:\nevents: 8w # logs and events retention. Must be expressed in weeks (w)\nmetrics:\nhigh_resolution: 14d # High frequency metrics. Must be expressed in days (d)\nmed_resolution: 12w # Must be expressed in weeks (w)\nlow_resolution: 12M # Must be expressed in months (M)\nsuper_low_resolution: 2y # Must be expressed in years (y)\nbackups: # Those are use as defaults but can be overridden from the UI\nlocal: 10d\nremote: 30d\n\n\n# Storage options for PDF reports\n# Override the default local path of /var/lib/axonops/reports\n#report_storage_path: /my/reports/storage/directory\n\n# Alternatively store PDF reports in an object store by providing report_storage_config\n#report_storage_path: my-reports-s3-bucket/reports-folder\n#report_storage_config:\n#  type: s3\n#  provider: AWS\n#  access_key_id: MY_ACCESS_KEY_ID\n#  secret_access_key: MY_SECRET_ACCESS_KEY\n#  region: us-east-1\n#  acl: private\n#  server_side_encryption: AES256\n#  storage_class: STANDARD\n</code></pre> <p>For better performances on large clusters (100+ nodes), you can use a CQL store for the metrics such as Cassandra, Scylla or Elassandra. To opt-in for CQL metrics storage, just specify at least one CQL host with axon-server configuration.</p>"},{"location":"installation/axon-server/ubuntu/#step-4-start-the-server","title":"Step 4 - Start the server","text":"<pre><code>sudo systemctl daemon-reload\nsudo systemctl start axon-server\nsudo systemctl status axon-server\n</code></pre> <p>This will start the <code>axon-server</code> process as the <code>axonops</code> user, which was created during the package installation.  The default listening address is <code>0.0.0.0:8080</code>.</p>"},{"location":"installation/axon-server/ubuntu/#package-details","title":"Package details","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-server.yml</code></li> <li>Binary: <code>/usr/share/axonops/axon-server</code></li> <li>Logs: <code>/var/log/axonops/axon-server.log</code> </li> <li>Systemd service: <code>/usr/lib/systemd/system/axon-server.service</code></li> <li>Copyright : <code>/usr/share/doc/axonops/axon-server/copyright</code></li> <li>Licenses : <code>/usr/share/axonops/licenses/axon-server/</code></li> </ul>"},{"location":"installation/axon-server/ubuntu/#step-5-installing-axon-dash","title":"Step 5 - Installing axon-dash","text":"<p>Now axon-server is installed, you can start installing the GUI for it: axon-dash</p>"},{"location":"installation/cassandra-agent/docker/","title":"Installing axon-agent for Cassandra in Docker","text":"<p>Caveats</p> <ul> <li>Cassandra logs cannot normally be collected by AxonOps as they are sent to stdout and handled by the     Docker logging driver</li> <li>If axon-agent is running under Docker it assumes that the Cassandra user's GID is 999 as it is in the     official Cassandra images. If this is not the case then AxonOps may not be able to backup the Cassandra data.</li> </ul> <p>To enable the full functionality of the AxonOps agent some directories must be accessible to both the Cassandra and AxonOps Agent processes.</p> Directory Required Description <code>/var/lib/axonops</code> Required Contains UNIX domain sockets, Cassandra agent jars and local data stored by the agent. This directory must be readable and writable by Cassandra and AxonOps <code>/etc/axonops</code> Required Contains the configuration for AxonOps. This directory must be readable by Cassandra and AxonOps <code>/var/log/axonops</code> Required The Cassandra agent will write logs to this directory where they will be buffered and sent to the AxonOps server This directory must be writable by Cassandra and readable by AxonOps <code>/var/lib/cassandra</code> Optional For the backups feature to function correctly the Cassandra data directory must be readable by the AxonOps agent <p>When running Cassandra under Docker it is possible to run the AxonOps agent either on the host or in another  Docker container. When installing on the host follow the instructions under AxonOps Cassandra agent installation to install the agent and ensure that the appropriate directories are mapped into the Cassandra container.</p>"},{"location":"installation/cassandra-agent/docker/#example-with-docker-compose","title":"Example with Docker Compose","text":"<p>This example shows running a single Cassandra node and the AxonOps agent under Docker Compose using host volumes to share data between the containers.</p> <pre><code>version: \"3\"\n\nservices:\ncassandra:\nimage: cassandra:4.0\ncontainer_name: cassandra\nrestart: always\nvolumes:\n- ./cassandra:/var/lib/cassandra\n- ./axonops/var:/var/lib/axonops\n- ./axonops/etc:/etc/axonops\n- ./axonops/log:/var/log/axonops\nports:\n- \"9042:9042\"\nenvironment:\n- JVM_EXTRA_OPTS=-javaagent:/var/lib/axonops/axon-cassandra4.0-agent.jar=/etc/axonops/axon-agent.yml\n- CASSANDRA_CLUSTER_NAME=my-cluster\n\naxon-agent:\nimage: registry.axonops.com/axonops-public/axonops-docker/axon-agent:latest\nrestart: always\nenvironment:\n# Enter the hostname or IP address of your AxonOps server here\n- AXON_AGENT_SERVER_HOST=axonops-server.example.com\nvolumes:\n- ./cassandra:/var/lib/cassandra\n- ./axonops/var:/var/lib/axonops\n- ./axonops/etc:/etc/axonops\n- ./axonops/log:/var/log/axonops\n</code></pre>"},{"location":"installation/cassandra-agent/install/","title":"AxonOps Cassandra agent installation","text":"<p>This agent will enable metrics, logs and events collection with adaptive repairs and backups for Cassandra.</p> <p>See Installing axon-agent for Cassandra in Docker if you are running Cassandra under Docker.</p>"},{"location":"installation/cassandra-agent/install/#available-versions","title":"Available versions","text":"<ul> <li>Apache Cassandra 4.1.x</li> <li>Apache Cassandra 4.0.x</li> <li>Apache Cassandra 3.11.x</li> <li>Apache Cassandra 3.0.x</li> </ul>"},{"location":"installation/cassandra-agent/install/#step-1-installation","title":"Step 1 - Installation","text":"<p>Make sure that the <code>{version}</code> of your Cassandra and Cassandra agent are compatible from the compatibility matrix. </p>"},{"location":"installation/cassandra-agent/install/#centos-redhat","title":"CentOS / RedHat","text":"<pre><code>sudo tee /etc/yum.repos.d/axonops-yum.repo &lt;&lt; EOL\n[axonops-yum]\nname=axonops-yum\nbaseurl=https://packages.axonops.com/yum/\nenabled=1\nrepo_gpgcheck=0\ngpgcheck=0\nEOL\n\nsudo yum install axon-cassandra{version}-agent\n</code></pre>"},{"location":"installation/cassandra-agent/install/#debian-ubuntu","title":"Debian / Ubuntu","text":"<pre><code>apt-get install curl gnupg\ncurl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add -\necho \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list\nsudo apt-get update\n\nsudo apt-get install axon-cassandra{version}-agent\n</code></pre> <p>Note: This will install the AxonOps Cassandra agent and its dependency: axon-agent</p>"},{"location":"installation/cassandra-agent/install/#step-2-agent-configuration","title":"Step 2 - Agent Configuration","text":"<p>Update the following highlighted lines from <code>/etc/axonops/axon-agent.yml</code>:</p> <pre><code>axon-server:\nhosts: \"axon-server_endpoint\" # Your axon-server IP or hostname, e.g. axonops.mycompany.com\nport: 1888 # The default axon-server port is 1888\n\naxon-agent:\norg: \"my-company\" # Your organisation name\n\nNTP:\nhost: \"ntp.mycompany.com\" # Your NTP server IP address or hostname \n</code></pre>"},{"location":"installation/cassandra-agent/install/#step-3-configure-cassandra","title":"Step 3 - Configure Cassandra","text":"<p>Edit <code>cassandra-env.sh</code>, which is usually located in <code>/&lt;Cassandra Installation Directory&gt;/conf/cassandra-env.sh</code> for tarball installs or <code>/etc/cassandra/cassandra-env.sh</code> for package installs, and append the following line at the end of the file:</p> <pre><code>JVM_OPTS=\"$JVM_OPTS -javaagent:/usr/share/axonops/axon-cassandra{version}-agent.jar=/etc/axonops/axon-agent.yml\"\n</code></pre> <p>example with Cassandra agent version 3.11:</p> <pre><code>JVM_OPTS=\"$JVM_OPTS -javaagent:/usr/share/axonops/axon-cassandra3.11-agent.jar=/etc/axonops/axon-agent.yml\"\n</code></pre> <p>Make sure that this configuration will not get overridden by an automation tool.</p>"},{"location":"installation/cassandra-agent/install/#step-4-add-axonops-user-to-cassandra-user-group-and-cassandra-user-to-axonops-group","title":"Step 4 - Add axonops user to Cassandra user group and Cassandra user to axonops group","text":"<pre><code>sudo usermod -aG &lt;your_cassandra_group&gt; axonops\nsudo usermod -aG axonops &lt;your_cassandra_user&gt;\n</code></pre>"},{"location":"installation/cassandra-agent/install/#step-5-start-cassandra","title":"Step 5 - Start Cassandra","text":""},{"location":"installation/cassandra-agent/install/#step-6-start-axon-agent","title":"Step 6 - Start axon-agent","text":"<pre><code>sudo systemctl start axon-agent\n</code></pre>"},{"location":"installation/cassandra-agent/install/#optional-step-7-cassandra-remote-backups-or-restore-prerequisites","title":"(Optional) Step 7 - Cassandra Remote Backups or Restore Prerequisites","text":"<ul> <li> <p>If you plan to use AxonOps remote backup functionality, axonops user will require read access on Cassandra data folder.</p> </li> <li> <p>As well if you plan to Restore data with AxonOps,  axonops user will require write access to Cassandra data folder. We recommend to only provide temporary write access to axonops when required.</p> </li> </ul>"},{"location":"installation/cassandra-agent/install/#cassandra-agent-package-details","title":"Cassandra agent Package details","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-agent.yml</code></li> <li>Binary: <code>/usr/share/axonops/axon-cassandra{version}-agent.jar</code></li> <li>Version number: <code>/usr/share/axonops/axon-cassandra{version}-agent.version</code></li> <li>Copyright : <code>/usr/share/doc/axonops/axon-cassandra{version}-agent/copyright</code></li> <li>Licenses : <code>/usr/share/axonops/licenses/axon-cassandra{version}-agent/</code></li> </ul>"},{"location":"installation/cassandra-agent/install/#axon-agent-package-details-dependency-of-cassandra-agent","title":"axon-agent Package details (dependency of Cassandra agent)","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-agent.yml</code></li> <li>Binary: <code>usr/share/axonops/axon-agent</code></li> <li>Logs : <code>/var/log/axonops/axon-agent.log</code></li> <li>Systemd service: <code>/usr/lib/systemd/system/axon-agent.service</code></li> </ul>"},{"location":"installation/compat_matrix/compat_matrix/","title":"Compatibility matrix","text":""},{"location":"installation/compat_matrix/compat_matrix/#axonops-server","title":"AxonOps Server","text":"Operating Systems Target Architecture RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [10,11,12] x86_64"},{"location":"installation/compat_matrix/compat_matrix/#axonops-gui-server","title":"AxonOps GUI Server","text":"Operating Systems Target Architecture RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [10,11,12] x86_64"},{"location":"installation/compat_matrix/compat_matrix/#axonops-cassandra-agent","title":"AxonOps Cassandra Agent","text":"System AxonOps Cassandra Agent Version Operating Systems Target Architecture Cassandra 3.0.x 3.0 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 3.11.x 3.11 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 4.0.x 4.0 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 4.1.x 4.1 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64"},{"location":"installation/dse-agent/install/","title":"axon-java-agent for DSE installation","text":"<p>This agent will enable metrics collection from DSE and enable adaptive repairs and backups.</p>"},{"location":"installation/dse-agent/install/#prerequisites","title":"Prerequisites","text":"<p>DSE agent needs axon-agent to be installed and configured properly. If not installed already, please go to axon-agent installation  page.</p>"},{"location":"installation/dse-agent/install/#setup-axon-agent-for-dse","title":"Setup axon-agent for DSE","text":"<p>You'll need the specify/update the following lines from axon-agent.yml located in <code>/etc/axonops/axon-agent.yml</code>:</p> <pre><code>axon-server:\nhosts: \"axon-server_endpoint\" # Specify axon-server endpoint\naxon-agent:\nhost: 0.0.0.0 # axon-agent listening address for it's OpenTSDB endpoint\nport: 9916 # axon-agent listening port for it's OpenTSDB endpoint\norg: \"your_organisation_name\" # Specify your organisation name\nstandalone_mode: false\ntype: \"dse\"\n#cluster_name: \"standalone\" # comment that line\nssl: false # SSL flag for it's OpenTSDB endpoint\n</code></pre> <ul> <li>Set <code>standalone_mode</code> to false</li> <li>Set <code>type</code> to dse</li> <li>Don't forget to comment or remove the <code>cluster_name</code> as it will be deduced from DSE configuration.</li> <li>Don't forget to specify axon-server host and port if that's not already specified.</li> </ul>"},{"location":"installation/dse-agent/install/#dse-agent-installation","title":"DSE agent installation","text":"<p>Make sure the <code>{version}</code> of your DSE and DSE agent are compatible from the compatibility matrix. </p>"},{"location":"installation/dse-agent/install/#centos-redhat-installer","title":"CentOS / RedHat installer","text":"<pre><code>sudo yum install &lt;TODO&gt;\n</code></pre>"},{"location":"installation/dse-agent/install/#debian-ubuntu-installer","title":"Debian / Ubuntu installer","text":"<pre><code>sudo apt-get install &lt;TODO&gt;\n</code></pre>"},{"location":"installation/dse-agent/install/#package-details","title":"Package details","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-java-agent.yml</code></li> <li>Binary: <code>usr/share/axonops/axon-dse{version}-agent-1.0.jar</code></li> <li>Version number: <code>usr/share/axonops/axon-dse{version}-agent-1.0.version</code></li> </ul>"},{"location":"installation/dse-agent/install/#configure-dse","title":"Configure DSE","text":"<p>Edit <code>cassandra-env.sh</code> usually located in your dse install path such as <code>/&lt;path_to_DSE&gt;/resources/cassandra/conf/cassandra-env.sh</code> and add at the end of the file the following line:</p> <pre><code>JVM_OPTS=\"$JVM_OPTS -javaagent:/usr/share/axonops/axon-dse{version}-agent-1.0.jar=/etc/axonops/axon-java-agent.yml\"\n</code></pre> <p>example:</p> <pre><code>JVM_OPTS=\"$JVM_OPTS -javaagent:/usr/share/axonops/axon-dse6.0.4-agent-1.0.jar=/etc/axonops/axon-java-agent.yml\"\n</code></pre>"},{"location":"installation/dse-agent/install/#start-dse","title":"Start DSE","text":"<p>All you need to do now is start DSE.</p>"},{"location":"installation/dse-agent/install/#configuration-defaults","title":"Configuration defaults","text":"<pre><code>tier0: # metrics collected every 5 seconds\nmetrics:\njvm_:\n- \"java.lang:*\"\ncas_:\n- \"org.apache.cassandra.metrics:*\"\n- \"org.apache.cassandra.net:type=FailureDetector\"\n- \"com.datastax.bdp:type=dsefs,*\"\n\ntier1:\nfrequency: 300 # metrics collected every 300 seconds (5m)\nmetrics:\ncas_:\n- \"org.apache.cassandra.metrics:name=EstimatedPartitionCount,*\"\n\n#tier2:\n#    frequency: 3600 # 1h\n\n#tier3:\n#    frequency: 86400 # 1d\n\nblacklist: #\u00a0You can blacklist metrics based on MBean query pattern\n- \"org.apache.cassandra.metrics:type=ColumnFamily,*\" # dup of tables\n- \"org.apache.cassandra.metrics:name=SnapshotsSize,*\" # generally takes time\n\nfree_text_blacklist: #\u00a0You can blacklist metrics based on Regex pattern\n- \"org.apache.cassandra.metrics:type=ThreadPools,path=internal,scope=Repair#.*\"\n\nwarningThresholdMillis: 100 # This will warn in logs when a MBean takes longer than the specified value.\n\nwhitelisted_clients: # Whitelist for CQL connections\n- \"127.0.0.1\"\n- \"^*.*.*.*\"\n</code></pre>"},{"location":"installation/kubernetes/","title":"Running AxonOps on Kubernetes","text":""},{"location":"installation/kubernetes/#introduction","title":"Introduction","text":"<p>The following shows how to install AxonOps for monitoring cassandra. AxonOps requires ElasticSearch and the documentation below shows how to install both. If you already have ElasticSearch running, you can omit the installation and just ensure the AxonOps config points to it.</p> <p>AxonOps installation uses Helm Charts. Helm v3.8.0 or later is required in order to access the OCI repository hosting the charts. The raw charts can be downloaded from the GitHub repository.</p>"},{"location":"installation/kubernetes/#preparing-the-configuration","title":"Preparing the configuration","text":""},{"location":"installation/kubernetes/#resources","title":"Resources","text":"Cassandra Nodes ElasticSearch CPU ElasticSearch Memory AxonOps Server CPU AxonOps Server Memory &lt;10 1000m 4Gi 750m 1Gi &lt;50 1000m 4Gi 2000m 6Gi 100 2000m 16Gi 4000m 12Gi 200 4000m 32Gi 8000m 24Gi"},{"location":"installation/kubernetes/#elasticsearch","title":"ElasticSearch","text":"<p>The example below is a configuration file for the official ElasticSearch helm repository. See inline comments:</p> <pre><code>---\nclusterName: \"axonops-elastic\"\n\nreplicas: 1\n\nesConfig:\nelasticsearch.yml: |\nthread_pool.write.queue_size: 2000\n\nroles:\nmaster: \"true\"\ningest: \"true\"\ndata: \"true\"\nremote_cluster_client: \"false\"\nml: \"false\"\n\n# Adjust the memory and cpu requirements to your deployment\n# \nesJavaOpts: \"-Xms2g -Xmx2g\"\n\nresources:\nrequests:\ncpu: \"750m\"\nmemory: \"2Gi\"\nlimits:\ncpu: \"1500m\"\nmemory: \"4Gi\"\n\nvolumeClaimTemplate:\naccessModes: [\"ReadWriteOnce\"]\nstorageClassName: \"\" # adjust to your storageClass if you don't want to use default\nresources:\nrequests:\nstorage: 50Gi\n\nrbac:\ncreate: true\n</code></pre>"},{"location":"installation/kubernetes/#axonops","title":"AxonOps","text":"<p>The default AxonOps installation does not expose the services outside of the cluster. We recommend that you use either a LoadBalancer service or an Ingress.</p> <p>Below you can find an example using <code>Ingress</code> to expose both the dashboard and the AxonOps server.</p> <pre><code>axon-dash:\nimage:\npullPolicy: IfNotPresent\nrepository: registry.axonops.com/axonops-public/axonops-docker/axon-dash\ntag: latest\ningress:\nenabled: true\nclassName: nginx\nannotations:\nexternal-dns.alpha.kubernetes.io/hostname: axonops.mycompany.com\nhosts:\n- host: axonops.mycompany.com\npath: \"/\"\ntls:\n- hosts:\n- axonops.mycompany.com\nsecretName: axon-dash-tls\nresources:\nlimits:\ncpu: 1000m\nmemory: 1536Mi\nrequests:\ncpu: 25m\nmemory: 256Mi\n\n# If you are using an existing ElasticSearch rather than installing it \n# as shown above then make sure you update the elasticHost URL below\naxon-server:\nelasticHost: http://axonops-elastic-master:9200\ndashboardUrl: https://axonops.mycompany.com\nconfig:\n# Set your organization name here. This must match the name used in your license key\norg_name: demo\n# Enter your AxonOps license key here\nlicense_key: \"...\"\nimage:\npullPolicy: IfNotPresent\nrepository: registry.axonops.com/axonops-public/axonops-docker/axon-server\ntag: latest\n# Enable the agent ingress to allow agents to connect from outside the Kubernetes cluster\nagentIngress:\nenabled: true\nclassName: nginx\nannotations:\nexternal-dns.alpha.kubernetes.io/hostname: axonops-server.mycompany.com\nhosts:\n- host: axonops-server.mycompany.com\npath: \"/\"\ntls:\n- hosts:\n- axonops-server.mycompany.com\nsecretName: axon-server-tls\n\nresources:\nlimits:\ncpu: 1\nmemory: 1Gi\nrequests:\ncpu: 100m\nmemory: 256Mi\n</code></pre> <p>An example values file showing all available options can be found in the GitHub repository here: values-full.yaml</p>"},{"location":"installation/kubernetes/#installing","title":"Installing","text":""},{"location":"installation/kubernetes/#elasticsearch_1","title":"ElasticSearch","text":"<p>Now you can install Elasticsearch referencing the configuration file created in the previous step:</p> <pre><code>helm repo add elastic https://helm.elastic.co\nhelm update\nhelm upgrade -n axonops --install \\\n--create-namespace \\\n-f \"elasticsearch.yaml\" \\\nelasticsearch elastic/elasticsearch\n</code></pre>"},{"location":"installation/kubernetes/#axonops_1","title":"AxonOps","text":"<p>Finally install the AxonOps helm chart:</p> <pre><code>helm upgrade -n axonops --install \\\n--create-namespace \\\n-f \"axonops.yaml\" \\\naxonops oci://helm.axonops.com/axonops-public/axonops-helm/axonops\n</code></pre>"},{"location":"installation/kubernetes/minikube/","title":"Cassandra with AxonOps on Kubernetes","text":""},{"location":"installation/kubernetes/minikube/#introduction","title":"Introduction","text":"<p>The following shows how to install AxonOps for monitoring cassandra. This process specifically requires the official cassandra helm repository.</p>"},{"location":"installation/kubernetes/minikube/#using-minikube","title":"Using minikube","text":"<p>The deployment should work fine on latest versions of minikube as long as you provide enough memory for it.</p> <pre><code>minikube start --memory 8192 --cpus=4\nminikube addons enable storage-provisioner\n</code></pre> <p>:warning: Make sure you use a recent version of minikube. Also check available drivers and select the most appropriate for your platform</p>"},{"location":"installation/kubernetes/minikube/#helmfile","title":"Helmfile","text":""},{"location":"installation/kubernetes/minikube/#overview","title":"Overview","text":"<p>As this deployment contains multiple applications we recommend you use an automation system such as Ansible or Helmfile to put together the config. The example below uses helmfile.</p>"},{"location":"installation/kubernetes/minikube/#install-requirements","title":"Install requirements","text":"<p>You would need to install the following components:</p> <ul> <li>helm: https://helm.sh/docs/intro/install/</li> <li>helmfile: https://github.com/roboll/helmfile/releases</li> </ul> <p>Alternatively you can consider using a dockerized version of them both such as https://hub.docker.com/r/chatwork/helmfile</p>"},{"location":"installation/kubernetes/minikube/#config-files","title":"Config files","text":"<p>The values below are set for running on a laptop with <code>minikube</code>, adjust accordingly for larger deployments.</p>"},{"location":"installation/kubernetes/minikube/#helmfileyaml","title":"helmfile.yaml","text":"<pre><code>---\nrepositories:\n- name: axonops\nurl: helm.axonops.com/axonops-public/axonops-helm/axonops\noci: true\n- name: bitnami\nurl: https://charts.bitnami.com/bitnami\n- name: ckotzbauer\nurl: https://ckotzbauer.github.io/helm-charts\nreleases:\n- name: axon-elastic\nnamespace: {{ env \"NAMESPACE\" | default \"axonops\" }}\nchart: \"bitnami/elasticsearch\"\nversion: '12.8.1'\nwait: true\nvalues:\n- fullnameOverride: axon-elastic\n- imageTag: \"7.8.0\"\n- data:\nreplicas: 1\npersistence:\nsize: 1Gi\nenabled: true\naccessModes: [ \"ReadWriteOnce\" ]\n- curator:\nenabled: true\n- coordinating:\nreplicas: 1\n- master:\nreplicas: 1\npersistence:\nsize: 1Gi\nenabled: true\naccessModes: [ \"ReadWriteOnce\" ]\n\n- name: axonops\nnamespace: {{ env \"NAMESPACE\" | default \"axonops\" }}\nchart: \"digitalis/axonops\"\nwait: true\nvalues:\n- values.yaml\n\n- name: cassandra\nnamespace: cassandra\nchart: \"digitalis/cassandra\"\nwait: true\nvalues:\n- values.yaml\n\n- name: cadvisor\nnamespace: kube-system\nchart: ckotzbauer/cadvisor\nversion: 1.2.0\nvalues:\n- container:\nadditionalArgs:\n- --housekeeping_interval=5s                       # kubernetes default args\n- --max_housekeeping_interval=10s\n- --event_storage_event_limit=default=0\n- --event_storage_age_limit=default=0\n- --disable_metrics=percpu,process,sched,tcp,udp    # enable only diskIO, cpu, memory, network, disk\n- --docker_only\n- image:\nrepository: gcr.io/cadvisor/cadvisor\ntag: v0.37.0\n</code></pre>"},{"location":"installation/kubernetes/minikube/#valuesyaml","title":"values.yaml","text":"<pre><code>---\npersistence:\nenabled: true\nsize: 2Gi\naccessMode: ReadWriteMany\n\npodSettings:\nterminationGracePeriodSeconds: 300\n\nimage:\ntag: 3.11.6\npullPolicy: IfNotPresent\n\nconfig:\ncluster_name: digitalis\ncluster_size: 2\ndc_name: dc1\nseed_size: 1\nnum_tokens: 256\nmax_heap_size: 512M\nheap_new_size: 512M\nendpoint_snitch: GossipingPropertyFileSnitch\n\nenv:\nJVM_OPTS: \"-javaagent:/var/lib/axonops/axon-cassandra3.11-agent.jar=/etc/axonops/axon-agent.yml\"\n\nserviceAccount:\ncreate: true\nrules:\n- apiGroups:\n- \"\"\nresources:\n- nodes\n- nodes/metrics\n- pods\nverbs:\n- get\n- list\n- watch\n- nonResourceURLs:\n- /metrics\nverbs:\n- get\n\nextraVolumes:\n- name: axonops-agent-config\nconfigMap:\nname: axonops-agent\n- name: axonops-shared\nemptyDir: {}\n- name: axonops-logs\nemptyDir: {}\n\nextraVolumeMounts:\n- name: axonops-shared\nmountPath: /var/lib/axonops\nreadOnly: false\n- name: axonops-agent-config\nmountPath: /etc/axonops\nreadOnly: true\n- name: axonops-logs\nmountPath: /var/log/axonops\n\nextraContainers:\n- name: axonops-agent\nimage: digitalisdocker/axon-agent:latest\nenv:\n- name: AXON_AGENT_VERBOSITY\nvalue: \"1\"\n- name: AXON_AGENT_ARGS\nvalue: \"-v 1\"\n- name: DATA_FILE_DIRECTORY\nvalue: \"/var/lib/cassandra\"\n- name: CASSANDRA_POD_NAME\nvalueFrom:\nfieldRef:\nfieldPath: metadata.name\n- name: CASSANDRA_POD_NAMESPACE\nvalueFrom:\nfieldRef:\nfieldPath: metadata.namespace\n- name: CASSANDRA_NODE_NAME\nvalueFrom:\nfieldRef:\nfieldPath: spec.nodeName\n- name: CASSANDRA_POD_IP\nvalueFrom:\nfieldRef:\napiVersion: v1\nfieldPath: status.podIP\nvolumeMounts:\n- name: axonops-agent-config\nmountPath: /etc/axonops\nreadOnly: true\n- name: axonops-shared\nmountPath: /var/lib/axonops\nreadOnly: false\n- name: axonops-logs\nmountPath: /var/log/axonops\n- name: data\nmountPath: /var/lib/cassandra\n\n\naxon-server:\nglobal:\ncustomer: minikube\nbaseDomain: axonops.com\n\nelasticHost: http://axon-elastic-elasticsearch-master.axonops:9200\ndashboardUrl: https://axonops.axonops.com\n\nimage:\nrepository: digitalisdocker/axon-server\ntag: latest\npullPolicy: IfNotPresent\nconfig:\nextraConfig:\ncql_hosts:\n- cassandra-0.cassandra.cassandra.svc.cluster.local\ncql_username: \"cassandra\"\ncql_password: \"cassandra\"\ncql_local_dc: dc1\ncql_proto_version: 4\ncql_max_searchqueriesparallelism: 100\ncql_batch_size: 100\ncql_page_size: 100\ncql_autocreate_tables: false\ncql_retrypolicy_numretries: 3\ncql_retrypolicy_min: 2s\ncql_retrypolicy_max: 10s\ncql_reconnectionpolicy_maxretries: 10\ncql_reconnectionpolicy_initialinterval: 1s\ncql_reconnectionpolicy_maxinterval: 10s\ncql_keyspace_replication: \"{ 'class': 'NetworkTopologyStrategy', 'dc1': 1 }\"\ncql_metrics_cache_max_size: 128  #MB\ncql_metrics_cache_max_items : 500000\n\naxon-dash:\nreplicaCount: 1\nconfig:\naxonServerUrl: http://axonops-axon-server:8080\nservice:\ntype: NodePort\ningress:\nenabled: true\nannotations:\nnginx.ingress.kubernetes.io/ssl-redirect: \"true\"\nhosts:\n- hosts: axonops.axonops.com\npaths:\n- /\nimage:\nrepository: digitalisdocker/axon-dash\ntag: latest\npullPolicy: IfNotPresent\nautoscaling:\nenabled: true\nresources:\nlimits:\ncpu: 500m\nmemory: 512Mi\nrequests:\ncpu: 50m\nmemory: 128Mi\n</code></pre>"},{"location":"installation/kubernetes/minikube/#axon-agentyml","title":"axon-agent.yml","text":"<pre><code>axon-server:\nhosts: \"axonops-axon-server.axonops\" # Specify axon-server IP axon-server.mycompany.\nport: 1888\n\naxon-agent:\norg: \"digitalis\"\nhuman_readable_identifier: \"axon_agent_ip\" # one of the following:\n\nNTP:\nhost: \"pool.ntp.org\" # Specify a NTP to determine a NTP offset\n\ncassandra:\ntier0: # metrics collected every 5 seconds\nmetrics:\njvm_:\n- \"java.lang:*\"\ncas_:\n- \"org.apache.cassandra.metrics:*\"\n- \"org.apache.cassandra.net:type=FailureDetector\"\n\ntier1:\nfrequency: 300 # metrics collected every 300 seconds (5m)\nmetrics:\ncas_:\n- \"org.apache.cassandra.metrics:name=EstimatedPartitionCount,*\"\n\nblacklist: # You can blacklist metrics based on Regex pattern. Hit the agent on http://agentIP:9916/metricslist to list JMX metrics it is collecting\n- \"org.apache.cassandra.metrics:type=ColumnFamily.*\" # duplication of table metrics\n- \"org.apache.cassandra.metrics:.*scope=Repair#.*\" # ignore each repair instance metrics\n- \"org.apache.cassandra.metrics:.*name=SnapshotsSize.*\" # Collecting SnapshotsSize metrics slows down collection\n- \"org.apache.cassandra.metrics:.*Max.*\"\n- \"org.apache.cassandra.metrics:.*Min.*\"\n- \".*999thPercentile|.*50thPercentile|.*FifteenMinuteRate|.*FiveMinuteRate|.*MeanRate|.*Mean|.*OneMinuteRate|.*StdDev\"\n\nJMXOperationsBlacklist:\n- \"getThreadInfo\"\n- \"getDatacenter\"\n- \"getRack\"\n\nDMLEventsWhitelist: # You can whitelist keyspaces / tables (list of \"keyspace\" and/or \"keyspace.table\") to log DML queries. Data is not analysed.\n# - \"system_distributed\"\n\nDMLEventsBlacklist: # You can blacklist keyspaces / tables from the DMLEventsWhitelist (list of \"keyspace\" and/or \"keyspace.table\") to log DML queries. Data is not analysed.\n# - system_distributed.parent_repair_history\n\nlogSuccessfulRepairs: false # set it to true if you want to log all the successful repair events.\n\nwarningThresholdMillis: 200 # This will warn in logs when a MBean takes longer than the specified value.\n\nlogFormat: \"%4$s %1$tY-%1$tm-%1$td %1$tH:%1$tM:%1$tS,%1$tL %5$s%6$s%n\"\n</code></pre>"},{"location":"installation/kubernetes/minikube/#start-up","title":"Start up","text":""},{"location":"installation/kubernetes/minikube/#create-axon-agent-configuration","title":"Create Axon Agent configuration","text":"<pre><code>kubectl create ns cassandra\nkubectl create configmap axonops-agent --from-file=axon-agent.yml -n cassandra\n</code></pre>"},{"location":"installation/kubernetes/minikube/#run-helmfile","title":"Run helmfile","text":""},{"location":"installation/kubernetes/minikube/#with-locally-installed-helm-and-helmfile","title":"With locally installed helm and helmfile","text":"<pre><code>cd your/config/directory\nhemlfile sync\n</code></pre>"},{"location":"installation/kubernetes/minikube/#with-docker-image","title":"With docker image","text":"<pre><code>docker run --rm \\\n-v ~/.kube:/root/.kube \\\n-v ${PWD}/.helm:/root/.helm \\\n-v ${PWD}/helmfile.yaml:/helmfile.yaml \\\n-v ${PWD}/values.yaml:/values.yaml \\\n--net=host chatwork/helmfile sync\n</code></pre>"},{"location":"installation/kubernetes/minikube/#access","title":"Access","text":""},{"location":"installation/kubernetes/minikube/#minikube","title":"Minikube","text":"<p>If you used <code>minikube</code>, identify the name of the service with <code>kubectl get svc -n monitoring</code> and launch it with </p> <pre><code>minikube service axonops-axon-dash -n monitoring\n</code></pre>"},{"location":"installation/kubernetes/minikube/#loadbalancer","title":"LoadBalancer","text":"<p>Find the DNS entry for it:</p> <pre><code>kubectl get svc -n monitoring -o wide\n</code></pre> <p>Open your browser and copy and paste the URL.</p>"},{"location":"installation/kubernetes/minikube/#troubleshooting","title":"Troubleshooting","text":"<p>Check the status of the pods:</p> <pre><code>kubectl get pod -n monitoring\nkubectl get pod -n cassandra\n</code></pre> <p>Any pod which is not on state <code>Running</code> check it out with</p> <pre><code>kubectl describe -n NAMESPACE pod POD-NAME\n</code></pre>"},{"location":"installation/kubernetes/minikube/#storage","title":"Storage","text":"<p>One common problem is regarding storage. If you have enabled persistent storage you may see an error about persistent volume claims (not found, unclaimed, etc.). If you're using <code>minikube</code> make sure you enable storage with </p> <pre><code>minikube addons enable storage-provisioner\n</code></pre>"},{"location":"installation/kubernetes/minikube/#memory","title":"Memory","text":"<p>The second most common problem is not enough memory (OOMKilled). You will see this often if your node does not have enough memory to run the containers or if the <code>heap</code> settings for Cassandra are not right. <code>kubectl describe</code> command will be showing <code>Error 127</code> when this occurs.</p> <p>In the <code>values.yaml</code> file adjust the heap options to match your hardware:</p> <pre><code>  max_heap_size: 512M\nheap_new_size: 512M\n</code></pre>"},{"location":"installation/kubernetes/minikube/#minikube_1","title":"Minikube","text":"<p>Review the way you have started up <code>minikube</code> and assign more memory if you can. Also check the available drivers and select the appropriate for your platform. On macOS where I tested <code>hyperkit</code> or <code>virtualbox</code> are the best ones.</p> <pre><code>minikube start --memory 10240 --cpus=4 --driver=hyperkit\n</code></pre>"},{"location":"installation/kubernetes/minikube/#putting-it-all-together","title":"Putting it all together","text":""},{"location":"integrations/email-integration/","title":"Setup SMTP notifications","text":"<p>On the Axonops application menu, select <code>Settings -&gt; Integrations</code> .</p> <p><code>Click</code> on the <code>SMTP</code> area.</p> <p>Infomy</p> <p></p> <p></p>"},{"location":"integrations/microsoft-teams-integration/","title":"Setup Microsoft Teams notifications","text":""},{"location":"integrations/microsoft-teams-integration/#create-microsoft-teams-webhooks","title":"Create Microsoft Teams Webhooks","text":"<ul> <li>On the Microsoft Teams interface, go to <code>Connectors</code> </li> </ul> <p> * Click on configure on the <code>Incoming Webhook</code> connector  </p> <p> * Provide a name and select <code>Create</code> </p> <p> * Copy the url provided to the clipboard  </p> <p> * On the Axonops application menu, select <code>Settings -&gt; Integrations</code> *  <code>Click</code> on the <code>Microsoft Teams</code> area.</p> <p></p> <ul> <li>Enter a <code>name</code> and copy the url in the <code>Webhook URL</code> field and select <code>Create</code> </li> </ul>"},{"location":"integrations/overview/","title":"Overview","text":"<p>AxonOps provide various integrations for the notifications.</p> <p>The functionality is accessible via Settings &gt; Integrations</p> <p>The current integrations are:</p> <ul> <li>SMTP</li> <li>Pagerduty</li> <li>Slack</li> <li>Microsoft Teams</li> <li>ServiceNow</li> <li>Generic webhooks</li> </ul> <p>Infomy</p> <p></p>"},{"location":"integrations/overview/#routing","title":"Routing","text":"<p>AxonOps provide a rich routing mechanism for the notifications.</p> <p>The current routing options are:</p> <ul> <li>Global - this will route all the notifications</li> <li>Metrics - notifications about the alerts on metrics</li> <li>Backups - notifications about the backups / restore</li> <li>Service Checks - notifications about the service checks / health checks</li> <li>Nodes - notifications raised from the nodes</li> <li>Commands - notifications from generic tasks</li> <li>Repairs - notifications from Cassandra repairs</li> <li>Rolling Restart - notification from the rolling restart feature</li> </ul> <p>Each severity (<code>info, warning, error</code>) can be routed independently </p> <p></p>"},{"location":"integrations/pagerduy-integration/","title":"Setup Pagerduty","text":""},{"location":"integrations/pagerduy-integration/#create-pagerduty-routing-key","title":"Create Pagerduty Routing Key","text":"<p>Using these steps. Please note down the pagerduty routing key</p>"},{"location":"integrations/pagerduy-integration/#insert-pagerduty-routing-key","title":"Insert Pagerduty Routing Key","text":"<p>On the Axonops application menu, select <code>Settings -&gt; Integrations</code> .</p> <p><code>Click</code> on the <code>Pagerduty</code> area.</p> <p>Infomy</p> <p></p> <p></p>"},{"location":"integrations/servicenow-integration/","title":"ServiceNow","text":"<p>Navigate to  Settings &gt; Integrations and click on ServiceNow</p> <p></p> <p>Once you have gathered your instance name, username and password from ServiceNow, you can validate the form: </p> <p>If you want to see the detailed description of a notification, you'll need to add the <code>description</code> field from ServiceNow incidents templates. </p> <p></p> <p></p>"},{"location":"integrations/slack-integration/","title":"Setup Slack","text":""},{"location":"integrations/slack-integration/#create-slack-incoming-webhooks","title":"Create Slack Incoming Webhooks","text":"<ul> <li>Go to Slack <code>Application</code></li> <li>On the side menu click  </li> <li>In search box type <code>Incoming Webhook</code>s</li> <li>From the App directory click <code>Install</code> on <code>Incoming WebHooks App</code>.</li> </ul> <p>Infomy</p> <p></p> <ul> <li><code>Click</code> Add Configuration</li> </ul> <p>Infomy</p> <p></p> <ul> <li> <p>In <code>Post to Channel</code> Box select an option from the <code>choose a channel</code> dropdown menu .</p> </li> <li> <p><code>Click</code> <code>Add Incoming WebHooks Integration</code></p> </li> </ul> <p>Infomy</p> <p></p> <ul> <li><code>Copy</code> and make a note of the <code>WebHook URL</code> that appears in the <code>Setup Instructions</code>.</li> </ul> <p>Infomy</p> <p></p>"},{"location":"integrations/slack-integration/#creating-the-slack-integration-on-axon-server","title":"Creating the Slack integration on axon-server","text":"<p>On the Axonops application menu, select <code>Settings -&gt; Integrations</code> .</p> <p><code>Click</code> on the <code>Slack</code> area.</p> <p>Infomy</p> <p></p> <p>Infomy</p> <p></p>"},{"location":"monitoring/overview/","title":"Monitoring Overview","text":"<p>When monitoring enterprise service there are 3 categories of how the service is performing that we generally capture and monitor. These are;</p> <ul> <li>Performance metrics</li> <li>Events (logs)</li> <li>Service availability</li> </ul>"},{"location":"monitoring/overview/#performance-metrics","title":"Performance Metrics","text":"<p>Performance metrics in Cassandra is highly extensive and there is a large number that can be captured to understand how Cassandra is performing. Another key metrics that also must be captured in order to effectively understand the performance of a database is the system resource utilisation.</p> <p>AxonOps agent captures both Cassandra and OS metrics and pushes them to the AxonOps server.</p>"},{"location":"monitoring/overview/#events","title":"Events","text":"<p>Cassandra event logs are, by default, written to log files. There are important information in the log files that allows SREs and DevOps engineers to identify issues when they occur. AxonOps agent captures the logs and pushes them to the AxonOps server. These logs are visible within AxonOps dashboard allowing quick access to them without having to log in to the individual servers.</p>"},{"location":"monitoring/overview/#service-availability","title":"Service Availability","text":"<p>Checking the momentary service availability and dashboards gives confidence that all services are running correctly as expected. Example service checks that allow engineers to gain confidence in the service availability are:</p> <ul> <li>System process</li> <li>Network open ports - e.g. CQL and storage ports</li> <li>Database availability - e.g. can execute CQL query</li> </ul>"},{"location":"monitoring/overview/#axonops-monitoring","title":"AxonOps Monitoring","text":"<p>AxonOps implements all three types of monitoring described above. AxonOps agent captures the information, sends them securely to AxonOps server, and the information is stored in the backend data store.</p> <p>AxonOps GUI provides comprehensive set of metrics dashboards combined with the event log view. It also provides separate service check status view showing the health of the cluster.</p> <p>This section describes how the AxonOps GUI organises the dashboards of all three types of monitoring.</p>"},{"location":"monitoring/logsandevents/logsandevents/","title":"Logs & Events","text":""},{"location":"monitoring/logsandevents/logsandevents/#logs-and-events","title":"Logs and Events","text":"<p>AxonOps provides a powerful logging feature that allows you to search and filter logs based on different parameters such as DC/Rack/Node, Log Level, Event Type, Source and Log Content.</p> <p></p> <p>The logs and events are visible within AxonOps dashboard and Logs &amp; Events tab allowing quick access to them without having to login to the individual servers.</p>"},{"location":"monitoring/logsandevents/logsandevents/#search-by-log-level","title":"Search by Log Level","text":"<p>Filter logs based on their log levels to focus on specific severity levels. The log level indicates the importance or severity of a message from the most critical (ERROR) to less severe (DEBUG).</p> <p></p>"},{"location":"monitoring/logsandevents/logsandevents/#setting-up-the-debug-level","title":"Setting up the Debug Level","text":"<p>To search logs by debug level you have to enable debug mode in cassandra by editing the logback.xml file:</p> <pre><code>&lt;appender name=\"SYSTEMLOG\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt;\n    &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt;\n        &lt;level&gt;DEBUG&lt;/level&gt;\n</code></pre>"},{"location":"monitoring/logsandevents/logsandevents/#search-by-logs-source-and-event-type","title":"Search by Logs Source and Event Type","text":"<p>You can filter logs based on the log source (cassandra, axon-server and axon-agent logs) and event type to narrow down search results.</p> <p> </p>"},{"location":"monitoring/logsandevents/logsandevents/#search-by-content","title":"Search by Content","text":"<p>For a free text search enter a keyword in the content input or use the <code>/&lt;expression&gt;/</code> syntax to search by regex expression.</p> <p>Here are some examples:</p> <ul> <li>Display logs that contain a specific word or phrase:</li> </ul> <p> </p> <ul> <li>Display logs that contain a match either what is before or after the |, in this case \"Validated\" or \"Compacted\":</li> </ul> <p></p> <ul> <li>Display logs that contain both patterns in a line, in this case \"Segment\" and \"deleted:</li> </ul> <p></p>"},{"location":"monitoring/metricsdashboards/cassandra/","title":"Cassandra","text":"<p>AxonOps dashboards provides a comprehensive set of charts with an embedded view for logs and events. </p> <p>You can correlate metrics with logs/events as you can zoom in the logs histogram or metrics charts to drill down both results. </p> <p>Alert rules can be defined graphically in each chart and Log collection is  defined in the bottom part of that page.</p> <p>Infomy</p> <p></p>"},{"location":"monitoring/servicechecks/configurations/","title":"Configurations","text":""},{"location":"monitoring/servicechecks/configurations/#adding-service-checks","title":"Adding Service Checks","text":"<p>On the Axonops application menu, click <code>Service Checks</code> and select <code>Setup</code> tab.</p> <p></p>"},{"location":"monitoring/servicechecks/configurations/#creating-services","title":"Creating Services","text":"<pre><code>&gt; Note that for a Cassandra node, you can use the variable `{{.listen_address}}` which will correspond to Cassandra listening address.\n\nExample:\n\n!!! infomy\n\n\n    [![servicecheckseditor](/img/servicecheckseditor.png)](/img/servicecheckseditor.png)\n\n\n####  Delete Services\n\nTo Delete a service `copy`/`paste` into the editor and `click` save  [![save](/img/disk.png)](/img/disk.png)\n\n``` jsonld\n{\n    \"shellchecks\": [],\n    \"httpchecks\": [],\n    \"tcpchecks\": []\n\n}\n</code></pre> <p>Example:</p> <p>Infomy</p> <p></p>"},{"location":"monitoring/servicechecks/notifications/","title":"Notifications","text":"<p>Service checks will notify with one of the three statuses:</p> <p>Service Statuses.</p> <p>  Success</p> <p>  Warning  </p> <p>  Error</p> <p>Depending on the status of the service an appropriate alert will be sent. The <code>alert</code> will be sent based on the <code>Default Routing</code> that has been setup via the integrations menu.</p> <p>Noticed: If the <code>Default Routing</code> has not been set up <code>no alerts</code> will be sent.</p> <p>Service Alerts will be sent using the following rules.</p>"},{"location":"monitoring/servicechecks/notifications/#info","title":"Info","text":"<p>Default routing rules will be used to send  success  alerts</p>"},{"location":"monitoring/servicechecks/notifications/#warning","title":"Warning","text":"<p>Default routing rules will be used to send   warning  alerts</p>"},{"location":"monitoring/servicechecks/notifications/#error","title":"Error","text":"<p>Default routing rules will be used to send  error  alerts</p>"},{"location":"monitoring/servicechecks/overview/","title":"Service Checks","text":""},{"location":"monitoring/servicechecks/overview/#overview","title":"Overview","text":"<p>Service Checks in AxonOps allows you to configure custom checks using three types of checks:</p> <ol> <li>Shell Scripts</li> <li>HTTP endpoint checks</li> <li>TCP endpoint checks</li> </ol> <p>The functionality is accessible via the Service Checks menu</p> <p>You can list the service checks by node: </p> <p>Or by services: </p> <p>You can click on a row within the node view to see all the <code>services</code> for that given Node. </p> <p>The following shows a successful check:</p> <p></p> <p>And a failing check:</p> <p></p>"},{"location":"monitoring/servicechecks/overview/#configure-service-checks","title":"Configure service checks","text":"<p>To set up the checks, go to Settings &gt; Service Checks and click one of the <code>+</code> buttons</p> <p></p> <p>Any changes made and saved are automatically pushed down to the agents. There is no need to deploy the check scripts to individual servers like you may do for instance with Nagios. The status will show once the check has been executed on the agent, so it might take some time depending on the interval you have specified within the Service Checks. Although the first execution of the checks will be spread across 30 seconds to prevent running all the checks at the same time.</p>"},{"location":"monitoring/servicechecks/overview/#service-checks-templating","title":"Service checks templating","text":"<p>You can provide templated checks with the following pattern: <code>{{.variable_name}}</code></p> <p></p> <p><code>{{.comp_listen_address}}</code> will be replace with Cassandra listen address.</p> <p>For instance, port <code>7000</code> in the previous example for check storage port could be replaced with {{.comp_storage_port}} on a Cassandra cluster:</p> <p>endpoint: <code>{{.comp_listen_address}}:{{.comp_storage_port}}</code> </p>"},{"location":"monitoring/servicechecks/overview/#cassandra-variables","title":"Cassandra variables","text":"<p>Here is the full list of variables that can be specified in any service check:</p> <pre><code>agent_version\ncomp_PROPERTY_PREFIX\ncomp_SENSITIVE_KEYS\ncomp_allocate_tokens_for_keyspace\ncomp_authenticator\ncomp_authorizer\ncomp_auto_bootstrap\ncomp_auto_snapshot\ncomp_back_pressure_enabled\ncomp_back_pressure_strategy\ncomp_batch_size_fail_threshold_in_kb\ncomp_batch_size_warn_threshold_in_kb\ncomp_batchlog_replay_throttle_in_kb\ncomp_broadcast_address\ncomp_broadcast_rpc_address\ncomp_buffer_pool_use_heap_if_exhausted\ncomp_cas_contention_timeout_in_ms\ncomp_cdc_enabled\ncomp_cdc_free_space_check_interval_ms\ncomp_cdc_raw_directory\ncomp_cdc_total_space_in_mb\ncomp_client_encryption_options\ncomp_cluster_name\ncomp_column_index_cache_size_in_kb\ncomp_column_index_size_in_kb\ncomp_commit_failure_policy\ncomp_commitlog_compression\ncomp_commitlog_directory\ncomp_commitlog_max_compression_buffers_in_pool\ncomp_commitlog_periodic_queue_size\ncomp_commitlog_segment_size_in_mb\ncomp_commitlog_sync\ncomp_commitlog_sync_batch_window_in_ms\ncomp_commitlog_sync_period_in_ms\ncomp_commitlog_total_space_in_mb\ncomp_compaction_large_partition_warning_threshold_mb\ncomp_compaction_throughput_mb_per_sec\ncomp_concurrent_compactors\ncomp_concurrent_counter_writes\ncomp_concurrent_materialized_view_writes\ncomp_concurrent_reads\ncomp_concurrent_replicates\ncomp_concurrent_writes\ncomp_counter_cache_keys_to_save\ncomp_counter_cache_save_period\ncomp_counter_cache_size_in_mb\ncomp_counter_write_request_timeout_in_ms\ncomp_credentials_cache_max_entries\ncomp_credentials_update_interval_in_ms\ncomp_credentials_validity_in_ms\ncomp_cross_node_timeout\ncomp_data_file_directories\ncomp_dc\ncomp_disk_access_mode\ncomp_disk_failure_policy\ncomp_disk_optimization_estimate_percentile\ncomp_disk_optimization_page_cross_chance\ncomp_disk_optimization_strategy\ncomp_dynamic_snitch\ncomp_dynamic_snitch_badness_threshold\ncomp_dynamic_snitch_reset_interval_in_ms\ncomp_dynamic_snitch_update_interval_in_ms\ncomp_enable_materialized_views\ncomp_enable_scripted_user_defined_functions\ncomp_enable_user_defined_functions\ncomp_enable_user_defined_functions_threads\ncomp_encryption_options\ncomp_endpoint_snitch\ncomp_file_cache_round_up\ncomp_file_cache_size_in_mb\ncomp_gc_log_threshold_in_ms\ncomp_gc_warn_threshold_in_ms\ncomp_hinted_handoff_disabled_datacenters\ncomp_hinted_handoff_enabled\ncomp_hinted_handoff_throttle_in_kb\ncomp_hints_compression\ncomp_hints_directory\ncomp_hints_flush_period_in_ms\ncomp_hostId\ncomp_incremental_backups\ncomp_index_interval\ncomp_index_summary_capacity_in_mb\ncomp_index_summary_resize_interval_in_minutes\ncomp_initial_token\ncomp_inter_dc_stream_throughput_outbound_megabits_per_sec\ncomp_inter_dc_tcp_nodelay\ncomp_internode_authenticator\ncomp_internode_compression\ncomp_internode_recv_buff_size_in_bytes\ncomp_internode_send_buff_size_in_bytes\ncomp_isClientMode\ncomp_jvm_VM name\ncomp_jvm_VM vendor\ncomp_jvm_VM version\ncomp_jvm_awt.toolkit\ncomp_jvm_boot classpath\ncomp_jvm_cassandra-foreground\ncomp_jvm_cassandra.config\ncomp_jvm_cassandra.jmx.local.port\ncomp_jvm_cassandra.native.epoll.enabled\ncomp_jvm_com.sun.management.jmxremote.ssl\ncomp_jvm_file.encoding\ncomp_jvm_file.encoding.pkg\ncomp_jvm_file.separator\ncomp_jvm_gc_G1 Old Generation_collection count\ncomp_jvm_gc_G1 Old Generation_collection time\ncomp_jvm_gc_G1 Old Generation_memory pool names\ncomp_jvm_gc_G1 Young Generation_collection count\ncomp_jvm_gc_G1 Young Generation_collection time\ncomp_jvm_gc_G1 Young Generation_memory pool names\ncomp_jvm_heap_heapFreeSize\ncomp_jvm_heap_heapMaxSize\ncomp_jvm_heap_heapSize\ncomp_jvm_input arguments\ncomp_jvm_io.netty.native.workdir\ncomp_jvm_java.awt.graphicsenv\ncomp_jvm_java.awt.printerjob\ncomp_jvm_java.class.path\ncomp_jvm_java.class.version\ncomp_jvm_java.endorsed.dirs\ncomp_jvm_java.ext.dirs\ncomp_jvm_java.home\ncomp_jvm_java.io.tmpdir\ncomp_jvm_java.library.path\ncomp_jvm_java.rmi.server.hostname\ncomp_jvm_java.rmi.server.randomIDs\ncomp_jvm_java.runtime.name\ncomp_jvm_java.runtime.version\ncomp_jvm_java.specification.name\ncomp_jvm_java.specification.vendor\ncomp_jvm_java.specification.version\ncomp_jvm_java.util.logging.SimpleFormatter.format\ncomp_jvm_java.vendor\ncomp_jvm_java.vendor.url\ncomp_jvm_java.vendor.url.bug\ncomp_jvm_java.version\ncomp_jvm_java.vm.info\ncomp_jvm_java.vm.name\ncomp_jvm_java.vm.specification.name\ncomp_jvm_java.vm.specification.vendor\ncomp_jvm_java.vm.specification.version\ncomp_jvm_java.vm.vendor\ncomp_jvm_java.vm.version\ncomp_jvm_jna.loaded\ncomp_jvm_jna.platform.library.path\ncomp_jvm_jnidispatch.path\ncomp_jvm_library classpath\ncomp_jvm_line.separator\ncomp_jvm_log4j.configuration\ncomp_jvm_management spec version\ncomp_jvm_name\ncomp_jvm_os.arch\ncomp_jvm_os.name\ncomp_jvm_os.version\ncomp_jvm_path.separator\ncomp_jvm_spec name\ncomp_jvm_spec vendor\ncomp_jvm_start time\ncomp_jvm_sun.arch.data.model\ncomp_jvm_sun.boot.class.path\ncomp_jvm_sun.boot.library.path\ncomp_jvm_sun.cpu.endian\ncomp_jvm_sun.cpu.isalist\ncomp_jvm_sun.io.unicode.encoding\ncomp_jvm_sun.java.command\ncomp_jvm_sun.java.launcher\ncomp_jvm_sun.jnu.encoding\ncomp_jvm_sun.management.compiler\ncomp_jvm_sun.nio.ch.bugLevel\ncomp_jvm_sun.os.patch.level\ncomp_jvm_up time\ncomp_jvm_user.country\ncomp_jvm_user.dir\ncomp_jvm_user.home\ncomp_jvm_user.language\ncomp_jvm_user.name\ncomp_jvm_user.timezone\ncomp_jvm_user.variant\ncomp_key_cache_keys_to_save\ncomp_key_cache_save_period\ncomp_key_cache_size_in_mb\ncomp_listen_address\ncomp_listen_interface\ncomp_listen_interface_prefer_ipv6\ncomp_listen_on_broadcast_address\ncomp_logger\ncomp_max_file_descriptors\ncomp_max_hint_window_in_ms\ncomp_max_hints_delivery_threads\ncomp_max_hints_file_size_in_mb\ncomp_max_mutation_size_in_kb\ncomp_max_streaming_retries\ncomp_max_value_size_in_mb\ncomp_memtable_allocation_type\ncomp_memtable_cleanup_threshold\ncomp_memtable_flush_writers\ncomp_memtable_heap_space_in_mb\ncomp_memtable_offheap_space_in_mb\ncomp_min_free_space_per_drive_in_mb\ncomp_mode\ncomp_native_transport_max_concurrent_connections\ncomp_native_transport_max_concurrent_connections_per_ip\ncomp_native_transport_max_frame_size_in_mb\ncomp_native_transport_max_threads\ncomp_native_transport_port\ncomp_native_transport_port_ssl\ncomp_num_tokens\ncomp_open_file_descriptors\ncomp_otc_backlog_expiration_interval_ms\ncomp_otc_backlog_expiration_interval_ms_default\ncomp_otc_coalescing_enough_coalesced_messages\ncomp_otc_coalescing_strategy\ncomp_otc_coalescing_window_us\ncomp_otc_coalescing_window_us_default\ncomp_ownership\ncomp_partitioner\ncomp_permissions_cache_max_entries\ncomp_permissions_update_interval_in_ms\ncomp_permissions_validity_in_ms\ncomp_phi_convict_threshold\ncomp_prepared_statements_cache_size_mb\ncomp_rack\ncomp_range_request_timeout_in_ms\ncomp_read_request_timeout_in_ms\ncomp_releaseVersion\ncomp_request_scheduler\ncomp_request_scheduler_id\ncomp_request_scheduler_options\ncomp_request_timeout_in_ms\ncomp_role_manager\ncomp_roles_cache_max_entries\ncomp_roles_update_interval_in_ms\ncomp_roles_validity_in_ms\ncomp_row_cache_class_name\ncomp_row_cache_keys_to_save\ncomp_row_cache_save_period\ncomp_row_cache_size_in_mb\ncomp_rpc_address\ncomp_rpc_interface\ncomp_rpc_interface_prefer_ipv6\ncomp_rpc_keepalive\ncomp_rpc_listen_backlog\ncomp_rpc_max_threads\ncomp_rpc_min_threads\ncomp_rpc_port\ncomp_rpc_recv_buff_size_in_bytes\ncomp_rpc_send_buff_size_in_bytes\ncomp_rpc_server_type\ncomp_saved_caches_directory\ncomp_schemaVersion\ncomp_seed_provider\ncomp_server_encryption_options\ncomp_slow_query_log_timeout_in_ms\ncomp_snapshot_before_compaction\ncomp_ssl_storage_port\ncomp_sstable_preemptive_open_interval_in_mb\ncomp_start_native_transport\ncomp_start_rpc\ncomp_storage_port\ncomp_stream_throughput_outbound_megabits_per_sec\ncomp_streaming_keep_alive_period_in_secs\ncomp_streaming_socket_timeout_in_ms\ncomp_thrift_framed_transport_size_in_mb\ncomp_thrift_max_message_length_in_mb\ncomp_thrift_prepared_statements_cache_size_mb\ncomp_tombstone_failure_threshold\ncomp_tombstone_warn_threshold\ncomp_tracetype_query_ttl\ncomp_tracetype_repair_ttl\ncomp_transparent_data_encryption_options\ncomp_trickle_fsync\ncomp_trickle_fsync_interval_in_kb\ncomp_truncate_request_timeout_in_ms\ncomp_unlogged_batch_across_partitions_warn_threshold\ncomp_user_defined_function_fail_timeout\ncomp_user_defined_function_warn_timeout\ncomp_user_function_timeout_policy\ncomp_windows_timer_interval\ncomp_write_request_timeout_in_ms\nhost_BootTime\nhost_Ctxt\nhost_HostID\nhost_Hostname\nhost_KernelArch\nhost_KernelVersion\nhost_OS\nhost_Platform\nhost_PlatformFamily\nhost_PlatformVersion\nhost_Procs\nhost_ProcsBlocked\nhost_ProcsRunning\nhost_ProcsTotal\nhost_Uptime\nhost_VirtualizationRole\nhost_VirtualizationSystem\nhost_cpu_CPU\nhost_cpu_CacheSize\nhost_cpu_CoreID\nhost_cpu_Cores\nhost_cpu_Family\nhost_cpu_Flags\nhost_cpu_Mhz\nhost_cpu_Microcode\nhost_cpu_Model\nhost_cpu_ModelName\nhost_cpu_PhysicalID\nhost_cpu_Stepping\nhost_cpu_VendorID\nhost_disk_/_Free\nhost_disk_/_Total\nhost_disk_/_Used\nhost_disk_/_fstype\nhost_swapmem_Free\nhost_swapmem_PgFault\nhost_swapmem_PgIn\nhost_swapmem_PgMajFault\nhost_swapmem_PgOut\nhost_swapmem_Sin\nhost_swapmem_Sout\nhost_swapmem_Total\nhost_swapmem_Used\nhost_swapmem_UsedPercent\nhost_virtualmem_Active\nhost_virtualmem_Available\nhost_virtualmem_Buffers\nhost_virtualmem_Cached\nhost_virtualmem_CommitLimit\nhost_virtualmem_CommittedAS\nhost_virtualmem_Dirty\nhost_virtualmem_Free\nhost_virtualmem_HighFree\nhost_virtualmem_HighTotal\nhost_virtualmem_HugePageSize\nhost_virtualmem_HugePagesFree\nhost_virtualmem_HugePagesTotal\nhost_virtualmem_Inactive\nhost_virtualmem_Laundry\nhost_virtualmem_LowFree\nhost_virtualmem_LowTotal\nhost_virtualmem_Mapped\nhost_virtualmem_PageTables\nhost_virtualmem_SReclaimable\nhost_virtualmem_SUnreclaim\nhost_virtualmem_Shared\nhost_virtualmem_Slab\nhost_virtualmem_SwapCached\nhost_virtualmem_SwapFree\nhost_virtualmem_SwapTotal\nhost_virtualmem_Total\nhost_virtualmem_Used\nhost_virtualmem_UsedPercent\nhost_virtualmem_VMallocChunk\nhost_virtualmem_VMallocTotal\nhost_virtualmem_VMallocUsed\nhost_virtualmem_Wired\nhost_virtualmem_Writeback\nhost_virtualmem_WritebackTmp\nhuman_readable_identifier\nhuman_readable_identifier_field\n</code></pre>"},{"location":"operations/cassandra/repair/","title":"Repair","text":"<p>Repairs must be completed regularly to maintain Cassandra nodes.</p> <p>AxonOps provide two mechanisms to ease Cassandra repairs:</p> <ul> <li> <p>Scheduled repair</p> </li> <li> <p>Adaptive repair service</p> </li> </ul>"},{"location":"operations/cassandra/repair/#scheduled-repair","title":"Scheduled repair","text":"<p>You can initiate three types of scheduled repair:</p> <ul> <li>Immediate scheduled repair: these will trigger immediately once</li> </ul> <p>Infomy</p> <p></p> <ul> <li>Simple scheduled repair: these will trigger base on the selected schedule repeatedly</li> </ul> <p>Infomy</p> <p></p> <ul> <li>Cron schedule repair: Same as simple scheduled repair but the schedule will be based on a Cron expression</li> </ul> <p>Infomy</p> <p></p> <p>The following capture presents a running repair that has been initiated immediately and a scheduled repair that is scheduled for 12:00 AM UTC:</p> <p>Infomy</p> <p></p>"},{"location":"operations/cassandra/repair/#adaptive-repair-service","title":"Adaptive repair service","text":"<p>Since AxonOps collects performance metrics and logs, we built an \u201cAdaptive\u201d repair system which regulates the velocity (parallelism and pauses between each subrange repair) based on performance trending data. The regulation of repair velocity takes input from various metrics including CPU utilisation, query latencies, Cassandra thread pools pending statistics, and IOwait percentage, while tracking the schedule of repair based on gc_grace_seconds for each table.</p> <p>The idea of this is to achieve the following:</p> <ul> <li>Completion of repair within gc_grace_seconds of each table.</li> <li>Repair process does not affect query performance.</li> <li>In essence, adaptive repair regulator slows down the repair velocity when it deems the load is going to be high based on the gradient of the rate of increase of load, and speeds up to catch up with the repair schedule when the resources are more readily available.</li> <li>This mechanism also doesn't require JMX access. The adaptive repair service running on AxonOps server orchestrates and issues commands to the agents over the existing connection.</li> </ul> <p>Infomy</p> <p></p> <p>If you want to keep the tables as fresh as possible we recommend to increase the <code>table parallelism</code> to be greater than the total number of tables of your cluster and reduce the <code>segments per VNode</code> to generate less repair requests.</p> <p>From a user\u2019s point of view there is only a single switch to enable this service. Keep this enabled and AxonOps will take care of the repair of all tables for you. You can also customize the following:</p> <ul> <li> <p>Blacklist some tables</p> </li> <li> <p>Specify the number of tables to repair in parallel</p> </li> <li> <p>Specify the number of segments per VNode to repair</p> </li> <li> <p>The GC grace threshold in seconds: if a table has a gc grace lesser than the specified value, it will be ignored from the adaptive repair service</p> </li> </ul>"},{"location":"operations/cassandra/backup/overview/","title":"Backups Overview","text":"<p>AxonOps provides scheduled backup and restore functionality for your Cassandra data.</p> <p>The functionality is accessible via Operations &gt; Backups</p> <p>Infomy</p> <p></p>"},{"location":"operations/cassandra/backup/overview/#scheduled-backup","title":"Scheduled backup","text":"<p>You can initiate three types of scheduled backup:</p> <ul> <li> <p>Immediate scheduled backup: these will trigger immediately once</p> </li> <li> <p>Simple scheduled backup: these will trigger based on the selected schedule repeatedly</p> </li> </ul> <p>Infomy</p> <p></p> <ul> <li>Cron schedule backup: Same as simple scheduled backup but the schedule will be based on a Cron expression</li> </ul> <p>Infomy</p> <p></p> <p>The following capture presents two backups, a local only and a local and remote backup:</p> <p>Infomy</p> <p></p> <p>And the details of the local and remote backup:</p> <p>Infomy</p> <p></p>"},{"location":"operations/cassandra/backup/overview/#remote-backups","title":"Remote backups","text":"<p>Note that axonops user will need read access on Cassandra data folders to be able to perform a remote backup.</p> <p>The available remote options are:</p> <ul> <li>AWS S3</li> <li>Google Cloud Storage</li> <li>Microsoft Azure Blob Storage</li> <li>SFTP/SSH</li> <li>local filesystem</li> </ul> <p>example of the AWS S3 remote interface:</p> <p>Infomy</p> <p></p>"},{"location":"operations/cassandra/restore/overview/","title":"Overview","text":"<p>AxonOps provides the ability to restore from local snapshots and remote backups.</p> <p>The Restore feature is accessible via Operations &gt; Restore</p> <p>Infomy</p> <p></p> <p>Note that axonops user will need temporary write access on Cassandra data folders to be able to perform the restoration.</p> <p>To restore Cassandra, click on the backup you wish to restore.</p> <p>This will provide the details of that backup and the ability to start the restoration by clicking the <code>LOCAL RESTORE</code> or <code>REMOTE RESTORE</code>  button depending on if you prefer to restore from the local snapshot or the remote backup (if remote backups were configured). Here you can also select a subset of nodes to restore via the checkboxes in the Nodes list.</p> <p>Infomy</p> <p></p> <p>Follow the links below for some more detailed backup restore scenarios</p> <p>Restore a single node - same IP address</p> <p>Replace a node - different IP address</p> <p>Restore whole cluster - same IP addresses</p>"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/","title":"Restore a whole cluster from a remote backup with different IP addresses","text":"<p>Follow this procedure when you have lost all nodes in a cluster and they have been recreated in the same topology (Cluster, DC and rack names are all the same and the same number of nodes in each) but the replacement nodes have different IP addresses from the original cluster.</p> <p>NOTE: This process is for disaster recovery and cannot be used to clone a backup to a different cluster</p>"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/#prepare-the-cluster-for-restoring-a-backup","title":"Prepare the cluster for restoring a backup","text":"<p>Before you start, ensure that Cassandra is stopped on all replacement nodes and that their data directories are empty</p> <pre><code>sudo systemctl stop cassandra\nsudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/*\n</code></pre> <p>Allow the AxonOps user to write to the Cassandra data directory on all nodes</p> <pre><code>sudo chmod -R g+w /var/lib/cassandra/data\n</code></pre> <p>The commands above assume you are storing the Cassandra data in the default location <code>/var/lib/cassandra</code>, you will need to change the paths shown if your data is stored at a different location</p> <p>As the IP addresses of the replacement Cassandra nodes are different to the old cluster you will need to update the seeds list in <code>cassandra.yaml</code> to point to the IPs of the nodes that are replacing the old seeds. For package-based  installations (RPM or DEB) you can find this in <code>/etc/cassandra/cassandra.yaml</code> or for tarball installations it should be in <code>&lt;install_path&gt;/conf/cassandra.yaml</code>.</p>"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/#manually-configure-the-axonops-agent-host-ids","title":"Manually configure the AxonOps Agent host IDs","text":"<p>AxonOps identifies nodes by a unique host ID which is assigned when the agent starts up. In order to restore a backup to a node with a different IP address you must manually assign the AxonOps host ID of the old node to its new replacement.</p> <p>In order to restore the whole cluster from a backup you will need to apply the old AxonOps host ID to all nodes in the replacement cluster.</p> <p>The host ID of the old node can be found on the Cluster Overview page of the AxonOps dashboard</p> <p>Infomy</p> <p></p> <p>If you still have access to the old server or its data then its host ID can also be found in the file <code>/var/lib/axonops/hostId</code></p>"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/#apply-the-old-nodes-host-id-to-its-replacement","title":"Apply the old node's host ID to its replacement","text":"<p>Ensure the AxonOps Agent is stopped and clear any data that it may have created on startup</p> <pre><code>sudo systemctl stop axon-agent\nsudo rm -rf /var/lib/axonops/*\n</code></pre> <p>Manually apply the old node's host ID on the replacement (replace the host ID shown with your host ID from the previous steps)</p> <pre><code>echo '24d0cbf9-3b5a-11ed-8433-16b3c6a7bcc5' | sudo tee /var/lib/axonops/hostId\nsudo chown axonops.axonops /var/lib/axonops/hostId\n</code></pre> <p>Start the AxonOps agent</p> <pre><code>sudo systemctl start axon-agent\n</code></pre> <p>In the AxonOps dashboard you should see the replacement nodes start up and take over from the old nodes after a few minutes.</p>"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/#restore-the-backup","title":"Restore the backup","text":"<p>Open the Restore page in the AxonOps Dashboard by going to Operations &gt; Restore</p> <p>Infomy</p> <p></p> <p>Choose the backup you wish to restore from the list and click the <code>RESTORE</code> button</p> <p>This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list.</p> <p>Infomy</p> <p></p> <p>Select all nodes in the checkbox list then start the restore by clicking the <code>REMOTE RESTORE</code> button.</p> <p>The restore progress will be displayed in the Backup Restorations in Progress list</p> <p>Infomy</p> <p></p> <p>After the restore operation has completed successfully, fix the ownership and permissions on the Cassandra data directories on all nodes in the cluster</p> <pre><code>sudo chown -R cassandra.cassandra /var/lib/cassandra/data\nsudo chmod -R g-w /var/lib/cassandra/data\n</code></pre> <p>Start cassandra on the restored nodes one at a time, starting with the seeds first</p> <pre><code>sudo systemctl start cassandra\n</code></pre> <p>After the whole cluster is started up you should be able to see the replaced nodes with their new IP addresses in the output of <code>nodetool status</code>. You may still see the IP addresses of the old cluster nodes in the output of <code>nodetool gossipinfo</code>, these should clear out automatically after a few days or they can be manually tidied up by performing a rolling restart of the cluster.</p>"},{"location":"operations/cassandra/restore/restore-cluster-same-ip/","title":"Restore a whole cluster from a remote backup","text":"<p>Follow this procedure when you have lost all nodes in a cluster but they have been recreated in the same cluster  topology (Cluster, DC and rack names are all the same and the same number of nodes in each) and the replacement nodes have the same IP addresses as the original cluster.</p> <p>Ensure that Cassandra is stopped on all nodes and that its data directories are empty</p> <pre><code>sudo systemctl stop cassandra\nsudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/*\n</code></pre> <p>Allow the AxonOps user to write to the Cassandra data directory</p> <pre><code>sudo chmod -R g+w /var/lib/cassandra/data\n</code></pre> <p>These commands assume you are storing the Cassandra data in the default location <code>/var/lib/cassandra/</code>, you will need to change the paths shown if your data is stored at a different location</p> <p>Start the AxonOps agent on all nodes</p> <pre><code>sudo systemctl start axon-agent\n</code></pre> <p>Now open the Restore page in the AxonOps Dashboard by going to Operations &gt; Restore</p> <p>Infomy</p> <p></p> <p>Choose the backup you wish to restore from the list and click the <code>RESTORE</code> button</p> <p>This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list.</p> <p>Infomy</p> <p></p> <p>Select all nodes in the checkbox list then start the restore by clicking the <code>REMOTE RESTORE</code> button.</p> <p>The restore progress will be displayed in the Backup Restorations in Progress list</p> <p>Infomy</p> <p></p> <p>After the restore operation has completed successfully, fix the ownership and permissions on the Cassandra data  directories on all nodes in the cluster</p> <pre><code>sudo chown -R cassandra.cassandra /var/lib/cassandra/data\nsudo chmod -R g-w /var/lib/cassandra/data\n</code></pre> <p>Start cassandra on the restored nodes, starting with the seeds first</p> <pre><code>sudo systemctl start cassandra\n</code></pre>"},{"location":"operations/cassandra/restore/restore-different-cluster/","title":"Restore a backup to a different cluster","text":"<p>Follow this procedure to restore an AxonOps backup from remote storage onto a different cluster</p> <p>NOTE: This facility is only available for backups created using AxonOps Agent version 1.0.58 or later</p> <p>AxonOps Agent from version 1.0.58 onwards includes a command-line tool which can be used to restore a backup created by AxonOps from remote storage (e.g. S3, GCS). This tool connects directly to your remote storage and does not require an AxonOps server or an active AxonOps Cloud account in order to function.</p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#installing-the-cassandra-restore-tool","title":"Installing the Cassandra Restore Tool","text":"<p>The AxonOps Cassandra Restore tool is included in the AxonOps Agent package.</p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#installing-on-debian-ubuntu","title":"Installing on Debian / Ubuntu","text":"<pre><code>apt-get install curl gnupg\ncurl https://packages.axonops.com/apt/repo-signing-key.gpg | sudo apt-key add -\necho \"deb https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list\nsudo apt-get update\nsudo apt-get install axon-agent\n</code></pre>"},{"location":"operations/cassandra/restore/restore-different-cluster/#installing-on-centos-redhat","title":"Installing on CentOS / RedHat","text":"<pre><code>sudo tee /etc/yum.repos.d/axonops-yum.repo &lt;&lt; EOL\n[axonops-yum]\nname=axonops-yum\nbaseurl=https://packages.axonops.com/yum/\nenabled=1\nrepo_gpgcheck=0\ngpgcheck=0\nEOL\nsudo yum install axon-agent\n</code></pre> <p>After the package has been installed you can find the Cassandra Restore Tool at <code>/usr/share/axonops/axon-cassandra-restore</code>.</p> <p>Run the tool with <code>--help</code> to see the available options:</p> <pre><code>~# /usr/share/axonops/axon-cassandra-restore --help\nUsage of /usr/share/axonops/axon-cassandra-restore:\n  -i, --backup-id string                UUID of the backup to restore\n      --cassandra-bin-dir string        Where the Cassandra binary files are stored (e.g. /opt/cassandra/bin)\n--cqlsh-options string            Options to pass to cqlsh when restoring a table schema\n  -h, --help                            Show command-line help\n-l, --list                            List backups available in remote storage\n  -d, --local-sstable-dir string        A local directory in which to store sstables downloaded from backup storage\n      --org-id string                   ID of the AxonOps organisation from which the backup was created\n  -r, --restore                         Restore a backup from remote storage\n      --restore-schema                  Set this when using --use-sstable-loader to restore the CQL schema for each table. Keyspaces must already exist.\n  -c, --source-cluster string           The name of the cluster from which to restore\n  -s, --source-hosts string             Comma-separated list containing host IDs for which to restore backups\n      --sstable-loader-options string   Options to pass to sstableloader when restoring a backup\n      --storage-config string           JSON-formatted remote storage configuration\n  -t, --tables string                   Comma-separated list of keyspace.table to restore. Defaults to all tables if omitted.\n      --use-sstable-loader              Use sstableloader to restore the backup. Requires --sstable-loader-options and --cassandra-bin-dir.\n  -v, --verbose                         Show verbose output when listing backups\n      --version                         Show version information and exit\n</code></pre>"},{"location":"operations/cassandra/restore/restore-different-cluster/#listing-the-available-backups","title":"Listing the available backups","text":"<p>NOTE: The host IDs used in this tool are the ID given to each host by AxonOps and do not relate to the Cassandra host ID. You can find the AxonOps host ID by selecting the node on the Cluster Overview page of the AxonOps dashboard and looking at the Agent ID field.</p> <p>To list the backups available in the remote storage bucket you can run the tool with the <code>--list</code> option. For example to list the backups in an Amazon S3 bucket you could use a command similar to this:</p> <pre><code>/usr/share/axonops/axon-cassandra-restore --list \\\n--org-id myaxonopsorg \\\n--storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}'\n</code></pre> <p>The restore tool will then scan the specified S3 bucket for AxonOps backups and it will display the date and backup ID for any backups it finds:</p> <pre><code>Org ID:    myaxonopsorg\nCluster:   testcluster\nTime                   Backup ID\n2023-09-14 14:30 UTC   c67cea2a-5310-11ee-b686-bed50b9335ec\n2023-09-15 14:31 UTC   2c1d9aca-5312-11ee-b686-bed50b9335ec\n2023-09-16 14:30 UTC   91be5007-5313-11ee-b686-bed50b9335ec\n2023-09-17 14:31 UTC   f75f13d9-5314-11ee-b686-bed50b9335ec\n2023-09-18 14:30 UTC   5cffc1e6-5316-11ee-b686-bed50b9335ec\n</code></pre> <p>If you pass the <code>--verbose</code> option when listing backups it will show the list of nodes and tables in each backup, for example:</p> <pre><code>Org ID:    myaxonopsorg\nCluster:   testcluster\nTime:      2023-09-14 14:30 UTC\nBackup ID: c67cea2a-5310-11ee-b686-bed50b9335ec\n\nHost:   026346a0-dc89-4235-ae34-552fcd453b42\nTables: system.prepared_statements, system.transferred_ranges_v2, system_distributed.repair_history, system_schema.types, system_traces.sessions, system.compaction_history, system.available_ranges_v2, system.batches, system.size_estimates, system_schema.aggregates, system.IndexInfo, system_auth.resource_role_permissons_index, system_schema.views, test.test, system.paxos, system.local, system.peers, system.peers_v2, system.table_estimates, system_auth.network_permissions, system_auth.roles, system_distributed.view_build_status, system.built_views, system_schema.triggers, system.peer_events, system.repairs, keyspace1.table1, system.sstable_activity, keyspace1.table2, system.transferred_ranges, system_auth.role_permissions, system_distributed.parent_repair_history, system.available_ranges, system_schema.dropped_columns, system_schema.columns, system_schema.keyspaces, system.view_builds_in_progress, system_auth.role_members, system_schema.functions, system_schema.indexes, system_schema.tables, system_traces.events, system.peer_events_v2\n\nHost:   84759df0-8a19-497e-965f-200bdb4c1c9b\nTables: system_traces.events, system.available_ranges_v2, system.peer_events, system_auth.resource_role_permissons_index, system_auth.role_members, system_schema.types, system.IndexInfo, system.sstable_activity, system_distributed.view_build_status, system_schema.indexes, system.batches, system.transferred_ranges, system_schema.keyspaces, system_schema.tables, system_traces.sessions, system_distributed.repair_history, system_schema.aggregates, system.available_ranges, system.compaction_history, system.paxos, system.peers_v2, system.view_builds_in_progress, system.size_estimates, keyspace1.table1, system_auth.roles, system_schema.dropped_columns, test.test, system_auth.role_permissions, system_distributed.parent_repair_history, system.local, system.peer_events_v2, system.repairs, system.table_estimates, system_auth.network_permissions, system.peers, system_schema.triggers, system_schema.views, system.built_views, system.prepared_statements, system.transferred_ranges_v2, system_schema.columns, system_schema.functions, keyspace1.table2\n\nHost:   94ed3811-12ce-487f-ac49-ae31299efa31\nTables: system.peers_v2, system.view_builds_in_progress, system_auth.resource_role_permissons_index, system_auth.role_permissions, system_schema.aggregates, system_schema.indexes, test.test, system.available_ranges_v2, system_distributed.parent_repair_history, system_schema.keyspaces, system_traces.sessions, system_auth.role_members, system_auth.network_permissions, system_schema.dropped_columns, system_schema.types, system.repairs, system.size_estimates, system_auth.roles, system_schema.tables, system_schema.views, system.paxos, system.table_estimates, system.transferred_ranges, system.peers, system.prepared_statements, system.sstable_activity, system.peer_events_v2, system.batches, system.built_views, system.compaction_history, system_traces.events, system.IndexInfo, system.local, keyspace1.table2, system.peer_events, system.transferred_ranges_v2, system_distributed.repair_history, system_distributed.view_build_status, system_schema.columns, system_schema.functions, system.available_ranges, system_schema.triggers, keyspace1.table1\n</code></pre> <p>Scanning for backups can take a long time depending on the storage type and the amount of data, so you can use command-line options to restrict the search. For example this will restrict the search to a specific backup,  cluster, hosts and tables:</p> <pre><code>/usr/share/axonops/axon-cassandra-restore --list \\\n--verbose \\\n--org-id myaxonopsorg \\\n--storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\\n--backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\\n--source-cluster testcluster \\\n--source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b\n  --tables keyspace1.table1,keyspace1.table2\n</code></pre>"},{"location":"operations/cassandra/restore/restore-different-cluster/#restoring-a-backup","title":"Restoring a Backup","text":"<p>The <code>axon-cassandra-restore</code> tool can perform the following operations to restore a backup from remote storage: 1. Download the sstable files from the bucket 2. Create table schemas in the target cluster 3. Import the downloaded sstable files into the target cluster using <code>sstableloader</code></p> <p>The default behaviour is to only download the sstable files to a local directory.</p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#downloading-a-backup-to-a-local-directory","title":"Downloading a backup to a local directory","text":"<p>This command will download the backup with ID <code>2c1d9aca-5312-11ee-b686-bed50b9335ec</code> for the 3 hosts listed in the <code>--list</code> output above into the local directory <code>/opt/cassandra/axonops-restore</code></p> <pre><code>/usr/share/axonops/axon-cassandra-restore \\\n--restore \\\n--org-id myaxonopsorg \\\n--storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\\n--source-cluster testcluster \\\n--backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\\n--source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\\n--local-sstable-dir /opt/cassandra/axonops-restore\n</code></pre> <p>The sstable files will be restored into directories named <code>{local-sstable-dir}/{host-id}/keyspace/table/</code> and from here you can copy/move the files to another location or import them into a cluster using <code>sstableloader</code>.</p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#download-and-import-a-backup-in-a-single-operation","title":"Download and import a backup in a single operation","text":"<p>The above example shows how to download the backed up files into a local directory but it does not import them into a new cluster. You can make the <code>axon-cassandra-restore</code> tool do this for you after it downloads the files by passing the <code>--use-sstable-loader</code>, <code>--cassandra-bin-dir</code> and <code>--sstable-loader-options</code> command-line arguments.</p> <p>For example this command will download the same backup files as the previous example but it will also run <code>sstableloader</code> to import the downloaded files into a new cluster with contact points 10.0.0.1, 10.0.0.2 and 10.0.0.3:</p> <pre><code>/usr/share/axonops/axon-cassandra-restore \\\n--restore \\\n--org-id myaxonopsorg \\\n--storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\\n--source-cluster testcluster \\\n--backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\\n--source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\\n--local-sstable-dir /opt/cassandra/axonops-restore \\\n--use-sstable-loader \\\n--cassandra-bin-dir /opt/cassandra/bin \\\n--sstable-loader-options \"-d 10.0.0.1,10.0.0.2,10.0.0.3 -u cassandra -pw cassandra\"\n</code></pre>"},{"location":"operations/cassandra/restore/restore-different-cluster/#importing-cql-schemas-during-the-restore","title":"Importing CQL schemas during the restore","text":"<p>When a backup is imported to a cluster using <code>sstableloader</code> it assumes that the destination tables already exist and will skip the import for any that are missing. AxonOps backs up the current table schema with each backup so it is possible to create any missing tables as part of the restore operation. This can be enabled with the <code>--restore-schema</code> and <code>--cqlsh-options</code> arguments to <code>axon-cassandra-restore</code>.</p> <p>Building on the example above this command will download the files from the backup, create the schema for any missing tables, and import the downloaded data with <code>sstableloader</code>:</p> <pre><code>/usr/share/axonops/axon-cassandra-restore \\\n--restore \\\n--org-id myaxonopsorg \\\n--storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\\n--source-cluster testcluster \\\n--backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\\n--source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\\n--local-sstable-dir /opt/cassandra/axonops-restore \\\n--use-sstable-loader \\\n--cassandra-bin-dir /opt/cassandra/bin \\\n--sstable-loader-options \"-d 10.0.0.1,10.0.0.2,10.0.0.3 -u cassandra -pw cassandra\" \\\n--restore-schema \\\n--cqlsh-options `-u cassandra -p cassandra 10.0.0.1`\n</code></pre> <p>NOTE: This will not create missing keyspaces. You must ensure that the target keyspaces already exist in the destination cluster before running the restore command.</p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#storage-config-examples","title":"Storage Config Examples","text":"<p>The AxonOps Cassandra restore tool can restore backups from any remote storage supported by AxonOps for backups. The <code>--storage-config</code> command-line option configures the type of remote storage and the credentials required for access.</p> <p>Here are some examples of the most common storage types:</p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#local-filesystem","title":"Local filesystem","text":"<pre><code>--storage-config '{\"type\":\"local\",\"path\":\"/backups/cassandra\"}'\n</code></pre>"},{"location":"operations/cassandra/restore/restore-different-cluster/#amazon-s3","title":"Amazon S3","text":"<pre><code>--storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}'\n</code></pre>"},{"location":"operations/cassandra/restore/restore-different-cluster/#azure-blob-storage","title":"Azure Blob Storage","text":"<pre><code>--storage-config '{\"type\":\"azureblob\",\"account\":\"MY_AZURE_ACCOUNT_NAME\",\"key\":\"MY_AZURE_STORAGE_KEY\"}'\n</code></pre>"},{"location":"operations/cassandra/restore/restore-different-cluster/#google-cloud-storage","title":"Google Cloud Storage","text":"<pre><code>--storage-config '{\"type\":\"googlecloudstorage\",\"location\":\"us\",\"service_account_credentials\":\"ESCAPED_JSON_PRIVATE_KEY\"}'\n</code></pre>"},{"location":"operations/cassandra/restore/restore-node-different-ip/","title":"Replace a node from a remote backup","text":"<p>Follow this procedure to restore a single Cassandra node from a total loss of all data where the replacement node has a different IP address from the original.</p> <p>NOTE: Restoring a node from a total loss can only be performed from a remote backup</p> <p>AxonOps identifies nodes by a unique host ID which is assigned when the agent starts up. In order to restore a backup to a node with a different IP address you must manually assign the AxonOps host ID of the old node to its new replacement.</p> <p>You can find the host ID of the old node on the Cluster Overview page of the AxonOps dashboard</p> <p>Infomy</p> <p></p> <p>If you still have access to the old server or its data then its host ID can also be found in the file <code>/var/lib/axonops/hostId</code></p>"},{"location":"operations/cassandra/restore/restore-node-different-ip/#manually-configure-the-axonops-agent-host-id","title":"Manually configure the AxonOps Agent host ID","text":"<p>Ensure the AxonOps Agent is stopped and clear any data that it may have created on startup</p> <pre><code>sudo systemctl stop axon-agent\nsudo rm -rf /var/lib/axonops/*\n</code></pre> <p>Manually apply the old node's host ID on the replacement (replace the host ID shown with your host ID from the previous steps)</p> <pre><code>echo '24d0cbf9-3b5a-11ed-8433-16b3c6a7bcc5' | sudo tee /var/lib/axonops/hostId\nsudo chown axonops.axonops /var/lib/axonops/hostId\n</code></pre> <p>Start the AxonOps agent</p> <pre><code>sudo systemctl start axon-agent\n</code></pre>"},{"location":"operations/cassandra/restore/restore-node-different-ip/#restore-a-backup-to-the-replacement-node","title":"Restore a backup to the replacement node","text":"<p>Ensure Cassandra is stopped on the new node and that its data directories are all empty</p> <pre><code>sudo systemctl stop cassandra\nsudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/*\n</code></pre> <p>Allow the AxonOps user to write to the Cassandra data directory</p> <pre><code>sudo chmod -R g+w /var/lib/cassandra/data\n</code></pre> <p>These commands assume you are storing the Cassandra data in the default location <code>/var/lib/cassandra/</code>, you will need to change the paths shown if your data is stored at a different location</p> <p>Now open the Restore page in the AxonOps Dashboard by going to Operations &gt; Restore</p> <p>Infomy</p> <p></p> <p>Choose the backup you wish to restore from the list and click the <code>RESTORE</code> button</p> <p>This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list.</p> <p>Infomy</p> <p></p> <p>Ensure only the node you wish to restore is selected in the checkbox list and start the restore by clicking the <code>REMOTE RESTORE</code> button.</p> <p>The restore progress will be displayed in the Backup Restorations in Progress list</p> <p>Infomy</p> <p></p> <p>After the restore has completed successfully, fix the ownership and permissions on the Cassandra data directories</p> <pre><code>sudo chown -R cassandra.cassandra /var/lib/cassandra/data\nsudo chmod -R g-w /var/lib/cassandra/data\n</code></pre> <p>Now you can start cassandra on the restored node</p> <pre><code>sudo systemctl start cassandra\n</code></pre> <p>After the replacement node has started up the old IP address may still be visible in Gossip. It should clear out automatically after a day or two, or you can perform a rolling restart of the cluster to make sure everything is up-to-date.</p>"},{"location":"operations/cassandra/restore/restore-node-same-ip/","title":"Restore a single node from a remote backup","text":"<p>Follow this procedure to restore a single Cassandra node from a total loss of all data where the replacement node has the same IP address as the original.</p> <p>NOTE: Restoring a node from a total loss can only be performed from a remote backup</p> <p>Ensure Cassandra is stopped on the new node and that its data directories are all empty</p> <pre><code>sudo systemctl stop cassandra\nsudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/*\n</code></pre> <p>Allow the AxonOps user to write to the Cassandra data directory</p> <pre><code>sudo chmod -R g+w /var/lib/cassandra/data\n</code></pre> <p>These commands assume you are storing the Cassandra data in the default location <code>/var/lib/cassandra/</code>, you will need to change the paths shown if your data is stored at a different location</p> <p>Start axon-agent if it is not already running</p> <pre><code>sudo systemctl start axon-agent\n</code></pre> <p>Now open the Restore page in the AxonOps Dashboard by going to Operations &gt; Restore</p> <p>Infomy</p> <p></p> <p>Choose the backup you wish to restore from the list and click the <code>RESTORE</code> button</p> <p>This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list.</p> <p>Infomy</p> <p></p> <p>Ensure only the node you wish to restore is selected in the checkbox list and start the restore by clicking the <code>REMOTE RESTORE</code> button.</p> <p>The restore progress will be displayed in the Backup Restorations in Progress list</p> <p>Infomy</p> <p></p> <p>After the restore has completed successfully, fix the ownership and permissions on the Cassandra data directories</p> <pre><code>sudo chown -R cassandra.cassandra /var/lib/cassandra/data\nsudo chmod -R g-w /var/lib/cassandra/data\n</code></pre> <p>Start cassandra on the restored node</p> <pre><code>sudo systemctl start cassandra\n</code></pre>"},{"location":"operations/cassandra/rollingrestart/overview/","title":"Rolling Restart","text":"<p>AxonOps provides a rolling restart functionality for Cassandra.</p> <p>The feature is accessible via Operations &gt; Rolling Restart</p> <p>Infomy</p> <p></p> <p>axonops user will require permissions to be able to stop and start Cassandra service. To do so you will add axonops user in the sudoers with for instance the following permissions:</p> <pre><code>#/etc/sudoers.d/axonops\naxonops ALL=NOPASSWD: /sbin/service cassandra *, /usr/bin/systemctl * cassandra*\n</code></pre> <p>You can start an immediate rolling restart or schedule it.</p> <p>The script field let you able to tweak the predefined script executed by axon-agents during the restart process.</p> <p>You can also specify different degree of parallelism for the restart: DC, Rack and Node.</p> <p>For instance, to restart one entire rack at once across the cluster, you can set a large Node parallelism (greater than the number of nodes the rack has, ie 999).</p> <pre><code>DC parallelism: 1\nRack parallelism: 1\nNode parallelism: 999\n</code></pre> <p>To restart one entire rack across each DC:</p> <pre><code>DC parallelism: 999\nRack parallelism: 1\nNode parallelism: 999\n</code></pre>"},{"location":"overview/architecture/","title":"Architecture","text":""},{"location":"overview/architecture/#before","title":"Before","text":"<p>Our deployment model with the use of open source tools </p> <p>Infomy</p> <p></p>"},{"location":"overview/architecture/#axonops-deployment-model","title":"AxonOps Deployment Model","text":"<p>As you can see from the diagram below, we have massively simplified the stack with AxonOps.</p> <p>Infomy</p> <p></p> <p>You can also use a CQL datastore such as Cassandra, Elassandra, or Scylla to store the metrics. We recommend storing metrics on a CQL store on 100+ nodes clusters to improve your experience navigating the metrics dashboards.</p>"},{"location":"overview/motivation/","title":"Motivation","text":"<p>AxonOps has been developed and actively maintained by digitalis.io, a company providing managed services for Apache Cassandra\u2122 and other modern distributed data technologies.</p> <p>digitalis.io used a variety of modern and popular open source tools to manage its customer's data platforms to gain quick insight into how the clusters are working, and be alerted when there are issues.</p> <p>The open source tools we used are:</p> <ul> <li>Grafana - metrics dashboarding</li> <li>Prometheus - time series database for metrics</li> <li>Prometheus Alertmanager - metrics alerting</li> <li>ELK - Log capture and visualisation</li> <li>elastalert - Alerting on logs</li> <li>Consul - Service Discovery and Health Checks</li> <li>consul-alerts - Alerting on service health check failures</li> <li>Rundeck - Job Scheduler</li> <li>Ansible - Provisioning automation</li> </ul>"},{"location":"overview/motivation/#problems","title":"Problems","text":"<p>The tools listed above served us well. They gave us the confidence to manage enterprise deployment of distributed data platforms \u2013 alerting us when there are problems, ability to diagnose issues quickly, automatically performing routine scheduled tasks etc.</p> <p>However, using these tools and their problems were realised over time.</p> <ol> <li>Too many components - There are many components including the agents that need to be installed. Takes a lot of effort to integrate all components for each customer's on-premises environment, even with fully automated implementation using Ansible.</li> <li>Steep learning curve - The learning curve of deploying and configuring all the components is high.</li> <li>Patching hell - Patching schedule became a nightmare because of the sheer number of components. Imagine having to raise change requests for patching all above components!</li> <li>Enterprise hell - Firewall configurations became big for enterprise on-premises customers, often required many hours of tracing which change requests were unsuccessfully executed.</li> <li>Multiple dashboards - Multiple dashboards for metrics, logs and service availability.</li> <li>Complex alerting configurations - Alert notification configurations were all over the place. Fine-tuning alerts and updating them takes a lot of work.</li> </ol>"},{"location":"overview/motivation/#wish-list","title":"Wish List","text":"<p>With the above problems in mind, we needed to become more efficient as a company deploying the tools we need to manage our customers. After promoting the above tools to our customers, we ate the humble pie, and went back to the drawing board with the aim of reducing the efforts needed to on-board new customers.</p> <ul> <li>On-premises / cloud deployment</li> <li>Single dashboard for metrics / logs / service health</li> <li>Simple alert rules configurations</li> <li>Capture all metrics at high resolution (with Cassandra there are well over 20,000 metrics!)</li> <li>Capture logs and internal events like authentication, DDL, DML etc</li> <li>Scheduled backup / restore feature</li> <li>Performs domain specific administrative tasks, including Cassandra repair</li> <li>Manages the following products;<ul> <li>Apache Cassandra</li> <li>Apache Kafka</li> <li>DataStax Enterprise</li> <li>Confluent Enterprise</li> <li>Elasticsearch</li> <li>Apache Spark</li> <li>etc</li> </ul> </li> <li>Simplified deployment model</li> <li>Single agent for collecting metrics, logs, event, configs</li> <li>The same agent performs execution of health checks, backup, restore</li> <li>No JMX to capture the metrics, and must be push from the JVM and not pull</li> <li>Single socket connection initiated by agent to management server requiring only simple firewall rules</li> <li>Bidirectional communication between agent and management server over the single socket</li> <li>Modern snappy GUI</li> </ul>"}]}