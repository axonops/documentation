{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AxonOps","text":"<p>AxonOps is the only platform available for the one-stop operations of Apache Cassandra. Built by Cassandra experts, AxonOps provides access to all of the capability required to effectively monitor, maintain and backup an Apache Cassandra environment. Accessed through a single intuitive UI and driven by a highly efficient bi-directional protocol ensuring unprecedented functionality and exceptional performance.</p>"},{"location":"#features","title":"Features","text":""},{"location":"#monitoring","title":"Monitoring","text":"<ul> <li>Visualize metrics &amp; logs with dynamic dashboards</li> <li>Proactive Service Checks to never miss an issue</li> <li>Comprehensive alerting with enterprise-wide integration</li> </ul>"},{"location":"#maintenance","title":"Maintenance","text":"<ul> <li>Adaptive and Scheduled Cassandra Repairs to always be one step ahead</li> <li>Maintenance Job Scheduler to automate your key tasks</li> <li>Detailed cluster Configuration Views for the insights you need</li> </ul>"},{"location":"#backup","title":"Backup","text":"<ul> <li>Backup scheduling</li> <li>Visualise your backups and restores</li> <li>Restore backups with confidence</li> </ul>"},{"location":"#integrations","title":"Integrations","text":"<ul> <li>AxonOps provide various integrations for notifications and backups</li> </ul>       Notifications:             Backups:         Soon you\u2019ll be enjoying the only Cassandra management tool that combines:  <ul> <li>Dynamic responsive dashboards curated by experts</li> <li>Adaptive Repair process that will never let you down</li> <li>Efficient and intuitive backup you can rely on</li> <li>Easy log-file association and interrogation</li> <li>Reliable rolling restart through automation</li> <li>Sophisticated alerting integration and routing</li> <li>PDF reporting to quickly deliver insights across the team</li> <li>Highly efficient protocol ensuring exceptional performance</li> </ul> <p>All of this is underpinned by an efficient bi-directional protocol ensuring exception performance and scale. If you would like to schedule some time with an AxonOps Cassandra expert to walk you through the install and the AxonOps platform please follow the link below.</p>"},{"location":"#axonops-editions","title":"AxonOps Editions","text":"<p>We offer AxonOps Cloud(SaaS) and Self-hosted solutions.</p> <p>To read more about our editions: </p> <p>Free Edition</p> <p>Enterprise Edition</p>"},{"location":"#ready-to-work-with-axonops","title":"Ready to work with AxonOps?","text":"<p>Dive into the Get Started guide and setup your account with everything you need to monitor, maintain and backup Cassandra in a matter of minutes.</p> <p>If you would like to schedule some time with an AxonOps Cassandra expert to walk you through the install and the AxonOps platform please follow the link below.</p> <p>Book an Expert</p>"},{"location":"cluster/cluster-overview/","title":"Overview","text":"<p>Cluster Overview is the home page and provides an overview of your cluster health, including OS, Cassandra and JVM details.</p> <p>The information is automatically extracted by the AxonOps agent and pushed to AxonOps server. There is no need to configure anything on the agent or the server side for this information to be populated in the Cluster Overview dashboard.</p> <p>On the Axonops application menu, select <code>Cluster Overview</code>.</p> <p></p> <p></p> <p>You can select a node to view some details on the OS, Cassandra or the JVM.</p> <p>Infomy</p> <p></p>"},{"location":"cluster/cluster-overview/#os-details","title":"OS Details","text":"<p>Operating System Details section shows general information including:</p> <ul> <li>General Information</li> <li>CPU</li> <li>Memory</li> <li>Swap</li> <li>Disk volumes</li> </ul> <p>Infomy</p> <p></p>"},{"location":"cluster/cluster-overview/#cassandra-details","title":"Cassandra Details","text":"<p>Cassandra Details view shows the details from cassandra.yaml loaded into Cassandra. There is a search field available near the top to filter the parameters.</p> <p>Infomy</p> <p></p>"},{"location":"cluster/cluster-overview/#jvm-details","title":"JVM Details","text":"<p>JVM Details section shows the general information about the JVM, including the version and some configurations such as the heap and Garbage Collection settings.</p> <p>Infomy</p> <p></p>"},{"location":"configuration/agent-configuration/","title":"Configuring AxonOps Agent","text":"<p>Work in progress</p>"},{"location":"configuration/axondash/","title":"Configuring AxonOps Agent","text":"<p>Work in progress</p>"},{"location":"configuration/server-configuration/","title":"Configuring AxonOps Server","text":"<p>Work in progress</p>"},{"location":"editions/enterprise_edition/","title":"Enterprise Edition","text":"<p>Scale-out Cassandra with Confidence!</p> <p>Scale-out with one-stop operations, full data retention, extended functionality and optional fully managed Cassandra.</p> <p>Book a consultation with a Cassandra Expert.</p> <p>Book an Expert</p>"},{"location":"editions/enterprise_edition/#enterprise-edition-features","title":"Enterprise edition features","text":"<ul> <li>Cloud or Self-Hosted, you choose.</li> <li>Unlimited Cassandra nodes</li> <li>Unlimited Cloud User Accounts</li> <li>Maximum metrics &amp; logs retention</li> <li>Pre-configured Dashboards</li> <li>All metrics at 5 sec resolutions</li> <li>Hands-free Adaptive Repair</li> <li>Integrated Log &amp; Event Search</li> <li>Cluster Health Service Checks</li> <li>Automate Rolling Restarts</li> <li>Backups &amp; Restore for all storage needs</li> <li>Extensive Alerting Integrations</li> <li>Optional Cassandra Support</li> <li>Optional Consulting Services</li> <li>Optional Fully Managed Cassandra</li> </ul>"},{"location":"editions/free_editions/","title":"Free Editions","text":"<p>The free plan is avaialble for AxonOps SaaS. Developer which has a limited set of features and a Starter version that is free up to 6 nodes.</p> <p>Free Sign Up</p>"},{"location":"editions/free_editions/#developer-edition-features","title":"Developer edition features","text":"<p>Develop software with ease with a desktop docker-compose install of a 3-node Cassandra cluster and AxonOps.</p> <ul> <li>Installed on developer desktop</li> <li>One command deployment</li> <li>A 3-node Cassandra cluster</li> <li>Only 1.5GB memory required</li> <li>Visualize your application queries</li> <li>Check your query consistency level</li> <li>Measure data model effectiveness</li> <li>Troubleshoot and optimize</li> <li>Accelerate production readiness</li> <li>No servers required</li> </ul>"},{"location":"editions/free_editions/#starter-edition-features","title":"Starter edition features","text":"<p>One-stop operations for your production cluster with everything you need to monitor, maintain and backup Cassandra.</p> <ul> <li>Cloud or Self-Hosted, you choose.</li> <li>Up to 6 Cassandra nodes</li> <li>2 Cloud User Accounts</li> <li>14 day metrics &amp; logs retention</li> <li>Pre-configured Dashboards</li> <li>All metrics at 5 sec resolutions</li> <li>Hands-free Adaptive Repair</li> <li>Integrated Log &amp; Event Search</li> <li>Alerting Integration</li> <li>Cluster Health Service Checks</li> <li>Automate Rolling Restarts</li> <li>Backups &amp; Restore for all storage needs</li> <li>Optional Cassandra Support</li> <li>Optional Consulting Services</li> </ul>"},{"location":"editions/intro/","title":"Get ready for enjoying one-stop Cassandra operations from AxonOps.","text":"<p>Soon you\u2019ll be enjoying the only Cassandra management tool that combines:</p> <ul> <li>Dynamic responsive dashboards curated by experts</li> <li>Adaptive Repair process that will never let you down</li> <li>Efficient and intuitive backup you can rely on</li> <li>Easy log-file association and interrogation</li> <li>Reliable rolling restart through automation</li> <li>Sophisticated alerting integration and routing</li> <li>PDF reporting to quickly deliver insights across the team</li> <li>Highly efficient protocol ensuring exceptional performance</li> </ul> <p>All of this is underpinned by an efficient bi-directional protocol ensuring exceptional performance and scale. If you would like to schedule some time with an AxonOps Cassandra expert to walk you through the install and the AxonOps platform please follow the link below.</p> <p>If you would like to schedule some time with an AxonOps Cassandra expert to walk you through the install and the AxonOps platform please follow the link below.</p> <p>Book an Expert</p>"},{"location":"get_started/agent_setup/","title":"Agent setup","text":""},{"location":"get_started/agent_setup/#axon-agent-setup","title":"Axon Agent Setup","text":""},{"location":"get_started/agent_setup/#axonops-cloud-agent-network-requirements","title":"AxonOps Cloud Agent Network Requirements","text":"<p>AxonOps agent connects securely to the following AxonOps Cloud service endpoint;</p> <pre><code>https://agents.axonops.cloud\n</code></pre> <p>The TLS HTTPS connection initiated by the agent is upgraded to a WebSocket connection and thus requires WebSocket support in your corporate infrastructure, such as a secure web proxy service.</p> <p>If you have a DNS based security policy then you will be required to allow outbound access to the following domain.</p> <pre><code>agents.axonops.cloud\n</code></pre> <p>If you have an IP address based security policy you will be required to open access to the IP address ranges provided in the following links.</p> <pre><code>https://agents.axonops.cloud/ips-v4\nhttps://agents.axonops.cloud/ips-v6\n</code></pre> <p>In order to test your connectivity execute the following command:</p> <pre><code>curl https://agents.axonops.cloud/test.html\n</code></pre> <p>You should expect the following response:</p> <p>AxonOps Agent Test Page</p>"},{"location":"get_started/agent_setup/#setup-the-axonops-repository-on-your-operating-system","title":"Setup the AxonOps repository on your Operating system","text":""},{"location":"get_started/agent_setup/#select-the-os-family","title":"Select the OS Family.","text":"Debian / UbuntuRedHat / CentOS <pre><code>sudo apt-get update\nsudo apt-get install -y curl gnupg ca-certificates\ncurl -L https://packages.axonops.com/apt/repo-signing-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/axonops.gpg\necho \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list\nsudo apt-get update\nsudo apt-get install axon-cassandra{version}-agent\n</code></pre> <pre><code>sudo tee /etc/yum.repos.d/axonops-yum.repo &lt;&lt; EOL\n[axonops-yum]\nname=axonops-yum\nbaseurl=https://packages.axonops.com/yum/\nenabled=1\nrepo_gpgcheck=0\ngpgcheck=0\nEOL\n</code></pre>"},{"location":"get_started/agent_setup/#install-cassandra-agent","title":"Install Cassandra Agent","text":""},{"location":"get_started/agent_setup/#select-the-cassandra-version","title":"Select the Cassandra Version","text":"Debian / UbuntuRedHat / CentOS Cassandra 3.0Cassandra 3.11Cassandra 4.0Cassandra 4.1 <pre><code>sudo apt-get install axon-cassandra3.0-agent\n</code></pre> <pre><code>sudo apt-get install axon-cassandra3.11-agent\n</code></pre> <pre><code>sudo apt-get install axon-cassandra4.0-agent\n</code></pre> <pre><code>sudo apt-get install axon-cassandra4.1-agent\n</code></pre> Cassandra 3.0Cassandra 3.11Cassandra 4.0Cassandra 4.1 <pre><code>sudo yum install axon-cassandra3.0-agent\n</code></pre> <pre><code>sudo yum install axon-cassandra3.11-agent\n</code></pre> <pre><code>sudo yum install axon-cassandra4.0-agent\n</code></pre> <pre><code>sudo yum install axon-cassandra4.1-agent\n</code></pre>"},{"location":"get_started/agent_setup/#agent-configuration","title":"Agent Configuration","text":"<p>Update and copy the below code snippet into /etc/axonops/axon-agent.yml file.</p> <p>Please update the key and org values by logging into console.axonops.cloud</p> <ul> <li>Organization(org) name is next to the logo in the console</li> <li>Agent Keys(key) found in Agent Setup</li> </ul> <p></p> <p>If there is a Dedicated NTP server in your Organization please uncomment and update the NTP section. </p> <pre><code>  axon-server:\n      hosts: \"agents.axonops.cloud\"\n\n  axon-agent:\n      key: &lt;THIS_IS_A_DUMMY_KEY_PLEASE_UPDATE&gt;\n      org: &lt;THIS_IS_A_DUMMY_ORG_NAME_PLEASE_UPDATE&gt;\n\n  # Specify the NTP server IP addresses or hostnames configured for your Cassandra hosts\n  # if using Cassandra deployed in Kubernetes or if auto-detection fails.\n  # The port defaults to 123 if not specified.\n  # NTP:\n  #    hosts:\n  #        - \"x.x.x.x:123\"\n  # Optionally restrict which commands can be executed by axon-agent.\n  # If \"true\", only scripts placed in scripts_location can be executed by axon-agent.\n  # disable_command_exec: false\n  # If disable_command_exec is true then axon-agent is only allowed to execute scripts\n  # under this path\n  # scripts_location: /var/lib/axonops/scripts/\n</code></pre> <p>Set file permissions on /etc/axonops/axon-agent.yml file by executing the following command</p> <pre><code>sudo chmod 0644 /etc/axonops/axon-agent.yml\n</code></pre>"},{"location":"get_started/agent_setup/#cassandra-configuration","title":"Cassandra Configuration","text":"<p>Edit cassandra-env.sh, usually located in your Cassandra install path such as //conf/cassandra-env.sh, and append the following line at the end of the file: Cassandra 3.0Cassandra 3.11Cassandra 4.0Cassandra 4.1 <pre><code>JVM_OPTS=\"$JVM_OPTS -javaagent:/usr/share/axonops/axon-cassandra3.0-agent.jar=/etc/axonops/axon-agent.yml\"\n</code></pre> <pre><code>JVM_OPTS=\"$JVM_OPTS -javaagent:/usr/share/axonops/axon-cassandra3.11-agent.jar=/etc/axonops/axon-agent.yml\"\n</code></pre> <pre><code>JVM_OPTS=\"$JVM_OPTS -javaagent:/usr/share/axonops/axon-cassandra4.0-agent.jar=/etc/axonops/axon-agent.yml\"\n</code></pre> <pre><code>JVM_OPTS=\"$JVM_OPTS -javaagent:/usr/share/axonops/axon-cassandra4.1-agent.jar=/etc/axonops/axon-agent.yml\"\n</code></pre> <p>NB. Make sure that this configuration will not get overridden by an automation tool.</p>"},{"location":"get_started/agent_setup/#add-axonops-user-to-cassandra-user-group-and-cassandra-user-to-axonops-group","title":"Add AxonOps user to Cassandra user group and Cassandra user to AxonOps group","text":"<pre><code>sudo usermod -aG &lt;your_cassandra_group&gt; axonops\nsudo usermod -aG axonops &lt;your_cassandra_user&gt;\n</code></pre>"},{"location":"get_started/agent_setup/#startrestart-cassandra","title":"Start/Restart Cassandra","text":"<p>To load the Axon java agent and Cassandra config changes please,</p> <ul> <li>Start the Cassandra service if stopped. </li> <li>Restart the Cassandra service if already running.</li> </ul>"},{"location":"get_started/agent_setup/#start-axon-agent","title":"Start axon-agent","text":"<pre><code>sudo systemctl start axon-agent\n</code></pre> <p>Once the Agents have been setup please use the Using AxonOps to familiarise yourself with AxonOps UI.</p>"},{"location":"get_started/docker/","title":"Quickstart - Docker","text":""},{"location":"get_started/docker/#docker-compose-3-node-cassandra-desktop-install","title":"Docker-compose 3-Node Cassandra Desktop Install","text":"<p>Install and start testing on a 3-node Cassandra cluster in minutes. No registration required just head to GitHub at: axonops-cassandra-dev-cluster</p> <ul> <li>Includes fully functional AxonOps management environment to enhance the developer experience.</li> <li>Visualize your application queries</li> <li>Check your query consistency level</li> <li>Measure data model effectiveness</li> <li>Troubleshoot and optimize</li> </ul>"},{"location":"get_started/saas/","title":"Setup account","text":""},{"location":"get_started/saas/#setup-your-account","title":"Setup your account","text":"<p>Click on the Free Sign Up button to get started.</p> <p>Fill in your details on the registration screen</p> <p></p> <p>Once your account has been registered a confirmation email will be sent. </p> <p></p> <p>Click on the link in the link in the email to complete the setup process.  You will be taken to the sign up screen where you can enter your sign-up details or choose to use Google or Microsoft Azure to complete the process.</p> <p></p> <p>There will be another email sent to your email account to verify this is correct. </p> <p></p> <p>Once the email has been verified you are signed on to AxonOps Cloud where you can now setup your Organisation </p> <p>On left hand menu select 'Subscriptions'</p> <p>From 'Subscriptions' select 'Plans'</p> <p>From 'Plans' select 'Get Started Free'</p> <p></p> <p>Finally follow the Agent Setup to connect your cluster to AxonOps Cloud.</p>"},{"location":"how-to/backup-restore-notifications/","title":"Setup Backup - Restore Notifications","text":"<p>On the AxonOps application menu, click <code>Operations -&gt; Backups -&gt; Setup</code> and select <code>Notifications</code> tab. </p>"},{"location":"how-to/backup-restore-notifications/#notification-severities","title":"Notification Severities.","text":"<p>Notification Severities.</p> <p>For each notifications severity   Info     Warning     Error you can either use the slider  to use the default routing or use the   icon to customize the notification integrations.</p> <p>Notice:  not available when default routing   selected</p> <p>Infomy</p> <p></p>"},{"location":"how-to/backup-restore-notifications/#customize-notifications","title":"Customize Notifications.","text":"<p>To customize notifications <code>click</code> on  select the integrations that you require and click <code>Close</code>.</p> <p>Infomy</p> <p></p> <p>Noticed: The<code>Warning Integration</code> were customized. You can remove these by clicking the .</p> <p>If you want to remove default routing groups from a severity and create custom groups , use the slider bar to remove default routing <code>click</code> the  and follow this steps</p> <p>If you do not require any notifications <code>ensure</code> the <code>default routing</code> is off  and delete any previously created custom notification.</p> <p>Infomy</p> <p></p>"},{"location":"how-to/default-routing/","title":"Setup Default Routing","text":"<p>Default Routing.</p> <p>Allows you to set up the channels though which alerts &amp; notifications will be received and the specific groups that will receive the alerts &amp; notifications</p>"},{"location":"how-to/default-routing/#setup-default-routing_1","title":"Setup Default Routing","text":"<p>On the Axonops application menu, select <code>Alert &amp; Notifications</code> -&gt; Integration and select<code>Default Routing</code> tab.</p> <ul> <li>Alert &amp; Notification types can be set up</li> </ul> <p>Infomy</p> <p>  Info    Warning    Error </p>"},{"location":"how-to/default-routing/#info","title":"Info","text":"<p>To Setup Default Routing For   Info click On  </p> <ul> <li>Select the desired group(s) from the dropdown menu for the desired integrations(s) and  click`   to confirm selections</li> </ul> <p>Infomy</p> <p> </p> <p>The group should now appear in the   Info  Info box on the <code>Default Routing Tab</code></p> <p>Infomy</p> <p></p>"},{"location":"how-to/default-routing/#warning-error","title":"Warning - Error","text":"<p>Repeat these steps to setup the Default Routing for   Warning    Error </p>"},{"location":"how-to/default-routing/#edit-default-routing","title":"Edit Default Routing","text":"<p>To Edit <code>Default Routing</code> click on the    icon on either    Add or <code>Remove</code> existing integrations using the <code>dropdown</code> menus."},{"location":"how-to/default-routing/#delete-default-routing","title":"Delete Default Routing","text":"<p>To Remove a <code>group</code> <code>click</code> on the <code>Delete</code> icon</p> <p>Infomy</p> <p></p>"},{"location":"how-to/reuse-host-id/","title":"Re-using an existing Host ID","text":"<p>Each agent connected to the AxonOps server is assigned a unique host ID that is used internally to associate metrics and events with the node. If a Cassandra host dies and is replaced by another one with the same IP and token range then normally a new host ID will be generated and the replacement server will appear as a new machine in AxonOps. In this situation it is possible to re-use the same host ID so AxonOps sees the replacement server as the same as the original one.</p> <p>You can find the host ID of a node in the AxonOps GUI by going to Cluster Overview and selecting a node. In Graph View the host ID is shown next to the hostname in the details panel and in List View it is shown as Agent ID at the top of the details popup. If the old server's filesystem is still accessible you can also find the host ID stored in <code>/var/lib/axonops/hostId</code>.</p> <p>Follow these steps to start up a replacement server using the old host ID:</p> <ol> <li>Install axon-agent on the replacement server</li> <li>Stop axon-agent if it is running then delete these files if they exist: <code>/var/lib/axonops/hostId</code>, <code>/var/lib/axonops/local.db</code></li> <li>Create a new file at <code>/var/lib/axonops/hostId</code> containing the host ID you wish to use</li> <li>Start axon-agent</li> </ol>"},{"location":"how-to/setup-alert-rules/","title":"Setup alert rules","text":""},{"location":"how-to/setup-alert-rules/#insert-alert-rules-credentials","title":"Insert Alert Rules Credentials","text":"<p>On the Axonops application menu, click <code>Dashboards</code> and <code>select</code> required Dashboard. eg. <code>System</code></p> <p><code>Hover over</code> the desired Chart <code>click</code> on the   button.</p> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-alert-rules/#complete-the-fields-in-form","title":"Complete The Fields In Form","text":"<ul> <li>Below the chart <code>click</code> on the <code>alert</code> tab.</li> </ul> <p>Infomy</p> <p></p> <ul> <li>A form will appear</li> </ul> <p>Infomy</p> <p></p> <ul> <li>Complete Alert settings in <code>Comparator Warning value</code> or <code>Critical value</code> or Both and the <code>Duration</code> ==&gt; (how often to check) In</li> </ul> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-alert-rules/#annotations","title":"Annotations","text":"<p>In the <code>Summary</code> box you can include free text &amp; type one <code>or</code> many of the following <code>$labels</code></p> <pre><code>$labels:\n   - cluster\n   - dc\n   - hostname\n   - org\n   - rack\n   - type\n   - keyspace\n$value:\n</code></pre> <p>In the <code>Description</code> box you can include free along with one <code>or</code> many of the above  <code>$labels</code></p> <p>Example</p> <p><code>CPU usage per DC Alerts usage on {{ $labels.hostname }} and cluster {{$labels.cluster}}</code></p> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-alert-rules/#integrations","title":"Integrations","text":"<ul> <li> <p>Using the slider bar   you can select any Integrations.</p> <p>Then <code>click</code> on the <code>Info</code>, <code>Warning</code>, <code>Error</code> icons, to select the group(s) of Integrations for the alert.</p> </li> </ul> <p>Infomy</p> <p> </p> <p>Not selecting integrations</p> <p>If you do not select any specific Integrations the Alert will automatically follow the <code>Global Dashboard Routing</code> or if this has not been setup the Default Routing Integrations.</p>"},{"location":"how-to/setup-alert-rules/#edit-alert-rule","title":"Edit Alert Rule","text":"<p>On the Axonops application menu, click <code>Alerts &amp; Notifications</code> and <code>click</code> Active. <code>Select</code> the <code>Alert Rules</code> tab and click  </p> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-alert-rules/#delete-alert-rules","title":"Delete Alert Rule(s)","text":"<p>To Delete An Alert Either...</p> <ul> <li>On the Axonops application menu, click <code>Dashboards</code> and <code>select</code> required Dashboard. <code>eg. System</code> <code>Hover over</code> the desired Chart click on the   button. Below the chart <code>click</code> on the <code>alert</code> tab and <code>click</code> on the   of the alert rule you want to remove.</li> </ul> <p>OR...</p> <ul> <li>On the Axonops application menu, click <code>Alerts &amp; Notifications</code> and <code>click</code> Active. <code>Select</code> the Alert Rules tab and click   </li> </ul> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-dashboards-global-integrations/","title":"Setup Dashboards Global Integrations","text":"<p>On the Axonops application menu, click <code>Alerts &amp; Notifications -&gt; Active</code> and select <code>Dashboards Global Routing</code> tab.</p>"},{"location":"how-to/setup-dashboards-global-integrations/#notification-severities","title":"Notification Severities.","text":"<p>Notification Severities.</p> <p>For each notifications severity    Info      Warning      Error you can either use the slider   to use the default routing or use the    icon to customize the notification integrations.</p> <p>Notice:   not available when default routing  selected</p> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-dashboards-global-integrations/#customize-notifications","title":"Customize Notifications.","text":"<p>To customize notifications <code>click</code> on  select the integrations that you require and click <code>Close</code>.</p> <p>Infomy</p> <p></p> <p>Noticed: The<code>Warning Integration</code> were customized. You can remove these by clicking the .</p> <p>If you want to remove default routing groups from a severity and create custom groups , use the slider bar to remove default routing <code>click</code> the   and follow this steps</p> <p>If you do not require any notifications <code>ensure</code> the <code>default routing</code> is off  and delete any previously created custom notification.</p> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-log-collection/","title":"Setup Log Collection","text":"<p>AxonOps dashboards provides a comprehensive set of charts with an embedded view for logs and events. The goal is to correlate metrics with logs/events as you can zoom in the logs histogram or metrics charts to drill down both results. </p> <p>Log and event view is located in the bottom part of that page that you can expand/collapse with the horizontal splitter.</p> <p></p> <p>The setup for log collection is accessible via Settings &gt; logs</p> <p>Infomy</p> <p></p> <p>newlineregex is used by the log collector to handle multilines logs. Default newlineregex for Cassandra should be ok unless you've customized it.</p>"},{"location":"how-to/setup-servicechecks/","title":"Setup Service Checks","text":""},{"location":"how-to/setup-servicechecks/#add-service-checks","title":"Add Service Checks","text":"<p>On the Axonops application menu, click <code>Service Checks</code> and select <code>Setup</code> tab.</p> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-servicechecks/#create-services","title":"Create Services","text":"<p>Below there few examples <code>copy</code> and <code>Paste</code> inside. and click <code>save</code> </p> <pre><code>{\n    \"shellchecks\": [\n     {\n        \"name\" : \"check_elastic\",\n        \"shell\" :  \"/bin/bash\",\n        \"script\":  \"if [ 'ps auxwww | grep elastic | wc -l' -lt 1 ] then exit 2 else exit 0  fi\",\n        \"interval\": \"5m\" ,\n        \"timeout\": \"1m\" \n     }\n   ],\n\n   \"httpchecks\": [],\n   \"tcpchecks\": [\n    {\n        \"name\": \"elastic_tcp_endpoint_check\",\n        \"interval\": \"5s\",\n        \"timeout\": \"1m\",\n        \"tcp\": \"localhost:9200\"\n    }\n   ]\n\n}\n</code></pre> <p>Example:</p> <p>Infomy</p> <p></p>"},{"location":"how-to/setup-servicechecks/#delete-services","title":"Delete Services","text":"<p>To Delete a service <code>copy</code> and <code>Paste</code> inside. and <code>click</code> save  </p> <pre><code>{\n    \"shellchecks\": [],\n    \"httpchecks\": [],\n    \"tcpchecks\": []\n\n}\n</code></pre> <p>Example:</p> <p>Infomy</p> <p></p>"},{"location":"installation/axon-agent/install/","title":"axon-agent installation","text":"<p>There 2 elements to the AxonOps agent. The first is the axon-agent, which is a native application for Linux running as a standalone daemon process. The second is the Java agent which is added to the Java process. Two components communicate with each other using the Unix domain socket. The reason for this approach are the following requirements we have on the agent process.</p> <ul> <li>No JMX</li> <li>Metrics must push metrics from Cassandra all the way to the AxonOps server - never pull.</li> </ul> <p>AxonOps Java agent will push the metrics to the AxonOps native agent, which in turn pushes them to the AxonOps server. Scraping a large volume of metrics against the JMX is slow. We also wanted to avoid exposing an HTTP endpoint within Cassandra like the Prometheus JMX exporter does.</p> <p>The messaging between native agent and Java agent is bidirectional, i.e. AxonOps server sends control messages to Cassandra for operations such as repair and backups without the use of JMX.</p> <p>This section describes how to install and configure both the native agent and Java agent.</p>"},{"location":"installation/axon-agent/install/#centos-redhat","title":"CentOS / RedHat","text":"<pre><code>sudo tee /etc/yum.repos.d/axonops-yum.repo &lt;&lt; EOL\n[axonops-yum]\nname=axonops-yum\nbaseurl=https://packages.axonops.com/yum/\nenabled=1\nrepo_gpgcheck=0\ngpgcheck=0\nEOL\n\nsudo yum install axon-agent\n</code></pre>"},{"location":"installation/axon-agent/install/#debian-ubuntu","title":"Debian / Ubuntu","text":"<pre><code>sudo apt-get update\nsudo apt-get install -y curl gnupg ca-certificates\ncurl -L https://packages.axonops.com/apt/repo-signing-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/axonops.gpg\necho \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list\nsudo apt-get update\nsudo apt-get install axon-agent\n</code></pre>"},{"location":"installation/axon-agent/install/#package-details","title":"Package details","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-agent.yml</code></li> <li>Binary: <code>usr/share/axonops/axon-agent</code></li> <li>Logs : <code>/var/log/axonops/axon-agent.log</code></li> <li>Systemd service: <code>/usr/lib/systemd/system/axon-agent.service</code></li> <li>certificate file used for it's OpenTSDB endpoint when SSL is active: <code>/etc/axonops/agent.crt</code></li> <li>key file used for it's OpenTSDB endpoint when SSL is active: <code>/etc/axonops/agent.key</code></li> </ul>"},{"location":"installation/axon-agent/install/#configuration","title":"Configuration","text":"<p>Make sure axon-agent configuration points to the correct axon-server address and your organisation name is specified:</p> <pre><code>axon-server:\n    hosts: \"axon-server_endpoint\" # Specify axon-server IP axon-server.mycompany.com\n\naxon-agent:\n    org: \"my-company-test\" # Specify your organisation name\n    type: \"cassandra\"\n\nNTP:\n    host: \"ntp.mycompany.com\" # Specify you NTP server IP address or hostname\n</code></pre>"},{"location":"installation/axon-agent/install/#start-axon-agent","title":"Start axon-agent","text":"<pre><code>systemctl daemon-reload\nsystemctl start axon-agent\nsystemctl status axon-agent\n</code></pre> <p>This will start the axon-agent process as the axonops user, which was created during the package installation.</p> <ul> <li>Note that you will have to refresh axon-dash page to show the newly connected node.</li> </ul>"},{"location":"installation/axon-agent/install/#next-steps","title":"Next Steps","text":"<p>To complete your agent installation you will need to follow the steps in the link below:</p> <ul> <li>cassandra</li> </ul>"},{"location":"installation/axon-dash/install/","title":"AxonOps GUI installation","text":"<p>AxonOps GUI service is installed as a separate service to AxonOps Server. The GUI service (axon-dash) can be co-hosted on the same server as the AxonOps Server process, or they can be running on 2 separate servers.</p> <p>This section describes the installation process for the GUI service.</p>"},{"location":"installation/axon-dash/install/#step-1-installation","title":"Step 1 - Installation","text":""},{"location":"installation/axon-dash/install/#centos-redhat","title":"CentOS / RedHat","text":"<pre><code>sudo tee /etc/yum.repos.d/axonops-yum.repo &lt;&lt; EOL\n[axonops-yum]\nname=axonops-yum\nbaseurl=https://packages.axonops.com/yum/\nenabled=1\nrepo_gpgcheck=0\ngpgcheck=0\nEOL\n\nsudo yum install axon-dash\n</code></pre>"},{"location":"installation/axon-dash/install/#debian-ubuntu","title":"Debian / Ubuntu","text":"<pre><code>sudo apt-get update\nsudo apt-get install -y curl gnupg ca-certificates\ncurl -L https://packages.axonops.com/apt/repo-signing-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/axonops.gpg\necho \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list\nsudo apt-get update\nsudo apt-get install axon-dash\n</code></pre>"},{"location":"installation/axon-dash/install/#step-2-configuration","title":"Step 2 - Configuration","text":"<p>Change axon-dash configuration to specify axon-server listening address.</p> <ul> <li><code>/etc/axonops/axon-dash.yml</code> <pre><code>axon-dash: # The listening address of axon-dash\n  host: 0.0.0.0\n  port: 3000\n  line_charts_max_results: 256\n\naxon-server:\n  private_endpoints: \"http://127.0.0.1:8080\" # HTTP endpoint to access axon-server API from axon-dash.\n  context_path: \"\" # example: \"/gui\"\n</code></pre> <p>axon-server default API port is 8080</p> </li> </ul>"},{"location":"installation/axon-dash/install/#step-3-axon-server-configuration-update","title":"Step 3 - axon-server configuration update","text":"<p>if required, update axon-server configuration by setting the correct axon-dash host and port:</p> <ul> <li><code>/etc/axonops/axon-server.yml</code></li> </ul> <pre><code>...\naxon-dash: # This must point to the axon-dash address accessible from axon-server\n  host: 127.0.0.1\n  port: 3000\n  https: false\n...\n</code></pre>"},{"location":"installation/axon-dash/install/#step-4-restart-axon-server-after-updating-its-configuration","title":"Step 4 - Restart axon-server after updating its configuration","text":"<pre><code>sudo systemctl restart axon-server\n</code></pre>"},{"location":"installation/axon-dash/install/#step-5-start-axon-dash","title":"Step 5 - Start axon-dash","text":"<pre><code>sudo systemctl daemon-reload\nsudo systemctl start axon-dash\nsudo systemctl status axon-dash\n</code></pre> <p>This will start the axon-dash process as the axonops user, which was created during the package installation. The default listening address is <code>0.0.0.0:3000</code>.</p>"},{"location":"installation/axon-dash/install/#package-details","title":"Package details","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-dash.yml</code></li> <li>Binary: <code>/usr/share/axonops/axon-dash</code></li> <li>Logs: <code>/var/log/axonops/axon-dash.log</code> </li> <li>Systemd service: <code>/usr/lib/systemd/system/axon-dash.service</code></li> <li>Copyright : <code>/usr/share/doc/axonops/axon-dash/copyright</code></li> <li>Licenses : <code>/usr/share/axonops/licenses/axon-dash/</code></li> </ul>"},{"location":"installation/axon-dash/install/#step-6-installing-agents","title":"Step 6 - Installing agents","text":"<p>Now axon-dash is installed, you can start installing cassandra-agent</p>"},{"location":"installation/axon-server/centos/","title":"axon-server installation (CentOS / RedHat)","text":""},{"location":"installation/axon-server/centos/#step-1-prerequisites","title":"Step 1 - Prerequisites","text":"<p>Elasticsearch stores the data collected by axon-server. AxonOps is currently only compatible with Elasticsearch 7.x, we recommend installing the latest available 7.x release.</p>"},{"location":"installation/axon-server/centos/#installing-elasticsearch","title":"Installing Elasticsearch","text":"<pre><code>wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.16-x86_64.rpm\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.16-x86_64.rpm.sha512\nsha512sum -c elasticsearch-7.17.16-x86_64.rpm.sha512\nsudo rpm -i elasticsearch-7.17.16-x86_64.rpm\n</code></pre> <p>The <code>sha512sum</code> command above verifies the downloaded package and should show this output: <pre><code>elasticsearch-7.17.16-x86_64.rpm: OK\n</code></pre></p> <p>Increase the bulk queue size of Elasticsearch by running the following command:</p> <pre><code>sudo echo 'thread_pool.write.queue_size: 2000' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre> <p>Increase the default heap size of elasticsearch by editing <code>/etc/elasticsearch/jvm.options</code>. From: <pre><code>-Xms1g\n-Xmx1g \n</code></pre> To:  <pre><code>-Xms8g\n-Xmx8g \n</code></pre> This will set the minimum and maximum heap size to 8 GB. Set Xmx and Xms to no more than 50% of your physical RAM. Elasticsearch requires memory for purposes other than the JVM heap and it is important to leave space for this.</p> <p>Set the following index codec by running the following command: <pre><code>sudo echo 'index.codec: best_compression' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre></p> <p>Elasticsearch uses an mmapfs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions.</p> <p>You can increase the limits by running the following command:</p> <p><pre><code>sudo sysctl -w vm.max_map_count=262144\n</code></pre> To make this change persist across reboots run this command:  <pre><code>echo \"vm.max_map_count = 262144\" | sudo tee /etc/sysctl.d/10-elasticsearch.conf &gt;dev/null\n</code></pre></p> <p>Also, Elasticsearch needs <code>max file descriptors</code> system settings at least to 65536. <pre><code>echo 'elasticsearch  -  nofile  65536' | sudo tee --append /etc/security/limits.conf &gt; /dev/null\n</code></pre></p>"},{"location":"installation/axon-server/centos/#start-elasticsearch","title":"Start Elasticsearch","text":"<pre><code>sudo systemctl start elasticsearch.service\n</code></pre> <p>After a short period of time, you can verify that your Elasticsearch node is running by sending an HTTP request to port 9200 on localhost:</p> <pre><code>curl \"localhost:9200\"\n</code></pre>"},{"location":"installation/axon-server/centos/#step-2-axon-server","title":"Step 2 - axon-server","text":"<pre><code>sudo tee /etc/yum.repos.d/axonops-yum.repo &lt;&lt; EOL\n[axonops-yum]\nname=axonops-yum\nbaseurl=https://packages.axonops.com/yum/\nenabled=1\nrepo_gpgcheck=0\ngpgcheck=0\nEOL\n\nsudo yum install axon-server\n</code></pre>"},{"location":"installation/axon-server/centos/#step-3-axon-server-configurations","title":"Step 3 - axon-server configurations","text":"<p>Make sure elastic_host and elastic_port are corresponding to your Elasticsearch instance.</p> <ul> <li><code>/etc/axonops/axon-server.yml</code></li> </ul> <pre><code>host: 0.0.0.0  # axon-server listening address (used by axon-dash and axon-agent) (env variable: AXONSERVER_HOST)\napi_port: 8080 # axon-server HTTP API listening port (used by axon-dash) (AXONSERVER_PORT)\nagents_port: 1888 # axon-server listening port for agent connections \nelastic_hosts: #\u00a0Elasticsearch endpoint (env variable:ELASTIC_HOSTS, comma separated list)\n  - http://localhost:9200\n#integrations_proxy: # proxy endpoint for integrations. (INTEGRATIONS_PROXY)\n\n\n# AxonOps licensing\nlicense_key: license-key\norg_name: my-company\n\n# For better performance on large clusters, you can use a CQL store for the metrics.\n# To opt-in for CQL metrics storage, just specify at least one CQL host.\n# We do recommend to specify a NetworkTopologyStrategy for cql_keyspace_replication\n#cql_hosts: #  (CQL_HOSTS, comma separated list)\n#  - 192.168.0.10:9042\n#  - 192.168.0.11:9042\n#cql_username: \"cassandra\" # (CQL_USERNAME)\n#cql_password: \"cassandra\" # (CQL_PASSWORD)\n#cql_local_dc: datacenter1 # (CQL_LOCAL_DC)\n#cql_ssl: false # (CQL_SSL)\n#cql_skip_verify: false  # (CQL_SSL_SKIP_VERIFY)\n#cql_ca_file: /path/to/ca_file  # (CQL_CA_FILE)\n#cql_cert_file: /path/to/cert_file  # (CQL_CERT_FILE)\n#cql_key_file: /path/to/key_file  # (CQL_KEY_FILE)\n#cql_proto_version: 4  # (CQL_PROTO_VERSION)\n#cql_max_concurrent_reads: 1000  # (CQL_MAX_CONCURRENT_READS)\n#cql_batch_size: 1  # (CQL_BATCH_SIZE)\n#cql_page_size: 10  # (CQL_PAGE_SIZE)\n#cql_autocreate_tables: true # (CQL_AUTO_CREATE_TABLES) this will tell axon-server to automatically create the metrics tables (true is recommended)\n#cql_keyspace_replication: \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\" # (CQL_KS_REPLICATION) keyspace replication for the metrics tables\n#cql_retrypolicy_numretries: 3  # (CQL_RETRY_POLICY_NUM_RETRIES)\n#cql_retrypolicy_min: 1s  # (CQL_RETRY_POLICY_MIN)\n#cql_retrypolicy_max: 10s  # (CQL_RETRY_POLICY_MAX)\n#cql_reconnectionpolicy_maxretries: 10 # (CQL_RECONNECTION_POLICY_MAX_RETRIES)\n#cql_reconnectionpolicy_initialinterval: 1s # (CQL_RECONNECTION_POLICY_INITIAL_INTERVAL)\n#cql_reconnectionpolicy_maxinterval: 10s # (CQL_RECONNECTION_POLICY_MAX_INTERVAL)\n#cql_metrics_cache_max_size_mb: 100  #MB # (CQL_METRICS_CACHE_MAX_SIZE_MB)\n#cql_read_consistency: \"LOCAL_ONE\" # (CQL_READ_CONSISTENCY) #One of the following:  ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE\n#cql_write_consistency: \"LOCAL_ONE\" # (CQL_WRITE_CONSISTENCY) #One of the following:    ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE\n#cql_lvl1_compaction_window_size: 12 # (CQL_LVL1_COMPACTION_WINDOW_SIZE)\n#cql_lvl2_compaction_window_size: 1 # (CQL_LVL2_COMPACTION_WINDOW_SIZE)\n#cql_lvl3_compaction_window_size: 1 # (CQL_LVL3_COMPACTION_WINDOW_SIZE)\n#cql_lvl4_compaction_window_size: 10 # (CQL_LVL4_COMPACTION_WINDOW_SIZE)\n#cql_lvl5_compaction_window_size: 120 # (CQL_LVL5_COMPACTION_WINDOW_SIZE)\n\naxon-dash: # This must point to the axon-dash address accessible from axon-server\n  host: 127.0.0.1\n  port: 3000\n  https: false\n\nalerting:\n# How long to wait before sending a notification again if it has already\n# been sent successfully for an alert. (Usually ~3h or more).\n  notification_interval: 3h\n\n# Default retention settings, most can be overridden from the frontend\nretention:\n  events: 8w # logs and events retention. Must be expressed in weeks (w)\n  metrics:\n      high_resolution: 14d # High frequency metrics. Must be expressed in days (d)\n      med_resolution: 12w # Must be expressed in weeks (w)\n      low_resolution: 12M # Must be expressed in months (M)\n      super_low_resolution: 2y # Must be expressed in years (y)\n  backups: # Those are use as defaults but can be overridden from the UI\n    local: 10d\n    remote: 30d\n\n\n# Storage options for PDF reports\n# Override the default local path of /var/lib/axonops/reports\n#report_storage_path: /my/reports/storage/directory\n\n# Alternatively store PDF reports in an object store by providing report_storage_config\n#report_storage_path: my-reports-s3-bucket/reports-folder\n#report_storage_config:\n#  type: s3\n#  provider: AWS\n#  access_key_id: MY_ACCESS_KEY_ID\n#  secret_access_key: MY_SECRET_ACCESS_KEY\n#  region: us-east-1\n#  acl: private\n#  server_side_encryption: AES256\n#  storage_class: STANDARD\n</code></pre> <p>For better performances on large clusters (100+ nodes), you can use a CQL store for the metrics such as Cassandra. To opt-in for CQL metrics storage, specify at least one CQL host with axon-server configuration.</p>"},{"location":"installation/axon-server/centos/#step-4-start-the-server","title":"Step 4 - Start the server","text":"<pre><code>sudo systemctl daemon-reload\nsudo systemctl start axon-server\nsudo systemctl status axon-server\n</code></pre> <p>This will start the <code>axon-server</code> process as the <code>axonops</code> user, which was created during the package installation.  The default listening address is <code>0.0.0.0:8080</code>.</p>"},{"location":"installation/axon-server/centos/#package-details","title":"Package details","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-server.yml</code></li> <li>Binary: <code>/usr/share/axonops/axon-server</code></li> <li>Logs: <code>/var/log/axonops/axon-server.log</code> </li> <li>Systemd service: <code>/usr/lib/systemd/system/axon-server.service</code></li> <li>Copyright : <code>/usr/share/doc/axonops/axon-server/copyright</code></li> <li>Licenses : <code>/usr/share/axonops/licenses/axon-server/</code></li> </ul>"},{"location":"installation/axon-server/centos/#step-5-installing-axon-dash","title":"Step 5 - Installing axon-dash","text":"<p>Now axon-server is installed, you can start installing the GUI for it: axon-dash</p>"},{"location":"installation/axon-server/elastic/","title":"Elastic","text":"<p>Increase the bulk queue size of Elasticsearch by running the following command:</p> <pre><code>sudo echo 'thread_pool.write.queue_size: 2000' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre> <p>Increase the default heap size of elasticsearch by editing <code>/etc/elasticsearch/jvm.options</code>. From: <pre><code>-Xms1g\n-Xmx1g \n</code></pre> To:  <pre><code>-Xms8g\n-Xmx8g \n</code></pre> This will set the minimum and maximum heap size to 8 GB. Set Xmx and Xms to no more than 50% of your physical RAM. Elasticsearch requires memory for purposes other than the JVM heap and it is important to leave space for this.</p> <p>Set the following index codec by running the following command: <pre><code>sudo echo 'index.codec: best_compression' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre></p> <p>Elasticsearch uses an mmapfs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions.</p> <p>You can increase the limits by running the following command:</p> <p><pre><code>sudo sysctl -w vm.max_map_count=262144\n</code></pre> To make this change persist across reboots run this command:  <pre><code>echo \"vm.max_map_count = 262144\" | sudo tee /etc/sysctl.d/10-elasticsearch.conf &gt;dev/null\n</code></pre></p> <p>Also, Elasticsearch needs <code>max file descriptors</code> system settings at least to 65536. <pre><code>echo 'elasticsearch  -  nofile  65536' | sudo tee --append /etc/security/limits.conf &gt; /dev/null\n</code></pre></p>"},{"location":"installation/axon-server/elastic/#start-elasticsearch","title":"Start Elasticsearch","text":"<pre><code>sudo systemctl start elasticsearch.service\n</code></pre> <p>After a short period of time, you can verify that your Elasticsearch node is running by sending an HTTP request to port 9200 on localhost:</p> <pre><code>curl \"localhost:9200\"\n</code></pre>"},{"location":"installation/axon-server/install/","title":"Install","text":""},{"location":"installation/axon-server/install/#step-3-axon-server-configurations","title":"Step 3 - axon-server configurations","text":"<p>Make sure elastic_host and elastic_port are corresponding to your Elasticsearch instance.</p> <ul> <li><code>/etc/axonops/axon-server.yml</code></li> </ul> <pre><code>host: 0.0.0.0  # axon-server listening address (used by axon-dash and axon-agent) (env variable: AXONSERVER_HOST)\napi_port: 8080 # axon-server HTTP API listening port (used by axon-dash) (AXONSERVER_PORT)\nagents_port: 1888 # axon-server listening port for agent connections \nelastic_hosts: #\u00a0Elasticsearch endpoint (env variable:ELASTIC_HOSTS, comma separated list)\n  - http://localhost:9200\n#integrations_proxy: # proxy endpoint for integrations. (INTEGRATIONS_PROXY)\n\n\n# AxonOps licensing\nlicense_key: license-key\norg_name: my-company\n\n# For better performance on large clusters, you can use a CQL store for the metrics.\n# To opt-in for CQL metrics storage, just specify at least one CQL host.\n# We do recommend to specify a NetworkTopologyStrategy for cql_keyspace_replication\n#cql_hosts: #  (CQL_HOSTS, comma separated list)\n#  - 192.168.0.10:9042\n#  - 192.168.0.11:9042\n#cql_username: \"cassandra\" # (CQL_USERNAME)\n#cql_password: \"cassandra\" # (CQL_PASSWORD)\n#cql_local_dc: datacenter1 # (CQL_LOCAL_DC)\n#cql_ssl: false # (CQL_SSL)\n#cql_skip_verify: false  # (CQL_SSL_SKIP_VERIFY)\n#cql_ca_file: /path/to/ca_file  # (CQL_CA_FILE)\n#cql_cert_file: /path/to/cert_file  # (CQL_CERT_FILE)\n#cql_key_file: /path/to/key_file  # (CQL_KEY_FILE)\n#cql_proto_version: 4  # (CQL_PROTO_VERSION)\n#cql_max_concurrent_reads: 1000  # (CQL_MAX_CONCURRENT_READS)\n#cql_batch_size: 1  # (CQL_BATCH_SIZE)\n#cql_page_size: 10  # (CQL_PAGE_SIZE)\n#cql_autocreate_tables: true # (CQL_AUTO_CREATE_TABLES) this will tell axon-server to automatically create the metrics tables (true is recommended)\n#cql_keyspace_replication: \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\" # (CQL_KS_REPLICATION) keyspace replication for the metrics tables\n#cql_retrypolicy_numretries: 3  # (CQL_RETRY_POLICY_NUM_RETRIES)\n#cql_retrypolicy_min: 1s  # (CQL_RETRY_POLICY_MIN)\n#cql_retrypolicy_max: 10s  # (CQL_RETRY_POLICY_MAX)\n#cql_reconnectionpolicy_maxretries: 10 # (CQL_RECONNECTION_POLICY_MAX_RETRIES)\n#cql_reconnectionpolicy_initialinterval: 1s # (CQL_RECONNECTION_POLICY_INITIAL_INTERVAL)\n#cql_reconnectionpolicy_maxinterval: 10s # (CQL_RECONNECTION_POLICY_MAX_INTERVAL)\n#cql_metrics_cache_max_size_mb: 100  #MB # (CQL_METRICS_CACHE_MAX_SIZE_MB)\n#cql_read_consistency: \"LOCAL_ONE\" # (CQL_READ_CONSISTENCY) #One of the following:  ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE\n#cql_write_consistency: \"LOCAL_ONE\" # (CQL_WRITE_CONSISTENCY) #One of the following:    ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE\n#cql_lvl1_compaction_window_size: 12 # (CQL_LVL1_COMPACTION_WINDOW_SIZE)\n#cql_lvl2_compaction_window_size: 1 # (CQL_LVL2_COMPACTION_WINDOW_SIZE)\n#cql_lvl3_compaction_window_size: 1 # (CQL_LVL3_COMPACTION_WINDOW_SIZE)\n#cql_lvl4_compaction_window_size: 10 # (CQL_LVL4_COMPACTION_WINDOW_SIZE)\n#cql_lvl5_compaction_window_size: 120 # (CQL_LVL5_COMPACTION_WINDOW_SIZE)\n\naxon-dash: # This must point to the axon-dash address accessible from axon-server\n  host: 127.0.0.1\n  port: 3000\n  https: false\n\nalerting:\n# How long to wait before sending a notification again if it has already\n# been sent successfully for an alert. (Usually ~3h or more).\n  notification_interval: 3h\n\n# Default retention settings, most can be overridden from the frontend\nretention:\n  events: 8w # logs and events retention. Must be expressed in weeks (w)\n  metrics:\n      high_resolution: 14d # High frequency metrics. Must be expressed in days (d)\n      med_resolution: 12w # Must be expressed in weeks (w)\n      low_resolution: 12M # Must be expressed in months (M)\n      super_low_resolution: 2y # Must be expressed in years (y)\n  backups: # Those are use as defaults but can be overridden from the UI\n    local: 10d\n    remote: 30d\n\n\n# Storage options for PDF reports\n# Override the default local path of /var/lib/axonops/reports\n#report_storage_path: /my/reports/storage/directory\n\n# Alternatively store PDF reports in an object store by providing report_storage_config\n#report_storage_path: my-reports-s3-bucket/reports-folder\n#report_storage_config:\n#  type: s3\n#  provider: AWS\n#  access_key_id: MY_ACCESS_KEY_ID\n#  secret_access_key: MY_SECRET_ACCESS_KEY\n#  region: us-east-1\n#  acl: private\n#  server_side_encryption: AES256\n#  storage_class: STANDARD\n</code></pre> <p>For better performances on large clusters (100+ nodes), you can use a CQL store for the metrics such as Cassandra. To opt-in for CQL metrics storage, specify at least one CQL host with axon-server configuration.</p>"},{"location":"installation/axon-server/install/#step-4-start-the-server","title":"Step 4 - Start the server","text":"<pre><code>sudo systemctl daemon-reload\nsudo systemctl start axon-server\nsudo systemctl status axon-server\n</code></pre> <p>This will start the <code>axon-server</code> process as the <code>axonops</code> user, which was created during the package installation.  The default listening address is <code>0.0.0.0:8080</code>.</p>"},{"location":"installation/axon-server/install/#package-details","title":"Package details","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-server.yml</code></li> <li>Binary: <code>/usr/share/axonops/axon-server</code></li> <li>Logs: <code>/var/log/axonops/axon-server.log</code> </li> <li>Systemd service: <code>/usr/lib/systemd/system/axon-server.service</code></li> <li>Copyright : <code>/usr/share/doc/axonops/axon-server/copyright</code></li> <li>Licenses : <code>/usr/share/axonops/licenses/axon-server/</code></li> </ul>"},{"location":"installation/axon-server/install/#step-5-installing-axon-dash","title":"Step 5 - Installing axon-dash","text":"<p>Now axon-server is installed, you can start installing the GUI for it: axon-dash</p>"},{"location":"installation/axon-server/metricsdatabase/","title":"axon-server metrics database","text":""},{"location":"installation/axon-server/metricsdatabase/#using-cassandra-as-a-metrics-store","title":"Using Cassandra as a metrics store","text":"<p>You can use Cassandra as a metrics store instead of Elasticsearch for better performances. To start using a CQL store you just have to specify CQL hosts in axon-server.yml: <pre><code>cql_hosts :\n    - 192.168.0.1:9042\n    - 192.168.0.2:9042\n    ...\n</code></pre></p> <p>By default, the AxonOps server automatically creates the necessary keyspace and tables. You can override this behavior by specifying the following field in axon-server.yml: <pre><code>cql_autocreate_tables : false\ncql_keyspace : \"axonops\"\ncql_keyspace_replication : \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\n</code></pre></p> <p>We recommend setting up at least a 3 nodes cluster with NetworkTopologyStrategy and a replication_factor of 3.</p>"},{"location":"installation/axon-server/metricsdatabase/#connecting-to-encrypted-cassandra-metrics-store","title":"Connecting to encrypted Cassandra metrics store","text":"<p>We recommend setting up a Secured Socket Layer connection to Cassandra with the following fields in axon-server.yml: <pre><code>cql_ssl: true\ncql_skip_verify: false\ncql_ca_file: '/path/to/ca_cert'\ncql_cert_file: '/path/to/cert_file'\ncql_key_file: '/path/to/key_file'\n</code></pre></p>"},{"location":"installation/axon-server/metricsdatabase/#metrics-cache-recommendations","title":"Metrics cache recommendations","text":"<p>When using Cassandra as a metrics store the AxonOps server can cache metrics data in memory to further improve performance. This is configured using the <code>cql_metrics_cache_max_items</code> and <code>cql_metrics_cache_max_size_mb</code> options  in axon-server.yml. The default values are shown here: <pre><code>cql_metrics_cache_max_items: 5000000\ncql_metrics_cache_max_size_mb: 400\n</code></pre></p> <p>The recommended settings for these options are as follows:</p> Cassandra Nodes cql_metrics_cache_max_items cql_metrics_cache_max_size_mb &lt;10 5000000 1000 &lt;50 5000000 4000 100 10000000 10000 200 10000000 20000 <p>These sizes can be tuned to balance memory use in AxonOps against the read workload on Cassandra. When tuning these parameters it is recommended to set <code>cql_metrics_cache_max_items</code> to a high value and limit the cache size with <code>cql_metrics_cache_max_size_mb</code>.</p>"},{"location":"installation/axon-server/metricsdatabase/#other-cql-fields","title":"Other CQL fields","text":"<p>You can also specify the following fields: <pre><code>cql_proto_version int                   \ncql_batch_size  int                   \ncql_page_size int                   \ncql_local_dc string                \ncql_username string                \ncql_password string                \ncql_max_concurrent_reads int                   \ncql_retrypolicy_numretries int                   \ncql_retrypolicy_min string \"1s\"\ncql_retrypolicy_max string \"10s\"\ncql_reconnectionpolicy_maxretries int                   \ncql_reconnectionpolicy_initialinterval string \"1s\"\ncql_reconnectionpolicy_maxinterval string  \"10s\"\ncql_metrics_cache_max_size_mb int64 in MB               \ncql_metrics_cache_max_items  int64 in MB                        \ncql_read_consistency string (controls the consistency of read operations, defaults to LOCAL_ONE)              \ncql_write_consistency string (controls the consistency of write operations, defaults to LOCAL_ONE)               \ncql_lvl1_compaction_window_size int (used for the table named 'metrics5' when you let axonserver managing the tables automatically)                  \ncql_lvl2_compaction_window_size int (used for the table named 'metrics60' when you let axonserver managing the tables automatically)                  \ncql_lvl3_compaction_window_size int (used for the table named 'metrics720' when you let axonserver managing the tables automatically)                  \ncql_lvl4_compaction_window_size int (used for the table named 'metrics7200' when you let axonserver managing the tables automatically)                  \ncql_lvl5_compaction_window_size int (used for the table named 'metrics86400' when you let axonserver managing the tables automatically)                  \n</code></pre></p> <p>The CQL for the default tables are the following: <pre><code>CREATE TABLE IF NOT EXISTS axonops.metrics5 (\n    orgid text,\n    metricid int,\n    time int,\n    value float,\n    PRIMARY KEY ((orgid, metricid), time)\n) WITH CLUSTERING ORDER BY (time DESC)\n   AND caching = {'keys': 'ALL', 'rows_per_partition': '256'}\n   AND compaction = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_size': '1', 'compaction_window_unit': 'DAYS', 'max_threshold': '32', 'min_threshold': '4'}\n   AND default_time_to_live = 604800\n   AND comment = '7 days retention for 5 seconds resolution metrics';\n\nCREATE TABLE IF NOT EXISTS axonops.metrics60 (\n    orgid text,\n    metricid int,\n    time int,\n    value float,\n    PRIMARY KEY ((orgid, metricid), time)\n) WITH CLUSTERING ORDER BY (time DESC)\n   AND caching = {'keys': 'ALL', 'rows_per_partition': '256'}\n   AND compaction = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_size': '1', 'compaction_window_unit': 'DAYS', 'max_threshold': '32', 'min_threshold': '4'}\n   AND default_time_to_live = 2592000\n   AND comment = '30 days retention for 60 seconds resolution metrics';\n\nCREATE TABLE IF NOT EXISTS axonops.metrics720 (\n    orgid text,\n    metricid int,\n    time int,\n    value float,\n    PRIMARY KEY ((orgid, metricid), time)\n) WITH CLUSTERING ORDER BY (time DESC)\n   AND caching = {'keys': 'ALL', 'rows_per_partition': '256'}\n   AND compaction = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_size': '4', 'compaction_window_unit': 'DAYS', 'max_threshold': '32', 'min_threshold': '4'}\n   AND default_time_to_live = 5184000\n   AND comment = '60 days retention for 720 seconds resolution metrics';\n\nCREATE TABLE IF NOT EXISTS axonops.metrics7200 (\n    orgid text,\n    metricid int,\n    time int,\n    value float,\n    PRIMARY KEY ((orgid, metricid), time)\n) WITH CLUSTERING ORDER BY (time DESC)\n   AND caching = {'keys': 'ALL', 'rows_per_partition': '256'}\n   AND compaction = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_size': '30', 'compaction_window_unit': 'DAYS', 'max_threshold': '32', 'min_threshold': '4'}\n   AND default_time_to_live = 15552000\n   AND comment = '180 days retention for 7200 seconds resolution metrics';\n\nCREATE TABLE IF NOT EXISTS axonops.metrics86400 (\n    orgid text,\n    metricid int,\n    time int,\n    value float,\n    PRIMARY KEY ((orgid, metricid), time)\n) WITH CLUSTERING ORDER BY (time DESC)\n   AND caching = {'keys': 'ALL', 'rows_per_partition': '365'}\n   AND compaction = {'class': 'TimeWindowCompactionStrategy', 'compaction_window_size': '60', 'compaction_window_unit': 'DAYS', 'max_threshold': '32', 'min_threshold': '4'}\n   AND default_time_to_live = 31536000\n   AND comment = '365 days retention for 86400 seconds resolution metrics';\n</code></pre></p>"},{"location":"installation/axon-server/ubuntu/","title":"axon-server installation (Debian / Ubuntu)","text":""},{"location":"installation/axon-server/ubuntu/#step-1-prerequisites","title":"Step 1 - Prerequisites","text":"<p>Elasticsearch stores the data collected by axon-server. AxonOps is currently only compatible with Elasticsearch 7.x, we recommend installing the latest available 7.x release.</p>"},{"location":"installation/axon-server/ubuntu/#installing-elasticsearch","title":"Installing Elasticsearch","text":"<pre><code>wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.16-amd64.deb\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.16-amd64.deb.sha512\nshasum -a 512 -c elasticsearch-7.17.16-amd64.deb.sha512\nsudo dpkg -i elasticsearch-7.17.16-amd64.deb\n</code></pre> <p>The <code>shasum</code> command above verifies the downloaded package and should show this output: <pre><code>elasticsearch-7.17.16-amd64.deb: OK\n</code></pre></p> <p>Increase the bulk queue size of Elasticsearch by running the following command:</p> <pre><code>sudo echo 'thread_pool.write.queue_size: 2000' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre> <p>Increase the default heap size of elasticsearch by editing <code>/etc/elasticsearch/jvm.options</code>. From: <pre><code>-Xms1g\n-Xmx1g \n</code></pre> To:  <pre><code>-Xms8g\n-Xmx8g \n</code></pre> This will set the minimum and maximum heap size to 8 GB. Set Xmx and Xms to no more than 50% of your physical RAM. Elasticsearch requires memory for purposes other than the JVM heap and it is important to leave space for this.</p> <p>Set the following index codec by running the following command: <pre><code>sudo echo 'index.codec: best_compression' &gt;&gt; /etc/elasticsearch/elasticsearch.yml\n</code></pre></p> <p>Elasticsearch uses an mmapfs directory by default to store its indices. The default operating system limits on mmap counts is likely to be too low, which may result in out of memory exceptions.</p> <p>You can increase the limits by running the following command:</p> <p><pre><code>sudo sysctl -w vm.max_map_count=262144\n</code></pre> To make this change persist across reboots run this command:  <pre><code>echo \"vm.max_map_count = 262144\" | sudo tee /etc/sysctl.d/10-elasticsearch.conf &gt;dev/null\n</code></pre></p> <p>Also, Elasticsearch needs <code>max file descriptors</code> system settings at least to 65536. <pre><code>echo 'elasticsearch  -  nofile  65536' | sudo tee --append /etc/security/limits.conf &gt; /dev/null\n</code></pre></p>"},{"location":"installation/axon-server/ubuntu/#start-elasticsearch","title":"Start Elasticsearch","text":"<pre><code>sudo systemctl start elasticsearch.service\n</code></pre> <p>After a short period of time, you can verify that your Elasticsearch node is running by sending an HTTP request to port 9200 on localhost:</p> <pre><code>curl \"localhost:9200\"\n</code></pre>"},{"location":"installation/axon-server/ubuntu/#step-2-axon-server","title":"Step 2 - axon-server","text":"<pre><code>sudo apt-get update\nsudo apt-get install -y curl gnupg ca-certificates\ncurl -L https://packages.axonops.com/apt/repo-signing-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/axonops.gpg\necho \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list\nsudo apt-get update\nsudo apt-get install axon-server\n</code></pre>"},{"location":"installation/axon-server/ubuntu/#step-3-axon-server-configurations","title":"Step 3 - axon-server configurations","text":"<p>Make sure elastic_host and elastic_port are corresponding to your Elasticsearch instance.</p> <ul> <li><code>/etc/axonops/axon-server.yml</code></li> </ul> <pre><code>host: 0.0.0.0  # axon-server listening address (used by axon-dash and axon-agent) (env variable: AXONSERVER_HOST)\napi_port: 8080 # axon-server HTTP API listening port (used by axon-dash) (AXONSERVER_PORT)\nagents_port: 1888 # axon-server listening port for agent connections \nelastic_hosts: #\u00a0Elasticsearch endpoint (env variable:ELASTIC_HOSTS, comma separated list)\n  - http://localhost:9200\n#integrations_proxy: # proxy endpoint for integrations. (INTEGRATIONS_PROXY)\n\n\n# AxonOps licensing\nlicense_key: license-key\norg_name: my-company\n\n# For better performance on large clusters, you can use a CQL store for the metrics.\n# To opt-in for CQL metrics storage, just specify at least one CQL host.\n# We do recommend to specify a NetworkTopologyStrategy for cql_keyspace_replication\n#cql_hosts: #  (CQL_HOSTS, comma separated list)\n#  - 192.168.0.10:9042\n#  - 192.168.0.11:9042\n#cql_username: \"cassandra\" # (CQL_USERNAME)\n#cql_password: \"cassandra\" # (CQL_PASSWORD)\n#cql_local_dc: datacenter1 # (CQL_LOCAL_DC)\n#cql_ssl: false # (CQL_SSL)\n#cql_skip_verify: false  # (CQL_SSL_SKIP_VERIFY)\n#cql_ca_file: /path/to/ca_file  # (CQL_CA_FILE)\n#cql_cert_file: /path/to/cert_file  # (CQL_CERT_FILE)\n#cql_key_file: /path/to/key_file  # (CQL_KEY_FILE)\n#cql_proto_version: 4  # (CQL_PROTO_VERSION)\n#cql_max_concurrent_reads: 1000  # (CQL_MAX_CONCURRENT_READS)\n#cql_batch_size: 1  # (CQL_BATCH_SIZE)\n#cql_page_size: 10  # (CQL_PAGE_SIZE)\n#cql_autocreate_tables: true # (CQL_AUTO_CREATE_TABLES) this will tell axon-server to automatically create the metrics tables (true is recommended)\n#cql_keyspace_replication: \"{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\" # (CQL_KS_REPLICATION) keyspace replication for the metrics tables\n#cql_retrypolicy_numretries: 3  # (CQL_RETRY_POLICY_NUM_RETRIES)\n#cql_retrypolicy_min: 1s  # (CQL_RETRY_POLICY_MIN)\n#cql_retrypolicy_max: 10s  # (CQL_RETRY_POLICY_MAX)\n#cql_reconnectionpolicy_maxretries: 10 # (CQL_RECONNECTION_POLICY_MAX_RETRIES)\n#cql_reconnectionpolicy_initialinterval: 1s # (CQL_RECONNECTION_POLICY_INITIAL_INTERVAL)\n#cql_reconnectionpolicy_maxinterval: 10s # (CQL_RECONNECTION_POLICY_MAX_INTERVAL)\n#cql_metrics_cache_max_size_mb: 100  #MB # (CQL_METRICS_CACHE_MAX_SIZE_MB)\n#cql_read_consistency: \"LOCAL_ONE\" # (CQL_READ_CONSISTENCY) #One of the following:  ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE\n#cql_write_consistency: \"LOCAL_ONE\" # (CQL_WRITE_CONSISTENCY) #One of the following:    ANY, ONE, TWO, THREE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM, LOCAL_ONE\n#cql_lvl1_compaction_window_size: 12 # (CQL_LVL1_COMPACTION_WINDOW_SIZE)\n#cql_lvl2_compaction_window_size: 1 # (CQL_LVL2_COMPACTION_WINDOW_SIZE)\n#cql_lvl3_compaction_window_size: 1 # (CQL_LVL3_COMPACTION_WINDOW_SIZE)\n#cql_lvl4_compaction_window_size: 10 # (CQL_LVL4_COMPACTION_WINDOW_SIZE)\n#cql_lvl5_compaction_window_size: 120 # (CQL_LVL5_COMPACTION_WINDOW_SIZE)\n\naxon-dash: # This must point to the axon-dash address accessible from axon-server\n  host: 127.0.0.1\n  port: 3000\n  https: false\n\nalerting:\n# How long to wait before sending a notification again if it has already\n# been sent successfully for an alert. (Usually ~3h or more).\n  notification_interval: 3h\n\n# Default retention settings, most can be overridden from the frontend\nretention:\n  events: 8w # logs and events retention. Must be expressed in weeks (w)\n  metrics:\n      high_resolution: 14d # High frequency metrics. Must be expressed in days (d)\n      med_resolution: 12w # Must be expressed in weeks (w)\n      low_resolution: 12M # Must be expressed in months (M)\n      super_low_resolution: 2y # Must be expressed in years (y)\n  backups: # Those are use as defaults but can be overridden from the UI\n    local: 10d\n    remote: 30d\n\n\n# Storage options for PDF reports\n# Override the default local path of /var/lib/axonops/reports\n#report_storage_path: /my/reports/storage/directory\n\n# Alternatively store PDF reports in an object store by providing report_storage_config\n#report_storage_path: my-reports-s3-bucket/reports-folder\n#report_storage_config:\n#  type: s3\n#  provider: AWS\n#  access_key_id: MY_ACCESS_KEY_ID\n#  secret_access_key: MY_SECRET_ACCESS_KEY\n#  region: us-east-1\n#  acl: private\n#  server_side_encryption: AES256\n#  storage_class: STANDARD\n</code></pre> <p>For better performances on large clusters (100+ nodes), you can use a CQL store for the metrics such as Cassandra. To opt-in for CQL metrics storage, specify at least one CQL host with axon-server configuration.</p>"},{"location":"installation/axon-server/ubuntu/#step-4-start-the-server","title":"Step 4 - Start the server","text":"<pre><code>sudo systemctl daemon-reload\nsudo systemctl start axon-server\nsudo systemctl status axon-server\n</code></pre> <p>This will start the <code>axon-server</code> process as the <code>axonops</code> user, which was created during the package installation.  The default listening address is <code>0.0.0.0:8080</code>.</p>"},{"location":"installation/axon-server/ubuntu/#package-details","title":"Package details","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-server.yml</code></li> <li>Binary: <code>/usr/share/axonops/axon-server</code></li> <li>Logs: <code>/var/log/axonops/axon-server.log</code> </li> <li>Systemd service: <code>/usr/lib/systemd/system/axon-server.service</code></li> <li>Copyright : <code>/usr/share/doc/axonops/axon-server/copyright</code></li> <li>Licenses : <code>/usr/share/axonops/licenses/axon-server/</code></li> </ul>"},{"location":"installation/axon-server/ubuntu/#step-5-installing-axon-dash","title":"Step 5 - Installing axon-dash","text":"<p>Now axon-server is installed, you can start installing the GUI for it: axon-dash</p>"},{"location":"installation/cassandra-agent/docker/","title":"Installing axon-agent for Cassandra in Docker","text":"<p>Caveats</p> <ul> <li>Cassandra logs cannot normally be collected by AxonOps as they are sent to stdout and handled by the     Docker logging driver</li> <li>If axon-agent is running under Docker it assumes that the Cassandra user's GID is 999 as it is in the     official Cassandra images. If this is not the case then AxonOps may not be able to backup the Cassandra data.</li> </ul> <p>To enable the full functionality of the AxonOps agent some directories must be accessible to both the Cassandra and AxonOps Agent processes.</p> Directory Required Description <code>/var/lib/axonops</code> Required Contains UNIX domain sockets, Cassandra agent jars and local data stored by the agent. This directory must be readable and writable by Cassandra and AxonOps <code>/etc/axonops</code> Required Contains the configuration for AxonOps. This directory must be readable by Cassandra and AxonOps <code>/var/log/axonops</code> Required The Cassandra agent will write logs to this directory where they will be buffered and sent to the AxonOps server This directory must be writable by Cassandra and readable by AxonOps <code>/var/lib/cassandra</code> Optional For the backups feature to function correctly the Cassandra data directory must be readable by the AxonOps agent <p>When running Cassandra under Docker it is possible to run the AxonOps agent either on the host or in another  Docker container. When installing on the host follow the instructions under AxonOps Cassandra agent installation to install the agent and ensure that the appropriate directories are mapped into the Cassandra container.</p>"},{"location":"installation/cassandra-agent/docker/#example-with-docker-compose","title":"Example with Docker Compose","text":"<p>This example shows running a single Cassandra node and the AxonOps agent under Docker Compose using host volumes to share data between the containers.</p> <pre><code>version: \"3\"\n\nservices:\n  cassandra:\n    image: cassandra:4.0\n    container_name: cassandra\n    restart: always\n    volumes:\n      - ./cassandra:/var/lib/cassandra\n      - ./axonops/var:/var/lib/axonops\n      - ./axonops/etc:/etc/axonops\n      - ./axonops/log:/var/log/axonops\n    ports:\n      - \"9042:9042\"\n    environment:\n      - JVM_EXTRA_OPTS=-javaagent:/var/lib/axonops/axon-cassandra4.0-agent.jar=/etc/axonops/axon-agent.yml\n      - CASSANDRA_CLUSTER_NAME=my-cluster\n\n  axon-agent:\n    image: registry.axonops.com/axonops-public/axonops-docker/axon-agent:latest\n    restart: always\n    environment:\n      # Enter the hostname or IP address of your AxonOps server here\n      - AXON_AGENT_SERVER_HOST=axonops-server.example.com\n    volumes:\n      - ./cassandra:/var/lib/cassandra\n      - ./axonops/var:/var/lib/axonops\n      - ./axonops/etc:/etc/axonops\n      - ./axonops/log:/var/log/axonops\n</code></pre>"},{"location":"installation/cassandra-agent/install/","title":"AxonOps Cassandra agent installation","text":"<p>This agent will enable metrics, logs and events collection with adaptive repairs and backups for Cassandra.</p> <p>See Installing axon-agent for Cassandra in Docker if you are running Cassandra under Docker.</p>"},{"location":"installation/cassandra-agent/install/#available-versions","title":"Available versions","text":"<ul> <li>Apache Cassandra 4.1.x</li> <li>Apache Cassandra 4.0.x</li> <li>Apache Cassandra 3.11.x</li> <li>Apache Cassandra 3.0.x</li> </ul>"},{"location":"installation/cassandra-agent/install/#step-1-installation","title":"Step 1 - Installation","text":"<p>Make sure that the <code>{version}</code> of your Cassandra and Cassandra agent are compatible from the compatibility matrix. </p>"},{"location":"installation/cassandra-agent/install/#centos-redhat","title":"CentOS / RedHat","text":"<pre><code>sudo tee /etc/yum.repos.d/axonops-yum.repo &lt;&lt; EOL\n[axonops-yum]\nname=axonops-yum\nbaseurl=https://packages.axonops.com/yum/\nenabled=1\nrepo_gpgcheck=0\ngpgcheck=0\nEOL\n\nsudo yum install axon-cassandra{version}-agent\n</code></pre>"},{"location":"installation/cassandra-agent/install/#debian-ubuntu","title":"Debian / Ubuntu","text":"<pre><code>sudo apt-get update\nsudo apt-get install -y curl gnupg ca-certificates\ncurl -L https://packages.axonops.com/apt/repo-signing-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/axonops.gpg\necho \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list\nsudo apt-get update\nsudo apt-get install axon-cassandra{version}-agent\n</code></pre> <p>Note: This will install the AxonOps Cassandra agent and its dependency: axon-agent</p>"},{"location":"installation/cassandra-agent/install/#step-2-agent-configuration","title":"Step 2 - Agent Configuration","text":"<p>Update the following highlighted lines from <code>/etc/axonops/axon-agent.yml</code>:</p> <pre><code>axon-server:\n    hosts: \"axon-server_endpoint\" # Your axon-server IP or hostname, e.g. axonops.mycompany.com\n    port: 1888 # The default axon-server port is 1888\n\naxon-agent:\n    org: \"my-company\" # Your organisation name\n\nNTP:\n    host: \"ntp.mycompany.com\" # Your NTP server IP address or hostname \n</code></pre>"},{"location":"installation/cassandra-agent/install/#step-3-configure-cassandra","title":"Step 3 - Configure Cassandra","text":"<p>Edit <code>cassandra-env.sh</code>, which is usually located in <code>/&lt;Cassandra Installation Directory&gt;/conf/cassandra-env.sh</code> for tarball installs or <code>/etc/cassandra/cassandra-env.sh</code> for package installs, and append the following line at the end of the file:</p> <pre><code>JVM_OPTS=\"$JVM_OPTS -javaagent:/usr/share/axonops/axon-cassandra{version}-agent.jar=/etc/axonops/axon-agent.yml\"\n</code></pre> <p>for example with Cassandra agent version 3.11: <pre><code>JVM_OPTS=\"$JVM_OPTS -javaagent:/usr/share/axonops/axon-cassandra3.11-agent.jar=/etc/axonops/axon-agent.yml\"\n</code></pre> Make sure that this configuration will not get overridden by an automation tool.</p>"},{"location":"installation/cassandra-agent/install/#step-4-add-axonops-user-to-cassandra-user-group-and-cassandra-user-to-axonops-group","title":"Step 4 - Add axonops user to Cassandra user group and Cassandra user to axonops group","text":"<pre><code>sudo usermod -aG &lt;your_cassandra_group&gt; axonops\nsudo usermod -aG axonops &lt;your_cassandra_user&gt;\n</code></pre>"},{"location":"installation/cassandra-agent/install/#step-5-start-cassandra","title":"Step 5 - Start Cassandra","text":""},{"location":"installation/cassandra-agent/install/#step-6-start-axon-agent","title":"Step 6 - Start axon-agent","text":"<pre><code>sudo systemctl start axon-agent\n</code></pre>"},{"location":"installation/cassandra-agent/install/#optional-step-7-cassandra-remote-backups-or-restore-prerequisites","title":"(Optional) Step 7 - Cassandra Remote Backups or Restore Prerequisites","text":"<ul> <li> <p>If you plan to use AxonOps remote backup functionality, axonops user will require read access on Cassandra data folder.</p> </li> <li> <p>As well if you plan to Restore data with AxonOps,  axonops user will require write access to Cassandra data folder. We recommend to only provide temporary write access to axonops when required.</p> </li> </ul>"},{"location":"installation/cassandra-agent/install/#cassandra-agent-package-details","title":"Cassandra agent Package details","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-agent.yml</code></li> <li>Binary: <code>/usr/share/axonops/axon-cassandra{version}-agent.jar</code></li> <li>Version number: <code>/usr/share/axonops/axon-cassandra{version}-agent.version</code></li> <li>Copyright : <code>/usr/share/doc/axonops/axon-cassandra{version}-agent/copyright</code></li> <li>Licenses : <code>/usr/share/axonops/licenses/axon-cassandra{version}-agent/</code></li> </ul>"},{"location":"installation/cassandra-agent/install/#axon-agent-package-details-dependency-of-cassandra-agent","title":"axon-agent Package details (dependency of Cassandra agent)","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-agent.yml</code></li> <li>Binary: <code>usr/share/axonops/axon-agent</code></li> <li>Logs : <code>/var/log/axonops/axon-agent.log</code></li> <li>Systemd service: <code>/usr/lib/systemd/system/axon-agent.service</code></li> </ul>"},{"location":"installation/compat_matrix/compat_matrix/","title":"Interoperability matrix","text":""},{"location":"installation/compat_matrix/compat_matrix/#axonops-server","title":"AxonOps Server","text":"Operating Systems Target Architecture RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [10,11,12] x86_64"},{"location":"installation/compat_matrix/compat_matrix/#axonops-gui-server","title":"AxonOps GUI Server","text":"Operating Systems Target Architecture RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [10,11,12] x86_64"},{"location":"installation/compat_matrix/compat_matrix/#axonops-cassandra-agent","title":"AxonOps Cassandra Agent","text":"System AxonOps Cassandra Agent Name Java Versions Operating Systems Target Architecture Cassandra 3.0.x axon-cassandra3.0-agent Java 8 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 3.11.x axon-cassandra3.11-agent Java 8 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 4.0.x axon-cassandra4.0-agent Java 11 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 4.0.x axon-cassandra4.0-agent-jdk8 Java 8 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 4.1.x axon-cassandra4.1-agent Java 11 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 4.1.x axon-cassandra4.1-agent-jdk8 Java 8 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64 Cassandra 5.0.x axon-cassandra5.0-agent-jdk11 Java 11 RedHat [7,8], CentOS [7,8], Ubuntu [18.04, 20.04, 22.04], Debian [11,12] x86_64, arm64"},{"location":"installation/compat_matrix/compat_matrix/#java-versions","title":"Java Versions","text":"<ul> <li>Oracle Java Standard Edition 8 </li> <li>Oracle Java Standard Edition 11 (Long Term Support)</li> <li>OpenJDK 8</li> <li>OpenJDK 11</li> </ul>"},{"location":"installation/dse-agent/install/","title":"axon-java-agent for DSE installation","text":"<p>This agent will enable metrics collection from DSE and enable adaptive repairs and backups.</p>"},{"location":"installation/dse-agent/install/#prerequisites","title":"Prerequisites","text":"<p>DSE agent needs axon-agent to be installed and configured properly. If not installed already, please go to axon-agent installation  page.</p>"},{"location":"installation/dse-agent/install/#setup-axon-agent-for-dse","title":"Setup axon-agent for DSE","text":"<p>You'll need the specify/update the following lines from axon-agent.yml located in <code>/etc/axonops/axon-agent.yml</code>:</p> <p><pre><code>axon-server:\n    hosts: \"axon-server_endpoint\" # Specify axon-server endpoint\n\naxon-agent:\n    host: 0.0.0.0 # axon-agent listening address for it's OpenTSDB endpoint\n    port: 9916 # axon-agent listening port for it's OpenTSDB endpoint\n    org: \"your_organisation_name\" # Specify your organisation name\n    standalone_mode: false\n    type: \"dse\"\n    #cluster_name: \"standalone\" # comment that line\n    ssl: false # SSL flag for it's OpenTSDB endpoint\n</code></pre> * Set <code>standalone_mode</code> to false * Set <code>type</code> to dse * Don't forget to comment or remove the <code>cluster_name</code> as it will be deduced from DSE configuration. * Don't forget to specify axon-server host and port if that's not already specified.</p>"},{"location":"installation/dse-agent/install/#dse-agent-installation","title":"DSE agent installation","text":"<p>Make sure the <code>{version}</code> of your DSE and DSE agent are compatible from the compatibility matrix. </p>"},{"location":"installation/dse-agent/install/#centos-redhat-installer","title":"CentOS / RedHat installer","text":"<pre><code>sudo yum install &lt;TODO&gt;\n</code></pre>"},{"location":"installation/dse-agent/install/#debian-ubuntu-installer","title":"Debian / Ubuntu installer","text":"<pre><code>sudo apt-get install &lt;TODO&gt;\n</code></pre>"},{"location":"installation/dse-agent/install/#package-details","title":"Package details","text":"<ul> <li>Configuration: <code>/etc/axonops/axon-java-agent.yml</code></li> <li>Binary: <code>usr/share/axonops/axon-dse{version}-agent-1.0.jar</code></li> <li>Version number: <code>usr/share/axonops/axon-dse{version}-agent-1.0.version</code></li> </ul>"},{"location":"installation/dse-agent/install/#configure-dse","title":"Configure DSE","text":"<p>Edit <code>cassandra-env.sh</code> usually located in your dse install path such as <code>/&lt;path_to_DSE&gt;/resources/cassandra/conf/cassandra-env.sh</code> and add at the end of the file the following line:</p> <pre><code>JVM_OPTS=\"$JVM_OPTS -javaagent:/usr/share/axonops/axon-dse{version}-agent-1.0.jar=/etc/axonops/axon-java-agent.yml\"\n</code></pre> <p>example: <pre><code>JVM_OPTS=\"$JVM_OPTS -javaagent:/usr/share/axonops/axon-dse6.0.4-agent-1.0.jar=/etc/axonops/axon-java-agent.yml\"\n</code></pre></p>"},{"location":"installation/dse-agent/install/#start-dse","title":"Start DSE","text":"<p>All you need to do now is start DSE.</p>"},{"location":"installation/dse-agent/install/#configuration-defaults","title":"Configuration defaults","text":"<pre><code>tier0: # metrics collected every 5 seconds\n    metrics:\n        jvm_:\n          - \"java.lang:*\"\n        cas_:\n          - \"org.apache.cassandra.metrics:*\"\n          - \"org.apache.cassandra.net:type=FailureDetector\"\n          - \"com.datastax.bdp:type=dsefs,*\"\n\ntier1:\n    frequency: 300 # metrics collected every 300 seconds (5m)\n    metrics:\n        cas_:\n          - \"org.apache.cassandra.metrics:name=EstimatedPartitionCount,*\"\n\n#tier2:\n#    frequency: 3600 # 1h\n\n#tier3:\n#    frequency: 86400 # 1d\n\nblacklist: #\u00a0You can blacklist metrics based on MBean query pattern\n  - \"org.apache.cassandra.metrics:type=ColumnFamily,*\" # dup of tables\n  - \"org.apache.cassandra.metrics:name=SnapshotsSize,*\" # generally takes time\n\nfree_text_blacklist: #\u00a0You can blacklist metrics based on Regex pattern\n  - \"org.apache.cassandra.metrics:type=ThreadPools,path=internal,scope=Repair#.*\"\n\nwarningThresholdMillis: 100 # This will warn in logs when a MBean takes longer than the specified value.\n\nwhitelisted_clients: # Whitelist for CQL connections\n  - \"127.0.0.1\"\n  - \"^*.*.*.*\"\n</code></pre>"},{"location":"installation/kubernetes/","title":"Running AxonOps on Kubernetes","text":""},{"location":"installation/kubernetes/#introduction","title":"Introduction","text":"<p>The following shows how to install AxonOps for monitoring cassandra. AxonOps requires ElasticSearch and the documentation below shows how to install both. If you already have ElasticSearch running, you can omit the installation and just ensure the AxonOps config points to it.</p> <p>AxonOps installation uses Helm Charts. Helm v3.8.0 or later is required in order to access the OCI repository hosting the charts. The raw charts can be downloaded from the GitHub repository.</p>"},{"location":"installation/kubernetes/#preparing-the-configuration","title":"Preparing the configuration","text":""},{"location":"installation/kubernetes/#resources","title":"Resources","text":"Cassandra Nodes ElasticSearch CPU ElasticSearch Memory AxonOps Server CPU AxonOps Server Memory &lt;10 1000m 4Gi 750m 1Gi &lt;50 1000m 4Gi 2000m 6Gi 100 2000m 16Gi 4000m 12Gi 200 4000m 32Gi 8000m 24Gi"},{"location":"installation/kubernetes/#elasticsearch","title":"ElasticSearch","text":"<p>The example below is a configuration file for the official ElasticSearch helm repository. See inline comments:</p> <pre><code>---\nclusterName: \"axonops-elastic\"\n\nreplicas: 1\n\nesConfig:\n  elasticsearch.yml: |\n    thread_pool.write.queue_size: 2000\n\nroles:\n  master: \"true\"\n  ingest: \"true\"\n  data: \"true\"\n  remote_cluster_client: \"false\"\n  ml: \"false\"\n\n# Adjust the memory and cpu requirements to your deployment\n# \nesJavaOpts: \"-Xms2g -Xmx2g\"\n\nresources:\n  requests:\n    cpu: \"750m\"\n    memory: \"2Gi\"\n  limits:\n    cpu: \"1500m\"\n    memory: \"4Gi\"\n\nvolumeClaimTemplate:\n  accessModes: [\"ReadWriteOnce\"]\n  storageClassName: \"\" # adjust to your storageClass if you don't want to use default\n  resources:\n    requests:\n      storage: 50Gi\n\nrbac:\n  create: true\n</code></pre>"},{"location":"installation/kubernetes/#axonops","title":"AxonOps","text":"<p>The default AxonOps installation does not expose the services outside of the cluster. We recommend that you use either a LoadBalancer service or an Ingress.</p> <p>Below you can find an example using <code>Ingress</code> to expose both the dashboard and the AxonOps server.</p> <pre><code>axon-dash:\n  image:\n    pullPolicy: IfNotPresent\n    repository: registry.axonops.com/axonops-public/axonops-docker/axon-dash\n    tag: latest\n  ingress:\n    enabled: true\n    className: nginx\n    annotations:\n      external-dns.alpha.kubernetes.io/hostname: axonops.mycompany.com\n    hosts:\n      - host: axonops.mycompany.com\n        path: \"/\"\n    tls:\n      - hosts:\n          - axonops.mycompany.com\n        secretName: axon-dash-tls\n  resources:\n    limits:\n      cpu: 1000m\n      memory: 1536Mi\n    requests:\n      cpu: 25m\n      memory: 256Mi\n\n# If you are using an existing ElasticSearch rather than installing it \n# as shown above then make sure you update the elasticHost URL below\naxon-server:\n  elasticHost: http://axonops-elastic-master:9200\n  dashboardUrl: https://axonops.mycompany.com\n  config:\n    # Set your organization name here. This must match the name used in your license key\n    org_name: demo\n    # Enter your AxonOps license key here\n    license_key: \"...\"\n  image:\n    pullPolicy: IfNotPresent\n    repository: registry.axonops.com/axonops-public/axonops-docker/axon-server\n    tag: latest\n  # Enable the agent ingress to allow agents to connect from outside the Kubernetes cluster\n  agentIngress:\n    enabled: true\n    className: nginx\n    annotations:\n      external-dns.alpha.kubernetes.io/hostname: axonops-server.mycompany.com\n    hosts:\n      - host: axonops-server.mycompany.com\n        path: \"/\"\n    tls:\n      - hosts:\n          - axonops-server.mycompany.com\n        secretName: axon-server-tls\n\n  resources:\n    limits:\n      cpu: 1\n      memory: 1Gi\n    requests:\n      cpu: 100m\n      memory: 256Mi\n</code></pre> <p>An example values file showing all available options can be found in the GitHub repository here: values-full.yaml</p>"},{"location":"installation/kubernetes/#installing","title":"Installing","text":""},{"location":"installation/kubernetes/#elasticsearch_1","title":"ElasticSearch","text":"<p>Now you can install Elasticsearch referencing the configuration file created in the previous step:</p> <pre><code>helm repo add elastic https://helm.elastic.co\nhelm update\nhelm upgrade -n axonops --install \\\n  --create-namespace \\\n  -f \"elasticsearch.yaml\" \\\n  elasticsearch elastic/elasticsearch\n</code></pre>"},{"location":"installation/kubernetes/#axonops_1","title":"AxonOps","text":"<p>Finally install the AxonOps helm chart:</p> <pre><code>helm upgrade -n axonops --install \\\n  --create-namespace \\\n  -f \"axonops.yaml\" \\\n  axonops oci://helm.axonops.com/axonops-public/axonops-helm/axonops\n</code></pre>"},{"location":"installation/kubernetes/minikube/","title":"Cassandra with AxonOps on Kubernetes","text":""},{"location":"installation/kubernetes/minikube/#introduction","title":"Introduction","text":"<p>The following shows how to install AxonOps for monitoring cassandra. This process specifically requires the official cassandra helm repository.</p>"},{"location":"installation/kubernetes/minikube/#using-minikube","title":"Using minikube","text":"<p>The deployment should work fine on latest versions of minikube as long as you provide enough memory for it.</p> <p><pre><code>minikube start --memory 8192 --cpus=4\nminikube addons enable storage-provisioner\n</code></pre> :warning: Make sure you use a recent version of minikube. Also check available drivers and select the most appropriate for your platform</p>"},{"location":"installation/kubernetes/minikube/#helmfile","title":"Helmfile","text":""},{"location":"installation/kubernetes/minikube/#overview","title":"Overview","text":"<p>As this deployment contains multiple applications we recommend you use an automation system such as Ansible or Helmfile to put together the config. The example below uses helmfile.</p>"},{"location":"installation/kubernetes/minikube/#install-requirements","title":"Install requirements","text":"<p>You would need to install the following components:</p> <ul> <li>helm: https://helm.sh/docs/intro/install/</li> <li>helmfile: https://github.com/roboll/helmfile/releases</li> </ul> <p>Alternatively you can consider using a dockerized version of them both such as https://hub.docker.com/r/chatwork/helmfile</p>"},{"location":"installation/kubernetes/minikube/#config-files","title":"Config files","text":"<p>The values below are set for running on a laptop with <code>minikube</code>, adjust accordingly for larger deployments.</p>"},{"location":"installation/kubernetes/minikube/#helmfileyaml","title":"helmfile.yaml","text":"<pre><code>---\nrepositories:\n  - name: axonops\n    url: helm.axonops.com/axonops-public/axonops-helm/axonops\n    oci: true\n  - name: bitnami\n    url: https://charts.bitnami.com/bitnami\n  - name: ckotzbauer\n    url: https://ckotzbauer.github.io/helm-charts\nreleases:\n  - name: axon-elastic\n    namespace: {{ env \"NAMESPACE\" | default \"axonops\" }}\n    chart: \"bitnami/elasticsearch\"\n    version: '12.8.1'\n    wait: true\n    values:\n      - fullnameOverride: axon-elastic\n      - imageTag: \"7.8.0\"\n      - data:\n          replicas: 1\n          persistence:\n            size: 1Gi\n            enabled: true\n            accessModes: [ \"ReadWriteOnce\" ]\n      - curator:\n          enabled: true\n      - coordinating:\n          replicas: 1\n      - master:\n          replicas: 1\n          persistence:\n            size: 1Gi\n            enabled: true\n            accessModes: [ \"ReadWriteOnce\" ]\n\n  - name: axonops\n    namespace: {{ env \"NAMESPACE\" | default \"axonops\" }}\n    chart: \"digitalis/axonops\"\n    wait: true\n    values:\n      - values.yaml\n\n  - name: cassandra\n    namespace: cassandra\n    chart: \"digitalis/cassandra\"\n    wait: true\n    values:\n      - values.yaml\n\n  - name: cadvisor\n    namespace: kube-system\n    chart: ckotzbauer/cadvisor\n    version: 1.2.0\n    values:\n      - container:\n          additionalArgs:\n            - --housekeeping_interval=5s                       # kubernetes default args\n            - --max_housekeeping_interval=10s\n            - --event_storage_event_limit=default=0\n            - --event_storage_age_limit=default=0\n            - --disable_metrics=percpu,process,sched,tcp,udp    # enable only diskIO, cpu, memory, network, disk\n            - --docker_only\n      - image:\n          repository: gcr.io/cadvisor/cadvisor\n          tag: v0.37.0\n</code></pre>"},{"location":"installation/kubernetes/minikube/#valuesyaml","title":"values.yaml","text":"<pre><code>---\npersistence:\n  enabled: true\n  size: 2Gi\n  accessMode: ReadWriteMany\n\npodSettings:\n  terminationGracePeriodSeconds: 300\n\nimage:\n  tag: 3.11.6\n  pullPolicy: IfNotPresent\n\nconfig:\n  cluster_name: digitalis\n  cluster_size: 2\n  dc_name: dc1\n  seed_size: 1\n  num_tokens: 256\n  max_heap_size: 512M\n  heap_new_size: 512M\n  endpoint_snitch: GossipingPropertyFileSnitch\n\nenv:\n  JVM_OPTS: \"-javaagent:/var/lib/axonops/axon-cassandra3.11-agent.jar=/etc/axonops/axon-agent.yml\"\n\nserviceAccount:\n  create: true\n  rules:\n  - apiGroups:\n    - \"\"\n    resources:\n    - nodes\n    - nodes/metrics\n    - pods\n    verbs:\n    - get\n    - list\n    - watch\n  - nonResourceURLs:\n    - /metrics\n    verbs:\n    - get\n\nextraVolumes:\n  - name: axonops-agent-config\n    configMap:\n      name: axonops-agent\n  - name: axonops-shared\n    emptyDir: {}\n  - name: axonops-logs\n    emptyDir: {}\n\nextraVolumeMounts:\n  - name: axonops-shared\n    mountPath: /var/lib/axonops\n    readOnly: false\n  - name: axonops-agent-config\n    mountPath: /etc/axonops\n    readOnly: true\n  - name: axonops-logs\n    mountPath: /var/log/axonops\n\nextraContainers:\n  - name: axonops-agent\n    image: digitalisdocker/axon-agent:latest\n    env:\n      - name: AXON_AGENT_VERBOSITY\n        value: \"1\"\n      - name: AXON_AGENT_ARGS\n        value: \"-v 1\"\n      - name: DATA_FILE_DIRECTORY\n        value: \"/var/lib/cassandra\"\n      - name: CASSANDRA_POD_NAME\n        valueFrom:\n          fieldRef:\n            fieldPath: metadata.name\n      - name: CASSANDRA_POD_NAMESPACE\n        valueFrom:\n          fieldRef:\n            fieldPath: metadata.namespace\n      - name: CASSANDRA_NODE_NAME\n        valueFrom:\n          fieldRef:\n            fieldPath: spec.nodeName\n      - name: CASSANDRA_POD_IP\n        valueFrom:\n          fieldRef:\n            apiVersion: v1\n            fieldPath: status.podIP\n    volumeMounts:\n      - name: axonops-agent-config\n        mountPath: /etc/axonops\n        readOnly: true\n      - name: axonops-shared\n        mountPath: /var/lib/axonops\n        readOnly: false\n      - name: axonops-logs\n        mountPath: /var/log/axonops\n      - name: data\n        mountPath: /var/lib/cassandra\n\n\naxon-server:\n  global:\n    customer: minikube\n    baseDomain: axonops.com\n\n  elasticHost: http://axon-elastic-elasticsearch-master.axonops:9200\n  dashboardUrl: https://axonops.axonops.com\n\n  image:\n    repository: digitalisdocker/axon-server\n    tag: latest\n    pullPolicy: IfNotPresent\n  config:\n    extraConfig:\n      cql_hosts:\n        - cassandra-0.cassandra.cassandra.svc.cluster.local\n      cql_username: \"cassandra\"\n      cql_password: \"cassandra\"\n      cql_local_dc: dc1\n      cql_proto_version: 4\n      cql_max_searchqueriesparallelism: 100\n      cql_batch_size: 100\n      cql_page_size: 100\n      cql_autocreate_tables: false\n      cql_retrypolicy_numretries: 3\n      cql_retrypolicy_min: 2s\n      cql_retrypolicy_max: 10s\n      cql_reconnectionpolicy_maxretries: 10\n      cql_reconnectionpolicy_initialinterval: 1s\n      cql_reconnectionpolicy_maxinterval: 10s\n      cql_keyspace_replication: \"{ 'class': 'NetworkTopologyStrategy', 'dc1': 1 }\"\n      cql_metrics_cache_max_size: 128  #MB\n      cql_metrics_cache_max_items : 500000\n\naxon-dash:\n  replicaCount: 1\n  config:\n    axonServerUrl: http://axonops-axon-server:8080\n  service:\n    type: NodePort\n  ingress:\n    enabled: true\n    annotations:\n      nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    hosts:\n      - hosts: axonops.axonops.com\n        paths:\n          - /\n  image:\n    repository: digitalisdocker/axon-dash\n    tag: latest\n    pullPolicy: IfNotPresent\n  autoscaling:\n    enabled: true\n  resources:\n    limits:\n      cpu: 500m\n      memory: 512Mi\n    requests:\n      cpu: 50m\n      memory: 128Mi\n</code></pre>"},{"location":"installation/kubernetes/minikube/#axon-agentyml","title":"axon-agent.yml","text":"<pre><code>axon-server:\n    hosts: \"axonops-axon-server.axonops\" # Specify axon-server IP axon-server.mycompany.\n    port: 1888\n\naxon-agent:\n    org: \"digitalis\"\n    human_readable_identifier: \"axon_agent_ip\" # one of the following:\n\nNTP:\n    host: \"pool.ntp.org\" # Specify a NTP to determine a NTP offset\n\ncassandra:\n  tier0: # metrics collected every 5 seconds\n      metrics:\n          jvm_:\n            - \"java.lang:*\"\n          cas_:\n            - \"org.apache.cassandra.metrics:*\"\n            - \"org.apache.cassandra.net:type=FailureDetector\"\n\n  tier1:\n      frequency: 300 # metrics collected every 300 seconds (5m)\n      metrics:\n          cas_:\n            - \"org.apache.cassandra.metrics:name=EstimatedPartitionCount,*\"\n\n  blacklist: # You can blacklist metrics based on Regex pattern. Hit the agent on http://agentIP:9916/metricslist to list JMX metrics it is collecting\n    - \"org.apache.cassandra.metrics:type=ColumnFamily.*\" # duplication of table metrics\n    - \"org.apache.cassandra.metrics:.*scope=Repair#.*\" # ignore each repair instance metrics\n    - \"org.apache.cassandra.metrics:.*name=SnapshotsSize.*\" # Collecting SnapshotsSize metrics slows down collection\n    - \"org.apache.cassandra.metrics:.*Max.*\"\n    - \"org.apache.cassandra.metrics:.*Min.*\"\n    - \".*999thPercentile|.*50thPercentile|.*FifteenMinuteRate|.*FiveMinuteRate|.*MeanRate|.*Mean|.*OneMinuteRate|.*StdDev\"\n\n  JMXOperationsBlacklist:\n    - \"getThreadInfo\"\n    - \"getDatacenter\"\n    - \"getRack\"\n\n  DMLEventsWhitelist: # You can whitelist keyspaces / tables (list of \"keyspace\" and/or \"keyspace.table\") to log DML queries. Data is not analysed.\n  # - \"system_distributed\"\n\n  DMLEventsBlacklist: # You can blacklist keyspaces / tables from the DMLEventsWhitelist (list of \"keyspace\" and/or \"keyspace.table\") to log DML queries. Data is not analysed.\n  # - system_distributed.parent_repair_history\n\n  logSuccessfulRepairs: false # set it to true if you want to log all the successful repair events.\n\n  warningThresholdMillis: 200 # This will warn in logs when a MBean takes longer than the specified value.\n\n  logFormat: \"%4$s %1$tY-%1$tm-%1$td %1$tH:%1$tM:%1$tS,%1$tL %5$s%6$s%n\"\n</code></pre>"},{"location":"installation/kubernetes/minikube/#start-up","title":"Start up","text":""},{"location":"installation/kubernetes/minikube/#create-axon-agent-configuration","title":"Create Axon Agent configuration","text":"<pre><code>kubectl create ns cassandra\nkubectl create configmap axonops-agent --from-file=axon-agent.yml -n cassandra\n</code></pre>"},{"location":"installation/kubernetes/minikube/#run-helmfile","title":"Run helmfile","text":""},{"location":"installation/kubernetes/minikube/#with-locally-installed-helm-and-helmfile","title":"With locally installed helm and helmfile","text":"<pre><code>cd your/config/directory\nhemlfile sync\n</code></pre>"},{"location":"installation/kubernetes/minikube/#with-docker-image","title":"With docker image","text":"<pre><code>docker run --rm \\\n    -v ~/.kube:/root/.kube \\\n    -v ${PWD}/.helm:/root/.helm \\\n    -v ${PWD}/helmfile.yaml:/helmfile.yaml \\\n    -v ${PWD}/values.yaml:/values.yaml \\\n    --net=host chatwork/helmfile sync\n</code></pre>"},{"location":"installation/kubernetes/minikube/#access","title":"Access","text":""},{"location":"installation/kubernetes/minikube/#minikube","title":"Minikube","text":"<p>If you used <code>minikube</code>, identify the name of the service with <code>kubectl get svc -n monitoring</code> and launch it with </p> <pre><code>minikube service axonops-axon-dash -n monitoring\n</code></pre>"},{"location":"installation/kubernetes/minikube/#loadbalancer","title":"LoadBalancer","text":"<p>Find the DNS entry for it:</p> <pre><code>kubectl get svc -n monitoring -o wide\n</code></pre> <p>Open your browser and copy and paste the URL.</p>"},{"location":"installation/kubernetes/minikube/#troubleshooting","title":"Troubleshooting","text":"<p>Check the status of the pods:</p> <pre><code>kubectl get pod -n monitoring\nkubectl get pod -n cassandra\n</code></pre> <p>Any pod which is not on state <code>Running</code> check it out with</p> <pre><code>kubectl describe -n NAMESPACE pod POD-NAME\n</code></pre>"},{"location":"installation/kubernetes/minikube/#storage","title":"Storage","text":"<p>One common problem is regarding storage. If you have enabled persistent storage you may see an error about persistent volume claims (not found, unclaimed, etc.). If you're using <code>minikube</code> make sure you enable storage with </p> <pre><code>minikube addons enable storage-provisioner\n</code></pre>"},{"location":"installation/kubernetes/minikube/#memory","title":"Memory","text":"<p>The second most common problem is not enough memory (OOMKilled). You will see this often if your node does not have enough memory to run the containers or if the <code>heap</code> settings for Cassandra are not right. <code>kubectl describe</code> command will be showing <code>Error 127</code> when this occurs.</p> <p>In the <code>values.yaml</code> file adjust the heap options to match your hardware:</p> <pre><code>  max_heap_size: 512M\n  heap_new_size: 512M\n</code></pre>"},{"location":"installation/kubernetes/minikube/#minikube_1","title":"Minikube","text":"<p>Review the way you have started up <code>minikube</code> and assign more memory if you can. Also check the available drivers and select the appropriate for your platform. On macOS where I tested <code>hyperkit</code> or <code>virtualbox</code> are the best ones.</p> <pre><code>minikube start --memory 10240 --cpus=4 --driver=hyperkit\n</code></pre>"},{"location":"installation/kubernetes/minikube/#putting-it-all-together","title":"Putting it all together","text":""},{"location":"integrations/email-integration/","title":"Setup SMTP notifications","text":"<p>On the Axonops application menu, select <code>Settings -&gt; Integrations</code> .</p> <p><code>Click</code> on the <code>SMTP</code> area.</p> <p>Infomy</p> <p></p> <p></p>"},{"location":"integrations/microsoft-teams-integration/","title":"Setup Microsoft Teams notifications","text":""},{"location":"integrations/microsoft-teams-integration/#create-microsoft-teams-webhooks","title":"Create Microsoft Teams Webhooks","text":"<ul> <li>On the Microsoft Teams interface, go to <code>Connectors</code> </li> </ul> <p> * Click on configure on the <code>Incoming Webhook</code> connector  </p> <p> * Provide a name and select <code>Create</code> </p> <p> * Copy the url provided to the clipboard  </p> <p> * On the Axonops application menu, select <code>Settings -&gt; Integrations</code> *  <code>Click</code> on the <code>Microsoft Teams</code> area.</p> <p></p> <ul> <li>Enter a <code>name</code> and copy the url in the <code>Webhook URL</code> field and select <code>Create</code> </li> </ul>"},{"location":"integrations/overview/","title":"Overview","text":"<p>AxonOps provide various integrations for the notifications.</p> <p>The functionality is accessible via Settings &gt; Integrations</p> <p>The current integrations are:</p> <ul> <li>SMTP</li> <li>Pagerduty</li> <li>Slack</li> <li>Microsoft Teams</li> <li>ServiceNow</li> <li>Generic webhooks</li> </ul> <p>Infomy</p> <p></p>"},{"location":"integrations/overview/#routing","title":"Routing","text":"<p>AxonOps provide a rich routing mechanism for the notifications.</p> <p>The current routing options are:</p> <ul> <li>Global - this will route all the notifications</li> <li>Metrics - notifications about the alerts on metrics</li> <li>Backups - notifications about the backups / restore</li> <li>Service Checks - notifications about the service checks / health checks</li> <li>Nodes - notifications raised from the nodes</li> <li>Commands - notifications from generic tasks</li> <li>Repairs - notifications from Cassandra repairs</li> <li>Rolling Restart - notification from the rolling restart feature</li> </ul> <p>Each severity (<code>info, warning, error</code>) can be routed independently </p> <p></p>"},{"location":"integrations/pagerduy-integration/","title":"Setup Pagerduty","text":""},{"location":"integrations/pagerduy-integration/#create-pagerduty-routing-key","title":"Create Pagerduty Routing Key","text":"<p>Using these steps. Please note down the pagerduty routing key</p>"},{"location":"integrations/pagerduy-integration/#insert-pagerduty-routing-key","title":"Insert Pagerduty Routing Key","text":"<p>On the Axonops application menu, select <code>Settings -&gt; Integrations</code> .</p> <p><code>Click</code> on the <code>Pagerduty</code> area.</p> <p>Infomy</p> <p></p> <p></p>"},{"location":"integrations/servicenow-integration/","title":"ServiceNow","text":"<p>Navigate to  Settings &gt; Integrations and click on ServiceNow</p> <p></p> <p>Once you have gathered your instance name, username and password from ServiceNow, you can validate the form: </p> <p>If you want to see the detailed description of a notification, you'll need to add the <code>description</code> field from ServiceNow incidents templates. </p> <p></p> <p></p>"},{"location":"integrations/slack-integration/","title":"Setup Slack","text":""},{"location":"integrations/slack-integration/#create-slack-incoming-webhooks","title":"Create Slack Incoming Webhooks","text":"<ul> <li>Go to Slack <code>Application</code></li> <li>On the side menu click  </li> <li>In search box type <code>Incoming Webhook</code>s</li> <li>From the App directory click <code>Install</code> on <code>Incoming WebHooks App</code>.</li> </ul> <p>Infomy</p> <p></p> <ul> <li><code>Click</code> Add Configuration</li> </ul> <p>Infomy</p> <p></p> <ul> <li> <p>In <code>Post to Channel</code> Box select an option from the <code>choose a channel</code> dropdown menu .</p> </li> <li> <p><code>Click</code> <code>Add Incoming WebHooks Integration</code></p> </li> </ul> <p>Infomy</p> <p></p> <ul> <li><code>Copy</code> and make a note of the <code>WebHook URL</code> that appears in the <code>Setup Instructions</code>.</li> </ul> <p>Infomy</p> <p></p>"},{"location":"integrations/slack-integration/#creating-the-slack-integration-on-axon-server","title":"Creating the Slack integration on axon-server","text":"<p>On the Axonops application menu, select <code>Settings -&gt; Integrations</code> .</p> <p><code>Click</code> on the <code>Slack</code> area.</p> <p>Infomy</p> <p></p> <p>Infomy</p> <p></p>"},{"location":"monitoring/overview/","title":"Monitoring Overview","text":"<p>When monitoring enterprise service there are 3 categories of how the service is performing that we generally capture and monitor. These are;</p> <ul> <li>Performance metrics</li> <li>Events (logs)</li> <li>Service availability</li> </ul>"},{"location":"monitoring/overview/#performance-metrics","title":"Performance Metrics","text":"<p>Performance metrics in Cassandra is highly extensive and there is a large number that can be captured to understand how Cassandra is performing. Another key metrics that also must be captured in order to effectively understand the performance of a database is the system resource utilisation.</p> <p>AxonOps agent captures both Cassandra and OS metrics and pushes them to the AxonOps server.</p>"},{"location":"monitoring/overview/#events","title":"Events","text":"<p>Cassandra event logs are, by default, written to log files. There are important information in the log files that allows SREs and DevOps engineers to identify issues when they occur. AxonOps agent captures the logs and pushes them to the AxonOps server. These logs are visible within AxonOps dashboard allowing quick access to them without having to log in to the individual servers.</p>"},{"location":"monitoring/overview/#service-availability","title":"Service Availability","text":"<p>Checking the momentary service availability and dashboards gives confidence that all services are running correctly as expected. Example service checks that allow engineers to gain confidence in the service availability are:</p> <ul> <li>System process</li> <li>Network open ports - e.g. CQL and storage ports</li> <li>Database availability - e.g. can execute CQL query</li> </ul>"},{"location":"monitoring/overview/#axonops-monitoring","title":"AxonOps Monitoring","text":"<p>AxonOps implements all three types of monitoring described above. AxonOps agent captures the information, sends them securely to AxonOps server, and the information is stored in the backend data store.</p> <p>AxonOps GUI provides comprehensive set of metrics dashboards combined with the event log view. It also provides separate service check status view showing the health of the cluster.</p> <p>This section describes how the AxonOps GUI organises the dashboards of all three types of monitoring.</p>"},{"location":"monitoring/logsandevents/logsandevents/","title":"Logs & Events","text":""},{"location":"monitoring/logsandevents/logsandevents/#logs-and-events","title":"Logs and Events","text":"<p>AxonOps provides a powerful logging feature that allows you to search and filter logs based on different parameters such as DC/Rack/Node, Log Level, Event Type, Source and Log Content.</p> <p></p> <p>The logs and events are visible within AxonOps dashboard and Logs &amp; Events tab allowing quick access to them without having to login to the individual servers.</p>"},{"location":"monitoring/logsandevents/logsandevents/#search-by-log-level","title":"Search by Log Level","text":"<p>Filter logs based on their log levels to focus on specific severity levels. The log level indicates the importance or severity of a message from the most critical (ERROR) to less severe (DEBUG).</p> <p></p>"},{"location":"monitoring/logsandevents/logsandevents/#setting-up-the-debug-level","title":"Setting up the Debug Level","text":"<p>To search logs by debug level you have to enable debug mode in cassandra by editing the logback.xml file: <pre><code>&lt;appender name=\"SYSTEMLOG\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt;\n    &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt;\n        &lt;level&gt;DEBUG&lt;/level&gt;\n</code></pre></p>"},{"location":"monitoring/logsandevents/logsandevents/#search-by-logs-source-and-event-type","title":"Search by Logs Source and Event Type","text":"<p>You can filter logs based on the log source (cassandra, axon-server and axon-agent logs) and event type to narrow down search results.</p> <p> </p>"},{"location":"monitoring/logsandevents/logsandevents/#search-by-content","title":"Search by Content","text":"<p>For a free text search enter a keyword in the content input or use the <code>/&lt;expression&gt;/</code> syntax to search by regex expression.</p> <p>Here are some examples:</p> <ul> <li>Display logs that contain a specific word or phrase:</li> </ul> <p> </p> <ul> <li>Display logs that contain a match either what is before or after the |, in this case \"Validated\" or \"Compacted\":</li> </ul> <p></p> <ul> <li>Display logs that contain both patterns in a line, in this case \"Segment\" and \"deleted:</li> </ul> <p></p>"},{"location":"monitoring/metricsdashboards/cassandra/","title":"Cassandra","text":"<p>AxonOps dashboards provides a comprehensive set of charts with an embedded view for logs and events. </p> <p>You can correlate metrics with logs/events as you can zoom in the logs histogram or metrics charts to drill down both results. </p> <p>Alert rules can be defined graphically in each chart and Log collection is  defined in the bottom part of that page.</p> <p>Infomy</p> <p></p>"},{"location":"monitoring/metricsdashboards/querysyntax/","title":"AxonOps Query Language Documentation","text":"<p>AxonOps uses a powerful query language for dashboarding performance metrics collected from the AxonOps agent. This language is largely based on the Prometheus query language, allowing users familiar with Prometheus to quickly adapt to AxonOps. For a comprehensive guide on the Prometheus query language, please refer to the Prometheus Query Language documentation</p>"},{"location":"monitoring/metricsdashboards/querysyntax/#key-difference-in-axonops-query-language","title":"Key Difference in AxonOps Query Language","text":"<p>While most of the AxonOps query language is identical to Prometheus, there is a notable difference in the handling of the <code>rate</code> function, specifically for metrics of the \"Count\" type. AxonOps has implemented an optimized method for generating rate graphs for these metrics at the source in the agent. This method ensures accurate rated metrics, as well as faster query time for the rated metrics when compared to dynamically calculating at query time.</p>"},{"location":"monitoring/metricsdashboards/querysyntax/#querying-rated-metrics","title":"Querying Rated Metrics","text":"<p>To query rated metrics in AxonOps, you need to use a specific syntax that includes embedding the <code>axonfunction='rate'</code> label within the query. This informs the AxonOps agent to generate the rated values for the Count metrics data at source. The following is an example of how to structure such a query:</p>"},{"location":"monitoring/metricsdashboards/querysyntax/#example-query","title":"Example Query","text":"<pre><code>cas_ClientRequest_Latency{axonfunction='rate',scope='Write_*$consistency',function='Count',dc=~'$dc',rack=~'$rack',host_id=~'$host_id'}\n</code></pre>"},{"location":"monitoring/metricsdashboards/querysyntax/#explanation-of-the-query","title":"Explanation of the Query","text":"<ul> <li><code>cas_ClientRequest_Latency</code>: The specific metric being queried.</li> <li><code>{axonfunction='rate', ...}</code>: The label set that includes <code>axonfunction='rate'</code>, which instructs the AxonOps agent to generate the rated values.</li> <li><code>axonfunction='rate'</code>: This label indicates that the agent should compute rate values.</li> <li><code>scope='Write_*$consistency'</code>: A scope pattern that matches the relevant metrics.</li> <li><code>function='Count'</code>: Specifies that the metric type is Count.</li> <li><code>dc=~'$dc'</code>, <code>rack=~'$rack'</code>, <code>host_id=~'$host_id'</code>: Additional labels that allow filtering by data center, rack, and host ID using regular expressions.</li> </ul>"},{"location":"monitoring/metricsdashboards/querysyntax/#parameters","title":"Parameters","text":"<ul> <li><code>axonfunction='rate'</code>: Tells the AxonOps agent to generate rated values for Count metrics.</li> <li><code>scope</code>: A pattern for filtering the scope of the query.</li> <li><code>function</code>: Specifies the metric type, which should be \"Count\" for rated metrics.</li> <li><code>dc</code>: Filters metrics by data center.</li> <li><code>rack</code>: Filters metrics by rack.</li> <li><code>host_id</code>: Filters metrics by host ID.</li> </ul>"},{"location":"monitoring/servicechecks/configurations/","title":"Configurations","text":""},{"location":"monitoring/servicechecks/configurations/#adding-service-checks","title":"Adding Service Checks","text":"<p>On the Axonops application menu, click <code>Service Checks</code> and select <code>Setup</code> tab.</p> <p></p>"},{"location":"monitoring/servicechecks/configurations/#creating-services","title":"Creating Services","text":"<pre><code>&gt; Note that for a Cassandra node, you can use the variable `{{.listen_address}}` which will correspond to Cassandra listening address.\n\nExample:\n\n!!! infomy\n\n\n    [![servicecheckseditor](/img/servicecheckseditor.png)](/img/servicecheckseditor.png)\n\n\n####  Delete Services\n\nTo Delete a service `copy`/`paste` into the editor and `click` save  [![save](/img/disk.png)](/img/disk.png)\n\n``` jsonld\n{\n    \"shellchecks\": [],\n    \"httpchecks\": [],\n    \"tcpchecks\": []\n\n}\n</code></pre> <p>Example:</p> <p>Infomy</p> <p></p>"},{"location":"monitoring/servicechecks/notifications/","title":"Notifications","text":"<p>Service checks will notify with one of the three statuses:</p> <p>Service Statuses.</p> <p>  Success</p> <p>  Warning  </p> <p>  Error</p> <p>Depending on the status of the service an appropriate alert will be sent. The <code>alert</code> will be sent based on the <code>Default Routing</code> that has been setup via the integrations menu.</p> <p>Noticed: If the <code>Default Routing</code> has not been set up <code>no alerts</code> will be sent.</p> <p>Service Alerts will be sent using the following rules.</p>"},{"location":"monitoring/servicechecks/notifications/#info","title":"Info","text":"<p>Default routing rules will be used to send  success  alerts</p>"},{"location":"monitoring/servicechecks/notifications/#warning","title":"Warning","text":"<p>Default routing rules will be used to send   warning  alerts</p>"},{"location":"monitoring/servicechecks/notifications/#error","title":"Error","text":"<p>Default routing rules will be used to send  error  alerts</p>"},{"location":"monitoring/servicechecks/overview/","title":"Service Checks","text":""},{"location":"monitoring/servicechecks/overview/#overview","title":"Overview","text":"<p>Service Checks in AxonOps allows you to configure custom checks using three types of checks:</p> <ol> <li>Shell Scripts</li> <li>HTTP endpoint checks</li> <li>TCP endpoint checks</li> </ol> <p>The functionality is accessible via the Service Checks menu</p> <p>You can list the service checks by node: </p> <p>Or by services: </p> <p>You can click on a row within the node view to see all the <code>services</code> for that given Node. </p> <p>The following shows a successful check:</p> <p></p> <p>And a failing check:</p> <p></p>"},{"location":"monitoring/servicechecks/overview/#configure-service-checks","title":"Configure service checks","text":"<p>To set up the checks, go to Settings &gt; Service Checks and click one of the <code>+</code> buttons</p> <p></p> <p>Any changes made and saved are automatically pushed down to the agents. There is no need to deploy the check scripts to individual servers like you may do for instance with Nagios. The status will show once the check has been executed on the agent, so it might take some time depending on the interval you have specified within the Service Checks. Although the first execution of the checks will be spread across 30 seconds to prevent running all the checks at the same time.</p>"},{"location":"monitoring/servicechecks/overview/#service-checks-templating","title":"Service checks templating","text":"<p>You can provide templated checks with the following pattern: <code>{{.variable_name}}</code></p> <p></p> <p><code>{{.comp_listen_address}}</code> will be replace with Cassandra listen address.</p> <p>For instance, port <code>7000</code> in the previous example for check storage port could be replaced with {{.comp_storage_port}} on a Cassandra cluster:</p> <p>endpoint: <code>{{.comp_listen_address}}:{{.comp_storage_port}}</code> </p>"},{"location":"monitoring/servicechecks/overview/#cassandra-variables","title":"Cassandra variables","text":"<p>Here is the full list of variables that can be specified in any service check: <pre><code>agent_version\ncomp_PROPERTY_PREFIX\ncomp_SENSITIVE_KEYS\ncomp_allocate_tokens_for_keyspace\ncomp_authenticator\ncomp_authorizer\ncomp_auto_bootstrap\ncomp_auto_snapshot\ncomp_back_pressure_enabled\ncomp_back_pressure_strategy\ncomp_batch_size_fail_threshold_in_kb\ncomp_batch_size_warn_threshold_in_kb\ncomp_batchlog_replay_throttle_in_kb\ncomp_broadcast_address\ncomp_broadcast_rpc_address\ncomp_buffer_pool_use_heap_if_exhausted\ncomp_cas_contention_timeout_in_ms\ncomp_cdc_enabled\ncomp_cdc_free_space_check_interval_ms\ncomp_cdc_raw_directory\ncomp_cdc_total_space_in_mb\ncomp_client_encryption_options\ncomp_cluster_name\ncomp_column_index_cache_size_in_kb\ncomp_column_index_size_in_kb\ncomp_commit_failure_policy\ncomp_commitlog_compression\ncomp_commitlog_directory\ncomp_commitlog_max_compression_buffers_in_pool\ncomp_commitlog_periodic_queue_size\ncomp_commitlog_segment_size_in_mb\ncomp_commitlog_sync\ncomp_commitlog_sync_batch_window_in_ms\ncomp_commitlog_sync_period_in_ms\ncomp_commitlog_total_space_in_mb\ncomp_compaction_large_partition_warning_threshold_mb\ncomp_compaction_throughput_mb_per_sec\ncomp_concurrent_compactors\ncomp_concurrent_counter_writes\ncomp_concurrent_materialized_view_writes\ncomp_concurrent_reads\ncomp_concurrent_replicates\ncomp_concurrent_writes\ncomp_counter_cache_keys_to_save\ncomp_counter_cache_save_period\ncomp_counter_cache_size_in_mb\ncomp_counter_write_request_timeout_in_ms\ncomp_credentials_cache_max_entries\ncomp_credentials_update_interval_in_ms\ncomp_credentials_validity_in_ms\ncomp_cross_node_timeout\ncomp_data_file_directories\ncomp_dc\ncomp_disk_access_mode\ncomp_disk_failure_policy\ncomp_disk_optimization_estimate_percentile\ncomp_disk_optimization_page_cross_chance\ncomp_disk_optimization_strategy\ncomp_dynamic_snitch\ncomp_dynamic_snitch_badness_threshold\ncomp_dynamic_snitch_reset_interval_in_ms\ncomp_dynamic_snitch_update_interval_in_ms\ncomp_enable_materialized_views\ncomp_enable_scripted_user_defined_functions\ncomp_enable_user_defined_functions\ncomp_enable_user_defined_functions_threads\ncomp_encryption_options\ncomp_endpoint_snitch\ncomp_file_cache_round_up\ncomp_file_cache_size_in_mb\ncomp_gc_log_threshold_in_ms\ncomp_gc_warn_threshold_in_ms\ncomp_hinted_handoff_disabled_datacenters\ncomp_hinted_handoff_enabled\ncomp_hinted_handoff_throttle_in_kb\ncomp_hints_compression\ncomp_hints_directory\ncomp_hints_flush_period_in_ms\ncomp_hostId\ncomp_incremental_backups\ncomp_index_interval\ncomp_index_summary_capacity_in_mb\ncomp_index_summary_resize_interval_in_minutes\ncomp_initial_token\ncomp_inter_dc_stream_throughput_outbound_megabits_per_sec\ncomp_inter_dc_tcp_nodelay\ncomp_internode_authenticator\ncomp_internode_compression\ncomp_internode_recv_buff_size_in_bytes\ncomp_internode_send_buff_size_in_bytes\ncomp_isClientMode\ncomp_jvm_VM name\ncomp_jvm_VM vendor\ncomp_jvm_VM version\ncomp_jvm_awt.toolkit\ncomp_jvm_boot classpath\ncomp_jvm_cassandra-foreground\ncomp_jvm_cassandra.config\ncomp_jvm_cassandra.jmx.local.port\ncomp_jvm_cassandra.native.epoll.enabled\ncomp_jvm_com.sun.management.jmxremote.ssl\ncomp_jvm_file.encoding\ncomp_jvm_file.encoding.pkg\ncomp_jvm_file.separator\ncomp_jvm_gc_G1 Old Generation_collection count\ncomp_jvm_gc_G1 Old Generation_collection time\ncomp_jvm_gc_G1 Old Generation_memory pool names\ncomp_jvm_gc_G1 Young Generation_collection count\ncomp_jvm_gc_G1 Young Generation_collection time\ncomp_jvm_gc_G1 Young Generation_memory pool names\ncomp_jvm_heap_heapFreeSize\ncomp_jvm_heap_heapMaxSize\ncomp_jvm_heap_heapSize\ncomp_jvm_input arguments\ncomp_jvm_io.netty.native.workdir\ncomp_jvm_java.awt.graphicsenv\ncomp_jvm_java.awt.printerjob\ncomp_jvm_java.class.path\ncomp_jvm_java.class.version\ncomp_jvm_java.endorsed.dirs\ncomp_jvm_java.ext.dirs\ncomp_jvm_java.home\ncomp_jvm_java.io.tmpdir\ncomp_jvm_java.library.path\ncomp_jvm_java.rmi.server.hostname\ncomp_jvm_java.rmi.server.randomIDs\ncomp_jvm_java.runtime.name\ncomp_jvm_java.runtime.version\ncomp_jvm_java.specification.name\ncomp_jvm_java.specification.vendor\ncomp_jvm_java.specification.version\ncomp_jvm_java.util.logging.SimpleFormatter.format\ncomp_jvm_java.vendor\ncomp_jvm_java.vendor.url\ncomp_jvm_java.vendor.url.bug\ncomp_jvm_java.version\ncomp_jvm_java.vm.info\ncomp_jvm_java.vm.name\ncomp_jvm_java.vm.specification.name\ncomp_jvm_java.vm.specification.vendor\ncomp_jvm_java.vm.specification.version\ncomp_jvm_java.vm.vendor\ncomp_jvm_java.vm.version\ncomp_jvm_jna.loaded\ncomp_jvm_jna.platform.library.path\ncomp_jvm_jnidispatch.path\ncomp_jvm_library classpath\ncomp_jvm_line.separator\ncomp_jvm_log4j.configuration\ncomp_jvm_management spec version\ncomp_jvm_name\ncomp_jvm_os.arch\ncomp_jvm_os.name\ncomp_jvm_os.version\ncomp_jvm_path.separator\ncomp_jvm_spec name\ncomp_jvm_spec vendor\ncomp_jvm_start time\ncomp_jvm_sun.arch.data.model\ncomp_jvm_sun.boot.class.path\ncomp_jvm_sun.boot.library.path\ncomp_jvm_sun.cpu.endian\ncomp_jvm_sun.cpu.isalist\ncomp_jvm_sun.io.unicode.encoding\ncomp_jvm_sun.java.command\ncomp_jvm_sun.java.launcher\ncomp_jvm_sun.jnu.encoding\ncomp_jvm_sun.management.compiler\ncomp_jvm_sun.nio.ch.bugLevel\ncomp_jvm_sun.os.patch.level\ncomp_jvm_up time\ncomp_jvm_user.country\ncomp_jvm_user.dir\ncomp_jvm_user.home\ncomp_jvm_user.language\ncomp_jvm_user.name\ncomp_jvm_user.timezone\ncomp_jvm_user.variant\ncomp_key_cache_keys_to_save\ncomp_key_cache_save_period\ncomp_key_cache_size_in_mb\ncomp_listen_address\ncomp_listen_interface\ncomp_listen_interface_prefer_ipv6\ncomp_listen_on_broadcast_address\ncomp_logger\ncomp_max_file_descriptors\ncomp_max_hint_window_in_ms\ncomp_max_hints_delivery_threads\ncomp_max_hints_file_size_in_mb\ncomp_max_mutation_size_in_kb\ncomp_max_streaming_retries\ncomp_max_value_size_in_mb\ncomp_memtable_allocation_type\ncomp_memtable_cleanup_threshold\ncomp_memtable_flush_writers\ncomp_memtable_heap_space_in_mb\ncomp_memtable_offheap_space_in_mb\ncomp_min_free_space_per_drive_in_mb\ncomp_mode\ncomp_native_transport_max_concurrent_connections\ncomp_native_transport_max_concurrent_connections_per_ip\ncomp_native_transport_max_frame_size_in_mb\ncomp_native_transport_max_threads\ncomp_native_transport_port\ncomp_native_transport_port_ssl\ncomp_num_tokens\ncomp_open_file_descriptors\ncomp_otc_backlog_expiration_interval_ms\ncomp_otc_backlog_expiration_interval_ms_default\ncomp_otc_coalescing_enough_coalesced_messages\ncomp_otc_coalescing_strategy\ncomp_otc_coalescing_window_us\ncomp_otc_coalescing_window_us_default\ncomp_ownership\ncomp_partitioner\ncomp_permissions_cache_max_entries\ncomp_permissions_update_interval_in_ms\ncomp_permissions_validity_in_ms\ncomp_phi_convict_threshold\ncomp_prepared_statements_cache_size_mb\ncomp_rack\ncomp_range_request_timeout_in_ms\ncomp_read_request_timeout_in_ms\ncomp_releaseVersion\ncomp_request_scheduler\ncomp_request_scheduler_id\ncomp_request_scheduler_options\ncomp_request_timeout_in_ms\ncomp_role_manager\ncomp_roles_cache_max_entries\ncomp_roles_update_interval_in_ms\ncomp_roles_validity_in_ms\ncomp_row_cache_class_name\ncomp_row_cache_keys_to_save\ncomp_row_cache_save_period\ncomp_row_cache_size_in_mb\ncomp_rpc_address\ncomp_rpc_interface\ncomp_rpc_interface_prefer_ipv6\ncomp_rpc_keepalive\ncomp_rpc_listen_backlog\ncomp_rpc_max_threads\ncomp_rpc_min_threads\ncomp_rpc_port\ncomp_rpc_recv_buff_size_in_bytes\ncomp_rpc_send_buff_size_in_bytes\ncomp_rpc_server_type\ncomp_saved_caches_directory\ncomp_schemaVersion\ncomp_seed_provider\ncomp_server_encryption_options\ncomp_slow_query_log_timeout_in_ms\ncomp_snapshot_before_compaction\ncomp_ssl_storage_port\ncomp_sstable_preemptive_open_interval_in_mb\ncomp_start_native_transport\ncomp_start_rpc\ncomp_storage_port\ncomp_stream_throughput_outbound_megabits_per_sec\ncomp_streaming_keep_alive_period_in_secs\ncomp_streaming_socket_timeout_in_ms\ncomp_thrift_framed_transport_size_in_mb\ncomp_thrift_max_message_length_in_mb\ncomp_thrift_prepared_statements_cache_size_mb\ncomp_tombstone_failure_threshold\ncomp_tombstone_warn_threshold\ncomp_tracetype_query_ttl\ncomp_tracetype_repair_ttl\ncomp_transparent_data_encryption_options\ncomp_trickle_fsync\ncomp_trickle_fsync_interval_in_kb\ncomp_truncate_request_timeout_in_ms\ncomp_unlogged_batch_across_partitions_warn_threshold\ncomp_user_defined_function_fail_timeout\ncomp_user_defined_function_warn_timeout\ncomp_user_function_timeout_policy\ncomp_windows_timer_interval\ncomp_write_request_timeout_in_ms\nhost_BootTime\nhost_Ctxt\nhost_HostID\nhost_Hostname\nhost_KernelArch\nhost_KernelVersion\nhost_OS\nhost_Platform\nhost_PlatformFamily\nhost_PlatformVersion\nhost_Procs\nhost_ProcsBlocked\nhost_ProcsRunning\nhost_ProcsTotal\nhost_Uptime\nhost_VirtualizationRole\nhost_VirtualizationSystem\nhost_cpu_CPU\nhost_cpu_CacheSize\nhost_cpu_CoreID\nhost_cpu_Cores\nhost_cpu_Family\nhost_cpu_Flags\nhost_cpu_Mhz\nhost_cpu_Microcode\nhost_cpu_Model\nhost_cpu_ModelName\nhost_cpu_PhysicalID\nhost_cpu_Stepping\nhost_cpu_VendorID\nhost_disk_/_Free\nhost_disk_/_Total\nhost_disk_/_Used\nhost_disk_/_fstype\nhost_swapmem_Free\nhost_swapmem_PgFault\nhost_swapmem_PgIn\nhost_swapmem_PgMajFault\nhost_swapmem_PgOut\nhost_swapmem_Sin\nhost_swapmem_Sout\nhost_swapmem_Total\nhost_swapmem_Used\nhost_swapmem_UsedPercent\nhost_virtualmem_Active\nhost_virtualmem_Available\nhost_virtualmem_Buffers\nhost_virtualmem_Cached\nhost_virtualmem_CommitLimit\nhost_virtualmem_CommittedAS\nhost_virtualmem_Dirty\nhost_virtualmem_Free\nhost_virtualmem_HighFree\nhost_virtualmem_HighTotal\nhost_virtualmem_HugePageSize\nhost_virtualmem_HugePagesFree\nhost_virtualmem_HugePagesTotal\nhost_virtualmem_Inactive\nhost_virtualmem_Laundry\nhost_virtualmem_LowFree\nhost_virtualmem_LowTotal\nhost_virtualmem_Mapped\nhost_virtualmem_PageTables\nhost_virtualmem_SReclaimable\nhost_virtualmem_SUnreclaim\nhost_virtualmem_Shared\nhost_virtualmem_Slab\nhost_virtualmem_SwapCached\nhost_virtualmem_SwapFree\nhost_virtualmem_SwapTotal\nhost_virtualmem_Total\nhost_virtualmem_Used\nhost_virtualmem_UsedPercent\nhost_virtualmem_VMallocChunk\nhost_virtualmem_VMallocTotal\nhost_virtualmem_VMallocUsed\nhost_virtualmem_Wired\nhost_virtualmem_Writeback\nhost_virtualmem_WritebackTmp\nhuman_readable_identifier\nhuman_readable_identifier_field\n</code></pre></p>"},{"location":"operations/cassandra/repair/","title":"Repair","text":"<p>Repairs must be completed regularly to maintain Cassandra nodes.</p> <p>AxonOps provide two mechanisms to ease Cassandra repairs:</p> <ul> <li> <p>Adaptive repair service</p> </li> <li> <p>Scheduled repair</p> </li> </ul>"},{"location":"operations/cassandra/repair/#adaptive-repair-service","title":"Adaptive repair service","text":"<p>Since AxonOps collects performance metrics and logs, we built an \u201cAdaptive\u201d repair system which regulates the velocity (parallelism and pauses between each subrange repair) based on performance trending data. The regulation of repair velocity takes input from various metrics including CPU utilisation, query latencies, Cassandra thread pools pending statistics, and IOwait percentage, while tracking the schedule of repair based on gc_grace_seconds for each table.</p> <p>The idea of this is to achieve the following:</p> <ul> <li>Completion of repair within gc_grace_seconds of each table.</li> <li>Repair process does not affect query performance.</li> <li>In essence, adaptive repair regulator slows down the repair velocity when it deems the load is going to be high based on the gradient of the rate of increase of load, and speeds up to catch up with the repair schedule when the resources are more readily available.</li> <li>This mechanism also doesn't require JMX access. The adaptive repair service running on AxonOps server orchestrates and issues commands to the agents over the existing connection.</li> </ul> <p>Infomy</p> <p></p> <p>If you want to keep the tables as fresh as possible we recommend to increase the <code>table parallelism</code> to be greater than the total number of tables of your cluster and reduce the <code>segments per VNode</code> to generate less repair requests.</p> <p>From a user\u2019s point of view there is only a single switch to enable this service. Keep this enabled and AxonOps will take care of the repair of all tables for you. You can also customize the following:</p> <ul> <li> <p>Blacklist some tables</p> </li> <li> <p>Specify the number of tables to repair in parallel</p> </li> <li> <p>Specify the number of segments per VNode to repair</p> </li> <li> <p>The GC grace threshold in seconds: if a table has a gc grace lesser than the specified value, it will be ignored from the adaptive repair service</p> </li> </ul>"},{"location":"operations/cassandra/repair/#scheduled-repair","title":"Scheduled repair","text":"<p>You can initiate three types of scheduled repair:</p> <ul> <li>Immediate scheduled repair: these will trigger immediately once</li> </ul> <p>Infomy</p> <p></p> <ul> <li>Simple scheduled repair: these will trigger base on the selected schedule repeatedly</li> </ul> <p>Infomy</p> <p></p> <ul> <li>Cron schedule repair: Same as simple scheduled repair but the schedule will be based on a Cron expression</li> </ul> <p>Infomy</p> <p></p> <p>The following capture presents a running repair that has been initiated immediately and a scheduled repair that is scheduled for 12:00 AM UTC:</p> <p>Infomy</p> <p></p>"},{"location":"operations/cassandra/backup/aws_s3/","title":"AWS S3","text":""},{"location":"operations/cassandra/backup/aws_s3/#what-is-s3","title":"What is S3","text":"<p>Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Read more at Amazon S3</p> <p>When selecting AWS S3 as a remote option there are a few fields that are mandatory the rest are optional.</p> <p>S3 allows any valid UTF-8 string as a key.</p> <p></p>"},{"location":"operations/cassandra/backup/aws_s3/#remote-backup-retention","title":"Remote Backup Retention","text":"<p>The amount of days that backup files will be kept in the remote storage provider location.  After this amount of days the file that are older will be removed.</p>"},{"location":"operations/cassandra/backup/aws_s3/#base-remote-path","title":"Base Remote Path","text":"<p>This is the name of the storage buckets, you can also add subfolders if using shared storage buckets or saving multiple clusters to the same bucket. by default AxonOps will save the backups to /bucket/folder/org/clustertype/clustername/host-id/</p> <p>The org/clustertype/clustername/host-id/ will match the top breadcrump navigation in your AxonOps Dashboard.</p>"},{"location":"operations/cassandra/backup/aws_s3/#region","title":"Region","text":"<p>This is a drop down selection of all the AWS regions that are available for your AWS account.</p>"},{"location":"operations/cassandra/backup/aws_s3/#access-key-id-and-secret-access-key","title":"Access Key ID and Secret Access Key","text":"<p>This is the standard AWS Access and Secret key that are associated with a IAM user.  This AxonOps IAM user that has the key assigned ideally would have the following permissions for accessing the S3 buckets.</p> <p>Please change the following 3 values</p> <ul> <li>BUCKETNAME : for some bucket naming rules please have a look here</li> <li>ACCOUNT_ID : for info on the AWS Account ID please have a look here</li> <li>AxonOpsS3User : the placeholder is an example name , it can be changed to any value for the the IAM User Name field</li> </ul> <pre><code>{\n  \"Id\": \"AxonOpsBackupBucketPolicy\",\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"BackupsSID\",\n      \"Action\": [\n        \"s3:ListBucket\",\n        \"s3:GetBucketLocation\",\n        \"s3:DeleteObject\",\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:PutObjectAcl\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"arn:aws:s3:::&lt;BUCKETNAME&gt;\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::&lt;ACCOUNT_ID&gt;:&lt;AxonOpsS3User&gt;\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"operations/cassandra/backup/aws_s3/#storage-class","title":"Storage Class","text":"<p>Amazon S3 offers a range of storage classes that you can choose from based on the performance, data access, resiliency, and cost requirements of your workloads. S3 storage classes are purpose-built to provide the lowest cost storage for different access patterns.</p> <p>Please pick from the available AWS Storage classes:</p> <ul> <li>Glacier</li> <li>Glacier Deep Archive</li> <li>Intelligent-Tiering</li> <li>One Zone Infrequent Access</li> <li>Reduced Redundancy</li> <li>Standard</li> <li>Stadard Infrequent Access</li> </ul> <p>For more inforamtion on S3 Storage classes please go here</p>"},{"location":"operations/cassandra/backup/aws_s3/#acl","title":"ACL","text":"<p>Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. It defines which AWS accounts or groups are granted access and the type of access. When a request is received against a resource, Amazon S3 checks the corresponding ACL to verify that the requester has the necessary access permissions.</p> <p>Please pick from the available AWS ACL's:</p> <ul> <li>Private</li> <li>Public Read</li> <li>Public Read-Write</li> <li>Authenticated Read</li> <li>Bucket Owner Read</li> <li>Bucket Owner Full Control</li> </ul> <p>For more inforamtion on S3 ACL's please go here</p>"},{"location":"operations/cassandra/backup/aws_s3/#server-side-encryption","title":"Server Side Encryption","text":"<p>The server-side encryption algorithm used when storing this object in S3.</p>"},{"location":"operations/cassandra/backup/aws_s3/#disable-checksum","title":"Disable Checksum","text":"<p>Normally AxonOps Backups will check that the checksums of transferred files match, and give an error \"corrupted on transfer\" if they don't. If you disable this then the checksum will be ignored if there are differences. This is not advised. </p>"},{"location":"operations/cassandra/backup/overview/","title":"Overview","text":"<p>AxonOps provides scheduled backup funtionality for your Cassandra data to local and remote storage options.</p> <p>The Backup feature is accessible via Operations &gt; Backups </p> <p>Infomy</p> <p></p>"},{"location":"operations/cassandra/backup/overview/#scheduled-backup","title":"Scheduled backup","text":"<p>You can initiate three types of scheduled backup:</p> <ul> <li> <p>Immediate scheduled backup: these will trigger immediately once</p> </li> <li> <p>Simple scheduled backup: these will trigger based on the selected schedule repeatedly</p> </li> </ul> <p>Infomy</p> <p></p> <ul> <li>Cron schedule backup: Same as simple scheduled backup but the schedule will be based on a Cron expression</li> </ul> <p>Infomy</p> <p></p> <p>The following capture presents two backups, a local only and a local and remote backup:</p> <p>Infomy</p> <p></p> <p>And the details of the local and remote backup:</p> <p>Infomy</p> <p></p>"},{"location":"operations/cassandra/backup/remote/","title":"Overview","text":""},{"location":"operations/cassandra/backup/remote/#remote-backups","title":"Remote backups","text":"<p>Note that axonops user will need read access on Cassandra data folders to be able to perform a remote backup.</p> <p>When selecting Remote backups there is some basic config presets.</p> <p></p>"},{"location":"operations/cassandra/backup/remote/#transfers-file-transfer-parallelism","title":"Transfers (File Transfer Parallelism)","text":"<p>The number of file transfers to run in parallel. It can be beneficial to set this to a smaller number if the remote is giving timeouts. You can set this to a bigger number or leave it at 0  if you have lots of bandwidth and a fast remote.</p>"},{"location":"operations/cassandra/backup/remote/#tps-limit","title":"TPS Limit","text":"<p>If you are getting errors from the cloud storage provider then you need to start adjusting this limit.</p> <p>The default is 0</p> <p>Storage provider errors consist of getting you banned or imposing rate limits.</p> <p>A transaction is any of but not limited to, PUT/GET/POST calls to the Storage backend. </p> <p>Different Storage providers have different limits. </p> <ul> <li>Amazon S3 has a limit of 5,500 GET requests per second per partitioned prefix.More here</li> <li>Google Cloud Storage has an approximate limit of 1,000 READ and 5,000 WRITE requests per second.More here</li> <li>Azure Blob Storage has a limit of 20,000 requests per second per storage account. More here</li> </ul>"},{"location":"operations/cassandra/backup/remote/#bandwidth-limit","title":"Bandwidth Limit","text":"<p>This will allow you to set a limit on how much data you want to transfer to your Storage provider during backups.  Note that the units are Byte/s, not bit/s. Typically connections are measured in bit/s - to convert divide by 8. For example, let's say you have a 10 Mbit/s connection and you wish AxonOps to use half of it - 5 Mbit/s.  You will calculate the limit value as 5MB/8 = 0.625 MiB/s. The value you would set it 0.625</p> <p>In most modern storage systems the value is not normally this low but if you have a VPN Gateway connection setup between an on-premise cluster and a storage provider that has a 100MB connection you could potentially limit how much of the pipe gets used by backups. </p>"},{"location":"operations/cassandra/backup/remote/#the-available-remote-options-are","title":"The available remote options are:","text":"<ul> <li>AWS S3</li> <li>Google Cloud Storage</li> <li>local filesystem</li> <li>Microsoft Azure Blob Storage</li> <li>S3 Compatible</li> <li>SFTP/SSH</li> </ul>"},{"location":"operations/cassandra/restore/overview/","title":"Overview","text":"<p>AxonOps provides the ability to restore from local snapshots and remote backups.</p> <p>The Restore feature is accessible via Operations &gt; Restore</p> <p>Infomy</p> <p></p> <p>Note that axonops user will need temporary write access on Cassandra data folders to be able to perform the restoration.</p> <p>To restore Cassandra, click on the backup you wish to restore.</p> <p>This will provide the details of that backup and the ability to start the restoration by clicking the <code>LOCAL RESTORE</code> or <code>REMOTE RESTORE</code>  button depending on if you prefer to restore from the local snapshot or the remote backup (if remote backups were configured). Here you can also select a subset of nodes to restore via the checkboxes in the Nodes list.</p> <p>Infomy</p> <p></p> <p>Follow the links below for some more detailed backup restore scenarios</p> <p>Restore a single node - same IP address</p> <p>Replace a node - different IP address</p> <p>Restore whole cluster - same IP addresses</p>"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/","title":"Restore a whole cluster from a remote backup with different IP addresses","text":"<p>Follow this procedure when you have lost all nodes in a cluster and they have been recreated in the same topology (Cluster, DC and rack names are all the same and the same number of nodes in each) but the replacement nodes have different IP addresses from the original cluster.</p> <p>NOTE: This process is for disaster recovery and cannot be used to clone a backup to a different cluster</p>"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/#prepare-the-cluster-for-restoring-a-backup","title":"Prepare the cluster for restoring a backup","text":"<p>Before you start, ensure that Cassandra is stopped on all replacement nodes and that their data directories are empty <pre><code>sudo systemctl stop cassandra\nsudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/*\n</code></pre></p> <p>Allow the AxonOps user to write to the Cassandra data directory on all nodes <pre><code>sudo chmod -R g+w /var/lib/cassandra/data\n</code></pre></p> <p>The commands above assume you are storing the Cassandra data in the default location <code>/var/lib/cassandra</code>, you will need to change the paths shown if your data is stored at a different location</p> <p>As the IP addresses of the replacement Cassandra nodes are different to the old cluster you will need to update the seeds list in <code>cassandra.yaml</code> to point to the IPs of the nodes that are replacing the old seeds. For package-based  installations (RPM or DEB) you can find this in <code>/etc/cassandra/cassandra.yaml</code> or for tarball installations it should be in <code>&lt;install_path&gt;/conf/cassandra.yaml</code>.</p>"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/#manually-configure-the-axonops-agent-host-ids","title":"Manually configure the AxonOps Agent host IDs","text":"<p>AxonOps identifies nodes by a unique host ID which is assigned when the agent starts up. In order to restore a backup to a node with a different IP address you must manually assign the AxonOps host ID of the old node to its new replacement.</p> <p>In order to restore the whole cluster from a backup you will need to apply the old AxonOps host ID to all nodes in the replacement cluster.</p> <p>The host ID of the old node can be found on the Cluster Overview page of the AxonOps dashboard</p> <p>Infomy</p> <p></p> <p>If you still have access to the old server or its data then its host ID can also be found in the file <code>/var/lib/axonops/hostId</code></p>"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/#apply-the-old-nodes-host-id-to-its-replacement","title":"Apply the old node's host ID to its replacement","text":"<p>Ensure the AxonOps Agent is stopped and clear any data that it may have created on startup</p> <pre><code>sudo systemctl stop axon-agent\nsudo rm -rf /var/lib/axonops/*\n</code></pre> <p>Manually apply the old node's host ID on the replacement (replace the host ID shown with your host ID from the previous steps)</p> <pre><code>echo '24d0cbf9-3b5a-11ed-8433-16b3c6a7bcc5' | sudo tee /var/lib/axonops/hostId\nsudo chown axonops.axonops /var/lib/axonops/hostId\n</code></pre> <p>Start the AxonOps agent</p> <pre><code>sudo systemctl start axon-agent\n</code></pre> <p>In the AxonOps dashboard you should see the replacement nodes start up and take over from the old nodes after a few minutes.</p>"},{"location":"operations/cassandra/restore/restore-cluster-different-ips/#restore-the-backup","title":"Restore the backup","text":"<p>Open the Restore page in the AxonOps Dashboard by going to Operations &gt; Restore</p> <p>Infomy</p> <p></p> <p>Choose the backup you wish to restore from the list and click the <code>RESTORE</code> button</p> <p>This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list.</p> <p>Infomy</p> <p></p> <p>Select all nodes in the checkbox list then start the restore by clicking the <code>REMOTE RESTORE</code> button.</p> <p>The restore progress will be displayed in the Backup Restorations in Progress list</p> <p>Infomy</p> <p></p> <p>After the restore operation has completed successfully, fix the ownership and permissions on the Cassandra data directories on all nodes in the cluster <pre><code>sudo chown -R cassandra.cassandra /var/lib/cassandra/data\nsudo chmod -R g-w /var/lib/cassandra/data\n</code></pre></p> <p>Start cassandra on the restored nodes one at a time, starting with the seeds first <pre><code>sudo systemctl start cassandra\n</code></pre></p> <p>After the whole cluster is started up you should be able to see the replaced nodes with their new IP addresses in the output of <code>nodetool status</code>. You may still see the IP addresses of the old cluster nodes in the output of <code>nodetool gossipinfo</code>, these should clear out automatically after a few days or they can be manually tidied up by performing a rolling restart of the cluster.</p>"},{"location":"operations/cassandra/restore/restore-cluster-same-ip/","title":"Restore a whole cluster from a remote backup","text":"<p>Follow this procedure when you have lost all nodes in a cluster but they have been recreated in the same cluster  topology (Cluster, DC and rack names are all the same and the same number of nodes in each) and the replacement nodes have the same IP addresses as the original cluster.</p> <p>Ensure that Cassandra is stopped on all nodes and that its data directories are empty</p> <pre><code>sudo systemctl stop cassandra\nsudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/*\n</code></pre> <p>Allow the AxonOps user to write to the Cassandra data directory <pre><code>sudo chmod -R g+w /var/lib/cassandra/data\n</code></pre></p> <p>These commands assume you are storing the Cassandra data in the default location <code>/var/lib/cassandra/</code>, you will need to change the paths shown if your data is stored at a different location</p> <p>Start the AxonOps agent on all nodes <pre><code>sudo systemctl start axon-agent\n</code></pre></p> <p>Now open the Restore page in the AxonOps Dashboard by going to Operations &gt; Restore</p> <p>Infomy</p> <p></p> <p>Choose the backup you wish to restore from the list and click the <code>RESTORE</code> button</p> <p>This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list.</p> <p>Infomy</p> <p></p> <p>Select all nodes in the checkbox list then start the restore by clicking the <code>REMOTE RESTORE</code> button.</p> <p>The restore progress will be displayed in the Backup Restorations in Progress list</p> <p>Infomy</p> <p></p> <p>After the restore operation has completed successfully, fix the ownership and permissions on the Cassandra data  directories on all nodes in the cluster <pre><code>sudo chown -R cassandra.cassandra /var/lib/cassandra/data\nsudo chmod -R g-w /var/lib/cassandra/data\n</code></pre></p> <p>Start cassandra on the restored nodes, starting with the seeds first <pre><code>sudo systemctl start cassandra\n</code></pre></p>"},{"location":"operations/cassandra/restore/restore-different-cluster/","title":"Restore a backup to a different cluster","text":"<p>Follow this procedure to restore an AxonOps backup from remote storage onto a different cluster</p> <p>NOTE: This facility is only available for backups created using AxonOps Agent version 1.0.60 or later</p> <p>AxonOps Agent version 1.0.60 and later includes a command-line tool which can be used to restore a backup created by AxonOps from remote storage (e.g. S3, GCS). This tool connects directly to your remote storage and does not require an AxonOps server or an active AxonOps Cloud account in order to function.</p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#installing-the-cassandra-restore-tool","title":"Installing the Cassandra Restore Tool","text":"<p>The AxonOps Cassandra Restore tool is included in the AxonOps Agent package.</p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#installing-on-debian-ubuntu","title":"Installing on Debian / Ubuntu","text":"<pre><code>sudo apt-get update\nsudo apt-get install -y curl gnupg ca-certificates\ncurl -L https://packages.axonops.com/apt/repo-signing-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/axonops.gpg\necho \"deb [arch=arm64,amd64 signed-by=/usr/share/keyrings/axonops.gpg] https://packages.axonops.com/apt axonops-apt main\" | sudo tee /etc/apt/sources.list.d/axonops-apt.list\nsudo apt-get update\nsudo apt-get install axon-agent\n</code></pre>"},{"location":"operations/cassandra/restore/restore-different-cluster/#installing-on-centos-redhat","title":"Installing on CentOS / RedHat","text":"<pre><code>sudo tee /etc/yum.repos.d/axonops-yum.repo &lt;&lt; EOL\n[axonops-yum]\nname=axonops-yum\nbaseurl=https://packages.axonops.com/yum/\nenabled=1\nrepo_gpgcheck=0\ngpgcheck=0\nEOL\nsudo yum install axon-agent\n</code></pre> <p>After the package has been installed you can find the Cassandra Restore Tool at <code>/usr/share/axonops/axon-cassandra-restore</code>.</p> <p>Run the tool with <code>--help</code> to see the available options: <pre><code>~# /usr/share/axonops/axon-cassandra-restore --help\nUsage of /usr/share/axonops/axon-cassandra-restore:\n  -i, --backup-id string                UUID of the backup to restore\n      --cassandra-bin-dir string        Where the Cassandra binary files are stored (e.g. /opt/cassandra/bin)\n      --cqlsh-options string            Options to pass to cqlsh when restoring a table schema\n      --dest-table string               The name of the destination table for the restore in keyspace.table format. Requires --tables with a single source table. (Added in v1.0.61)\n  -h, --help                            Show command-line help\n  -l, --list                            List backups available in remote storage\n  -d, --local-sstable-dir string        A local directory in which to store sstables downloaded from backup storage\n      --org-id string                   ID of the AxonOps organisation from which the backup was created\n  -r, --restore                         Restore a backup from remote storage\n      --restore-schema                  Set this when using --use-sstable-loader to restore the CQL schema for each table. Keyspaces must already exist.\n  -e, --skip-existing-files             Don't download files that already exist in the local destination path (Added in v1.0.61)\n  -c, --source-cluster string           The name of the cluster from which to restore\n  -s, --source-hosts string             Comma-separated list containing host IDs for which to restore backups\n      --sstable-loader-options string   Options to pass to sstableloader when restoring a backup\n      --storage-config string           JSON-formatted remote storage configuration\n  -t, --tables string                   Comma-separated list of keyspace.table to restore. Defaults to all tables if omitted.\n      --use-sstable-loader              Use sstableloader to restore the backup. Requires --sstable-loader-options and --cassandra-bin-dir.\n  -v, --verbose                         Show verbose output when listing backups\n      --version                         Show version information and exit\n</code></pre></p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#listing-the-available-backups","title":"Listing the available backups","text":"<p>NOTE: The host IDs used in this tool are the ID given to each host by AxonOps and do not relate to the Cassandra host ID. You can find the AxonOps host ID by selecting the node on the Cluster Overview page of the AxonOps dashboard and looking at the Agent ID field.</p> <p>To list the backups available in the remote storage bucket you can run the tool with the <code>--list</code> option. For example to list the backups in an Amazon S3 bucket you could use a command similar to this:</p> <p><pre><code>/usr/share/axonops/axon-cassandra-restore --list \\\n  --org-id myaxonopsorg \\\n  --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}'\n</code></pre> The restore tool will then scan the specified S3 bucket for AxonOps backups and it will display the date and backup ID for any backups it finds: <pre><code>Org ID:    myaxonopsorg\nCluster:   testcluster\nTime                   Backup ID\n2023-09-14 14:30 UTC   c67cea2a-5310-11ee-b686-bed50b9335ec\n2023-09-15 14:31 UTC   2c1d9aca-5312-11ee-b686-bed50b9335ec\n2023-09-16 14:30 UTC   91be5007-5313-11ee-b686-bed50b9335ec\n2023-09-17 14:31 UTC   f75f13d9-5314-11ee-b686-bed50b9335ec\n2023-09-18 14:30 UTC   5cffc1e6-5316-11ee-b686-bed50b9335ec\n</code></pre> If you pass the <code>--verbose</code> option when listing backups it will show the list of nodes and tables in each backup, for example: <pre><code>Org ID:    myaxonopsorg\nCluster:   testcluster\nTime:      2023-09-14 14:30 UTC\nBackup ID: c67cea2a-5310-11ee-b686-bed50b9335ec\n\nHost:   026346a0-dc89-4235-ae34-552fcd453b42\nTables: system.prepared_statements, system.transferred_ranges_v2, system_distributed.repair_history, system_schema.types, system_traces.sessions, system.compaction_history, system.available_ranges_v2, system.batches, system.size_estimates, system_schema.aggregates, system.IndexInfo, system_auth.resource_role_permissons_index, system_schema.views, test.test, system.paxos, system.local, system.peers, system.peers_v2, system.table_estimates, system_auth.network_permissions, system_auth.roles, system_distributed.view_build_status, system.built_views, system_schema.triggers, system.peer_events, system.repairs, keyspace1.table1, system.sstable_activity, keyspace1.table2, system.transferred_ranges, system_auth.role_permissions, system_distributed.parent_repair_history, system.available_ranges, system_schema.dropped_columns, system_schema.columns, system_schema.keyspaces, system.view_builds_in_progress, system_auth.role_members, system_schema.functions, system_schema.indexes, system_schema.tables, system_traces.events, system.peer_events_v2\n\nHost:   84759df0-8a19-497e-965f-200bdb4c1c9b\nTables: system_traces.events, system.available_ranges_v2, system.peer_events, system_auth.resource_role_permissons_index, system_auth.role_members, system_schema.types, system.IndexInfo, system.sstable_activity, system_distributed.view_build_status, system_schema.indexes, system.batches, system.transferred_ranges, system_schema.keyspaces, system_schema.tables, system_traces.sessions, system_distributed.repair_history, system_schema.aggregates, system.available_ranges, system.compaction_history, system.paxos, system.peers_v2, system.view_builds_in_progress, system.size_estimates, keyspace1.table1, system_auth.roles, system_schema.dropped_columns, test.test, system_auth.role_permissions, system_distributed.parent_repair_history, system.local, system.peer_events_v2, system.repairs, system.table_estimates, system_auth.network_permissions, system.peers, system_schema.triggers, system_schema.views, system.built_views, system.prepared_statements, system.transferred_ranges_v2, system_schema.columns, system_schema.functions, keyspace1.table2\n\nHost:   94ed3811-12ce-487f-ac49-ae31299efa31\nTables: system.peers_v2, system.view_builds_in_progress, system_auth.resource_role_permissons_index, system_auth.role_permissions, system_schema.aggregates, system_schema.indexes, test.test, system.available_ranges_v2, system_distributed.parent_repair_history, system_schema.keyspaces, system_traces.sessions, system_auth.role_members, system_auth.network_permissions, system_schema.dropped_columns, system_schema.types, system.repairs, system.size_estimates, system_auth.roles, system_schema.tables, system_schema.views, system.paxos, system.table_estimates, system.transferred_ranges, system.peers, system.prepared_statements, system.sstable_activity, system.peer_events_v2, system.batches, system.built_views, system.compaction_history, system_traces.events, system.IndexInfo, system.local, keyspace1.table2, system.peer_events, system.transferred_ranges_v2, system_distributed.repair_history, system_distributed.view_build_status, system_schema.columns, system_schema.functions, system.available_ranges, system_schema.triggers, keyspace1.table1\n</code></pre></p> <p>Scanning for backups can take a long time depending on the storage type and the amount of data, so you can use command-line options to restrict the search. For example this will restrict the search to a specific backup,  cluster, hosts and tables: <pre><code>/usr/share/axonops/axon-cassandra-restore --list \\\n  --verbose \\\n  --org-id myaxonopsorg \\\n  --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\\n  --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\\n  --source-cluster testcluster \\\n  --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b\n  --tables keyspace1.table1,keyspace1.table2\n</code></pre></p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#restoring-a-backup","title":"Restoring a Backup","text":"<p>The <code>axon-cassandra-restore</code> tool can perform the following operations to restore a backup from remote storage: 1. Download the sstable files from the bucket 2. Create table schemas in the target cluster 3. Import the downloaded sstable files into the target cluster using <code>sstableloader</code></p> <p>The default behaviour is to only download the sstable files to a local directory.</p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#downloading-a-backup-to-a-local-directory","title":"Downloading a backup to a local directory","text":"<p>This command will download the backup with ID <code>2c1d9aca-5312-11ee-b686-bed50b9335ec</code> for the 3 hosts listed in the <code>--list</code> output above into the local directory <code>/opt/cassandra/axonops-restore</code> <pre><code>/usr/share/axonops/axon-cassandra-restore \\\n  --restore \\\n  --org-id myaxonopsorg \\\n  --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\\n  --source-cluster testcluster \\\n  --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\\n  --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\\n  --local-sstable-dir /opt/cassandra/axonops-restore\n</code></pre> The sstable files will be restored into directories named <code>{local-sstable-dir}/{host-id}/keyspace/table/</code> and from here you can copy/move the files to another location or import them into a cluster using <code>sstableloader</code>.</p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#download-and-import-a-backup-in-a-single-operation","title":"Download and import a backup in a single operation","text":"<p>The above example shows how to download the backed up files into a local directory but it does not import them into a new cluster. You can make the <code>axon-cassandra-restore</code> tool do this for you after it downloads the files by passing the <code>--use-sstable-loader</code>, <code>--cassandra-bin-dir</code> and <code>--sstable-loader-options</code> command-line arguments.</p> <p>For example this command will download the same backup files as the previous example but it will also run <code>sstableloader</code> to import the downloaded files into a new cluster with contact points 10.0.0.1, 10.0.0.2 and 10.0.0.3: <pre><code>/usr/share/axonops/axon-cassandra-restore \\\n  --restore \\\n  --org-id myaxonopsorg \\\n  --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\\n  --source-cluster testcluster \\\n  --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\\n  --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\\n  --local-sstable-dir /opt/cassandra/axonops-restore \\\n  --use-sstable-loader \\\n  --cassandra-bin-dir /opt/cassandra/bin \\\n  --sstable-loader-options \"-d 10.0.0.1,10.0.0.2,10.0.0.3 -u cassandra -pw cassandra\"\n</code></pre></p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#importing-cql-schemas-during-the-restore","title":"Importing CQL schemas during the restore","text":"<p>When a backup is imported to a cluster using <code>sstableloader</code> it assumes that the destination tables already exist and will skip the import for any that are missing. AxonOps stores the current table schema with each backup, so it is possible to create any missing tables as part of the restore operation. This can be enabled with the <code>--restore-schema</code> and <code>--cqlsh-options</code> arguments to <code>axon-cassandra-restore</code>.</p> <p>Building on the example above this command will download the files from the backup, create the schema for any missing tables, and import the downloaded data with <code>sstableloader</code>: <pre><code>/usr/share/axonops/axon-cassandra-restore \\\n  --restore \\\n  --org-id myaxonopsorg \\\n  --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\\n  --source-cluster testcluster \\\n  --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\\n  --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\\n  --local-sstable-dir /opt/cassandra/axonops-restore \\\n  --use-sstable-loader \\\n  --cassandra-bin-dir /opt/cassandra/bin \\\n  --sstable-loader-options \"-d 10.0.0.1,10.0.0.2,10.0.0.3 -u cassandra -pw cassandra\" \\\n  --restore-schema \\\n  --cqlsh-options \"-u cassandra -p cassandra 10.0.0.1\"\n</code></pre></p> <p>NOTE: This will not create missing keyspaces. You must ensure that the target keyspaces already exist in the destination cluster before running the restore command.</p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#storage-config-examples","title":"Storage Config Examples","text":"<p>The AxonOps Cassandra restore tool can restore backups from any remote storage supported by AxonOps for backups. The <code>--storage-config</code> command-line option configures the type of remote storage and the credentials required for access.</p> <p>Here are some examples of the most common storage types:</p>"},{"location":"operations/cassandra/restore/restore-different-cluster/#local-filesystem","title":"Local filesystem","text":"<pre><code>--storage-config '{\"type\":\"local\",\"path\":\"/backups/cassandra\"}'\n</code></pre>"},{"location":"operations/cassandra/restore/restore-different-cluster/#amazon-s3","title":"Amazon S3","text":"<pre><code>--storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}'\n</code></pre>"},{"location":"operations/cassandra/restore/restore-different-cluster/#azure-blob-storage","title":"Azure Blob Storage","text":"<pre><code>--storage-config '{\"type\":\"azureblob\",\"account\":\"MY_AZURE_ACCOUNT_NAME\",\"key\":\"MY_AZURE_STORAGE_KEY\"}'\n</code></pre>"},{"location":"operations/cassandra/restore/restore-different-cluster/#google-cloud-storage","title":"Google Cloud Storage","text":"<pre><code>--storage-config '{\"type\":\"googlecloudstorage\",\"location\":\"us\",\"service_account_credentials\":\"ESCAPED_JSON_PRIVATE_KEY\"}'\n</code></pre>"},{"location":"operations/cassandra/restore/restore-different-cluster/#sshsftp","title":"SSH/SFTP","text":"<pre><code>--storage-config '{\"type\":\"sftp\",\"host\":\"&lt;sftp_server_hostname&gt;\",\"port\":\"22\",\"path\":\"/backup/path\",\"user\":\"&lt;sftp_username&gt;\",\"key_file\":\"~/private/key/file\"}'\n</code></pre>"},{"location":"operations/cassandra/restore/restore-different-cluster/#restore-to-a-different-table","title":"Restore to a different table","text":"<p>This feature is available in AxonOps Agent v1.0.61 or later</p> <p>When restoring a single table from a backup it is possible to use the <code>--dest-table</code> option on the command-line to load the restored data into table with a different name and/or keyspace to the original table. If you also supply the  <code>--restore-schema</code> option then the new table will be created as part of the restore process.</p> <p>NOTE: The destination keyspace must already exist before running the restore command.</p> <p>This example shows restoring the table <code>keyspace1.table1</code> into a table named <code>table1_restored</code> in keysace <code>restoreks</code>: <pre><code>/usr/share/axonops/axon-cassandra-restore \\\n  --restore \\\n  --org-id myaxonopsorg \\\n  --storage-config '{\"type\":\"s3\",\"path\":\"/axonops-cassandra-backups\",\"access_key_id\":\"MY_AWS_ACCESS_KEY\",\"secret_access_key\":\"MY_AWS_SECRET_ACCESS_KEY\",\"region\":\"eu-west-3\"}' \\\n  --source-cluster testcluster \\\n  --backup-id 2c1d9aca-5312-11ee-b686-bed50b9335ec \\\n  --source-hosts 026346a0-dc89-4235-ae34-552fcd453b42,84759df0-8a19-497e-965f-200bdb4c1c9b,94ed3811-12ce-487f-ac49-ae31299efa31 \\\n  --local-sstable-dir /opt/cassandra/axonops-restore \\\n  --use-sstable-loader \\\n  --cassandra-bin-dir /opt/cassandra/bin \\\n  --sstable-loader-options \"-d 10.0.0.1,10.0.0.2,10.0.0.3 -u cassandra -pw cassandra\" \\\n  --restore-schema \\\n  --cqlsh-options \"-u cassandra -p cassandra 10.0.0.1\" \\\n  --tables keyspace1.table1 \\\n  --dest-table restoreks.table1_restored\n</code></pre></p>"},{"location":"operations/cassandra/restore/restore-node-different-ip/","title":"Replace a node from a remote backup","text":"<p>Follow this procedure to restore a single Cassandra node from a total loss of all data where the replacement node has a different IP address from the original.</p> <p>NOTE: Restoring a node from a total loss can only be performed from a remote backup</p> <p>AxonOps identifies nodes by a unique host ID which is assigned when the agent starts up. In order to restore a backup to a node with a different IP address you must manually assign the AxonOps host ID of the old node to its new replacement.</p> <p>You can find the host ID of the old node on the Cluster Overview page of the AxonOps dashboard</p> <p>Infomy</p> <p></p> <p>If you still have access to the old server or its data then its host ID can also be found in the file <code>/var/lib/axonops/hostId</code></p>"},{"location":"operations/cassandra/restore/restore-node-different-ip/#manually-configure-the-axonops-agent-host-id","title":"Manually configure the AxonOps Agent host ID","text":"<p>Ensure the AxonOps Agent is stopped and clear any data that it may have created on startup</p> <pre><code>sudo systemctl stop axon-agent\nsudo rm -rf /var/lib/axonops/*\n</code></pre> <p>Manually apply the old node's host ID on the replacement (replace the host ID shown with your host ID from the previous steps)</p> <pre><code>echo '24d0cbf9-3b5a-11ed-8433-16b3c6a7bcc5' | sudo tee /var/lib/axonops/hostId\nsudo chown axonops.axonops /var/lib/axonops/hostId\n</code></pre> <p>Start the AxonOps agent</p> <pre><code>sudo systemctl start axon-agent\n</code></pre>"},{"location":"operations/cassandra/restore/restore-node-different-ip/#restore-a-backup-to-the-replacement-node","title":"Restore a backup to the replacement node","text":"<p>Ensure Cassandra is stopped on the new node and that its data directories are all empty <pre><code>sudo systemctl stop cassandra\nsudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/*\n</code></pre></p> <p>Allow the AxonOps user to write to the Cassandra data directory <pre><code>sudo chmod -R g+w /var/lib/cassandra/data\n</code></pre></p> <p>These commands assume you are storing the Cassandra data in the default location <code>/var/lib/cassandra/</code>, you will need to change the paths shown if your data is stored at a different location</p> <p>Now open the Restore page in the AxonOps Dashboard by going to Operations &gt; Restore</p> <p>Infomy</p> <p></p> <p>Choose the backup you wish to restore from the list and click the <code>RESTORE</code> button</p> <p>This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list.</p> <p>Infomy</p> <p></p> <p>Ensure only the node you wish to restore is selected in the checkbox list and start the restore by clicking the <code>REMOTE RESTORE</code> button.</p> <p>The restore progress will be displayed in the Backup Restorations in Progress list</p> <p>Infomy</p> <p></p> <p>After the restore has completed successfully, fix the ownership and permissions on the Cassandra data directories <pre><code>sudo chown -R cassandra.cassandra /var/lib/cassandra/data\nsudo chmod -R g-w /var/lib/cassandra/data\n</code></pre></p> <p>Now you can start cassandra on the restored node <pre><code>sudo systemctl start cassandra\n</code></pre></p> <p>After the replacement node has started up the old IP address may still be visible in Gossip. It should clear out automatically after a day or two, or you can perform a rolling restart of the cluster to make sure everything is up-to-date.</p>"},{"location":"operations/cassandra/restore/restore-node-same-ip/","title":"Restore a single node from a remote backup","text":"<p>Follow this procedure to restore a single Cassandra node from a total loss of all data where the replacement node has the same IP address as the original.</p> <p>NOTE: Restoring a node from a total loss can only be performed from a remote backup</p> <p>Ensure Cassandra is stopped on the new node and that its data directories are all empty <pre><code>sudo systemctl stop cassandra\nsudo rm -rf /var/lib/cassandra/commitlog/* /var/lib/cassandra/data/* /var/lib/cassandra/hints/* /var/lib/cassandra/saved_caches/*\n</code></pre></p> <p>Allow the AxonOps user to write to the Cassandra data directory <pre><code>sudo chmod -R g+w /var/lib/cassandra/data\n</code></pre></p> <p>These commands assume you are storing the Cassandra data in the default location <code>/var/lib/cassandra/</code>, you will need to change the paths shown if your data is stored at a different location</p> <p>Start axon-agent if it is not already running <pre><code>sudo systemctl start axon-agent\n</code></pre></p> <p>Now open the Restore page in the AxonOps Dashboard by going to Operations &gt; Restore</p> <p>Infomy</p> <p></p> <p>Choose the backup you wish to restore from the list and click the <code>RESTORE</code> button</p> <p>This will show the details of the backup and allow you to restore to all nodes or a subset using the checkboxes in the Nodes list.</p> <p>Infomy</p> <p></p> <p>Ensure only the node you wish to restore is selected in the checkbox list and start the restore by clicking the <code>REMOTE RESTORE</code> button.</p> <p>The restore progress will be displayed in the Backup Restorations in Progress list</p> <p>Infomy</p> <p></p> <p>After the restore has completed successfully, fix the ownership and permissions on the Cassandra data directories <pre><code>sudo chown -R cassandra.cassandra /var/lib/cassandra/data\nsudo chmod -R g-w /var/lib/cassandra/data\n</code></pre></p> <p>Start cassandra on the restored node <pre><code>sudo systemctl start cassandra\n</code></pre></p>"},{"location":"operations/cassandra/rollingrestart/overview/","title":"Rolling Restart","text":"<p>AxonOps provides a rolling restart functionality for Cassandra.</p> <p>The feature is accessible via Operations &gt; Rolling Restart</p> <p>Infomy</p> <p></p> <p>axonops user will require permissions to be able to stop and start Cassandra service. To do so you will add axonops user in the sudoers with for instance the following permissions: <pre><code>#/etc/sudoers.d/axonops\naxonops ALL=NOPASSWD: /sbin/service cassandra *, /usr/bin/systemctl * cassandra*\n</code></pre></p> <p>You can start an immediate rolling restart or schedule it.</p> <p>The script field let you able to tweak the predefined script executed by axon-agents during the restart process.</p> <p>You can also specify different degree of parallelism for the restart: DC, Rack and Node.</p> <p>For instance, to restart one entire rack at once across the cluster, you can set a large Node parallelism (greater than the number of nodes the rack has, ie 999). <pre><code>DC parallelism: 1\nRack parallelism: 1\nNode parallelism: 999\n</code></pre></p> <p>To restart one entire rack across each DC: <pre><code>DC parallelism: 999\nRack parallelism: 1\nNode parallelism: 999\n</code></pre></p>"},{"location":"overview/architecture/","title":"Architecture","text":""},{"location":"overview/architecture/#before","title":"Before","text":"<p>Our deployment model with the use of open source tools </p> <p>Infomy</p> <p></p>"},{"location":"overview/architecture/#axonops-deployment-model","title":"AxonOps Deployment Model","text":"<p>As you can see from the diagram below, we have massively simplified the stack with AxonOps.</p> <p>Infomy</p> <p></p> <p>You can also use a CQL datastore such as Cassandra, Elassandra, or Scylla to store the metrics. We recommend storing metrics on a CQL store on 100+ nodes clusters to improve your experience navigating the metrics dashboards.</p>"},{"location":"overview/motivation/","title":"What is AxonOps?","text":"<p>AxonOps is a One-Stop Operations Platform for Apache Cassandra\u00ae</p> <p>Built by Cassandra experts, the only cloud native solution to monitor, maintain and backup any Cassandra cluster anywhere</p>"},{"location":"overview/motivation/#why","title":"Why ?","text":"<p>Frustrated by the lack of tooling available to manage the next generation open source data platforms, AxonOps engineers decided to build a one-stop monitoring and operations platform to ensure they could provide the highest quality of services. Proven in some of the most demanding environments, AxonOps is now available as a standalone platform supporting Apache Cassandra today with Apache Kafka coming soon.</p>"},{"location":"overview/motivation/#features","title":"Features","text":"<ul> <li>Metric Dashboard - The AxonOps dashboard is pre-configured and well laid out in order for you to easily visualise the performance of your multiple Cassandra clusters across all of your data centres,</li> <li>Logs and Events - AxonOps agents collect logs from log files, as well as internal Cassandra events such as \u201crepair\u201d and JMX calls.</li> <li>Service Checks - As a site reliability engineer, service checks and the RAG status dashboard gives you great confidence in how your platform is operating. Regular checks against your processes, open ports, service health can be quickly implemented and deployed with minimum setup.</li> <li>Alert Integrations - Alerts can be configured for multiple services including Slack, Pagerduty, SMTP, and generic webhooks.</li> <li>Cassandra Repairs - Cassandra repairs are essential for maintaining the data integrity across all replicas.</li> <li>Backup and Restore - There are very few Cassandra tools that allow to setup Cassandra data backups as easily as AxonOps.</li> </ul>"},{"location":"overview/motivation/#company-timeline","title":"Company Timeline","text":"<p>To view the journey we have embarked on since 2017 have a look here</p>"},{"location":"overview/motivation/#motivation","title":"Motivation","text":"<p>AxonOps has been developed and actively maintained by AxonOps Limited, a company providing managed services for Apache Cassandra\u2122 and other modern distributed data technologies.</p> <p>AxonOps used a variety of modern and popular open source tools to manage its customer's data platforms to gain quick insight into how the clusters are working, and be alerted when there are issues.</p> <p>The open source tools we used are:</p> <ul> <li>Grafana - metrics dashboarding</li> <li>Prometheus - time series database for metrics</li> <li>Prometheus Alertmanager - metrics alerting</li> <li>ELK - Log capture and visualisation</li> <li>elastalert - Alerting on logs</li> <li>Consul - Service Discovery and Health Checks</li> <li>consul-alerts - Alerting on service health check failures</li> <li>Rundeck - Job Scheduler</li> <li>Ansible - Provisioning automation</li> </ul>"},{"location":"overview/motivation/#problems","title":"Problems","text":"<p>The tools listed above served us well. They gave us the confidence to manage enterprise deployment of distributed data platforms \u2013 alerting us when there are problems, ability to diagnose issues quickly, automatically performing routine scheduled tasks etc.</p> <p>However, using these tools and their problems were realised over time.</p> <ol> <li>Too many components - There are many components including the agents that need to be installed. Takes a lot of effort to integrate all components for each customer's on-premises environment, even with fully automated implementation using Ansible.</li> <li>Steep learning curve - The learning curve of deploying and configuring all the components is high.</li> <li>Patching hell - Patching schedule became a nightmare because of the sheer number of components. Imagine having to raise change requests for patching all above components!</li> <li>Enterprise hell - Firewall configurations became big for enterprise on-premises customers, often required many hours of tracing which change requests were unsuccessfully executed.</li> <li>Multiple dashboards - Multiple dashboards for metrics, logs and service availability.</li> <li>Complex alerting configurations - Alert notification configurations were all over the place. Fine-tuning alerts and updating them takes a lot of work.</li> </ol>"},{"location":"overview/motivation/#wish-list","title":"Wish List","text":"<p>With the above problems in mind, we needed to become more efficient as a company deploying the tools we need to manage our customers. After promoting the above tools to our customers, we ate the humble pie, and went back to the drawing board with the aim of reducing the efforts needed to on-board new customers.</p> <ul> <li>On-premises / cloud deployment</li> <li>Single dashboard for metrics / logs / service health</li> <li>Simple alert rules configurations</li> <li>Capture all metrics at high resolution (with Cassandra there are well over 20,000 metrics!)</li> <li>Capture logs and internal events like authentication, DDL, DML etc</li> <li>Scheduled backup / restore feature</li> <li>Performs domain specific administrative tasks, including Cassandra repair</li> <li>Manages the following products;<ul> <li>Apache Cassandra</li> <li>Apache Kafka</li> <li>DataStax Enterprise</li> <li>Confluent Enterprise</li> <li>Elasticsearch</li> <li>Apache Spark</li> <li>etc</li> </ul> </li> <li>Simplified deployment model</li> <li>Single agent for collecting metrics, logs, event, configs</li> <li>The same agent performs execution of health checks, backup, restore</li> <li>No JMX to capture the metrics, and must be push from the JVM and not pull</li> <li>Single socket connection initiated by agent to management server requiring only simple firewall rules</li> <li>Bidirectional communication between agent and management server over the single socket</li> <li>Modern snappy GUI</li> </ul>"},{"location":"pitr/configuration/","title":"Configure Commitlog Archiving","text":""},{"location":"pitr/configuration/#prerequisites","title":"Prerequisites","text":"<p>Backups are enabled and setup. For more on how to setup backups please follow the guide here</p>"},{"location":"pitr/configuration/#steps","title":"Steps:","text":"<p>In the AxonOps Dashboard on the left hand menu navigate to Operations --&gt; Backups</p> <p></p> <p>On the top tab select Commitlog Archiving.</p> <p></p>"},{"location":"pitr/configuration/#configuration","title":"Configuration","text":"<p>Complete the fields with the required value and click </p>"},{"location":"pitr/configuration/#fields","title":"Fields","text":"<ul> <li> <p><code>Data Centers (Mandatory)</code></p> <p>The Cassandra Cluster DC that you want to enable Commitlog Archiving.</p> </li> <li> <p><code>Retention (Mandatory)</code></p> <p>The amount of time in hours(h), days(d), weeks(w), month(M) or year(y) that you want to archive the commit logs for.</p> </li> <li> <p><code>Remote Storage</code></p> <p>The following options are available for Remote storage locations.</p> <ul> <li>AWS S3</li> <li>Google Cloud Storage</li> <li>local filesystem</li> <li>Microsoft Azure Blob Storage</li> <li>S3 Compatible</li> <li>SFTP/SSH</li> </ul> </li> <li> <p><code>Base Remote Path</code></p> <p>This is the name of the storage buckets, you can also add subfolders if using shared storage buckets or saving multiple clusters to the same bucket. By default AxonOps will save the backups to /bucket/folder/org/clustertype/clustername/host-id/</p> <p>The org/clustertype/clustername/host-id/ will match the top breadcrump navigation in your AxonOps Dashboard.</p> </li> </ul>"},{"location":"pitr/overview/","title":"Point in Time Recovery","text":"<p>The aim of the AxonOps Cassandra Commitlog Archiving(PITR) feature is to provide an easy to use graphical user interface instead of users having to configure it manually on every Cassandra node in a cluster.</p>"},{"location":"pitr/overview/#features","title":"Features","text":"<ul> <li>UI to configure Commitlog archiving per Data Center(DC) in your cluster.</li> <li>UI that uses backups and commitlogs to specify the Point-in-Time to restore your cluster state too. </li> <li>UI for for viewing current commitlog archiving status.</li> <li>Local or Remote Storage locations to store your commitlog archives.</li> <li>Retention periods for how long to keep your commitlog archives.</li> </ul>"},{"location":"pitr/overview/#understanding-commitlog-archiving","title":"Understanding Commitlog Archiving","text":"<p>In Apache Cassandra, the commitlog is a vital component of the database that records every write operation before it is applied to the data files (SSTables).  This mechanism ensures durability and helps recover data in case of a node failure.</p> <p>Commitlog Archiving takes this a step further by continuously saving these logs to a secure location. This process involves:</p> <ul> <li> <p><code>Capturing Every Change</code></p> <p>Every modification to the database, including inserts, updates, and deletions, is recorded in the commitlog.</p> </li> <li> <p><code>Archiving Logs</code></p> <p>These logs are periodically copied to an external storage location, creating a history of all database operations.</p> </li> <li> <p><code>Ensuring Data Durability</code></p> <p>In the event of a hardware failure or data corruption, the archived commitlogs can be used to reconstruct the state of the database.</p> </li> </ul> <p>This archiving process is essential for maintaining data integrity, enabling disaster recovery, and supporting compliance with data retention policies.</p>"},{"location":"pitr/overview/#point-in-time-restore-pitr","title":"Point-in-Time Restore (PITR)","text":"<p>Point-in-Time Restore (PITR) is a powerful feature that allows you to restore your database to a specific moment in time. This is particularly useful for recovering from data corruption, accidental deletions, or other operational errors.</p> <p>Here\u2019s how PITR works in Apache Cassandra:</p> <ul> <li> <p><code>Continuous Archiving</code></p> <p>As mentioned, commitlogs are continuously archived, creating a comprehensive record of all database operations.</p> </li> <li> <p><code>Restore Process</code></p> <p>When a restore is needed, the archived commitlogs are replayed from the last known good snapshot up to the desired point in time. This involves:</p> <ul> <li><code>Stopping the Database:</code> Halting operations to ensure data consistency.</li> <li><code>Applying Logs:</code> Reapplying the archived commitlogs to reconstruct the database state up to the specified timestamp.</li> <li><code>Restarting Operations:</code> Bringing the database back online, now restored to the desired point in time.</li> </ul> </li> </ul> <p>PITR is invaluable for maintaining business continuity and minimizing data loss in critical situations. It provides a granular level of control over data recovery, allowing enterprises to revert their databases to any precise moment before an issue occurred.</p>"},{"location":"pitr/overview/#why-commitlog-archiving-and-pitr-matter","title":"Why Commitlog Archiving and PITR Matter","text":"<p>Commitlog archiving and PITR are not just advanced database features; they are essential tools for enterprise-grade data management. They ensure that:   - <code>Data Integrity</code></p> <pre><code>Your data remains accurate and consistent, even in the face of failures.\n</code></pre> <ul> <li> <p><code>Disaster Recovery</code></p> <p>You can recover quickly from unforeseen disasters with minimal data loss.</p> </li> <li> <p><code>Regulatory Compliance</code></p> <p>You meet stringent data retention and auditing requirements.</p> </li> <li> <p><code>Operational Resilience</code></p> <p>You can handle accidental data modifications or deletions without significant downtime.</p> </li> </ul>"},{"location":"pitr/overview/#the-traditional-challenges-of-cassandra-commitlog-archiving-and-point-in-time-restore","title":"The Traditional Challenges of Cassandra Commitlog Archiving and Point-in-Time Restore","text":"<p>Typically, setting up Commitlog Archiving and Point-in-Time Restore in Cassandra is a complex and time-consuming process.  It involves configuring various components, ensuring compatibility between different systems, and maintaining an intricate setup that can often be fragile and prone to errors. These configurations require in-depth knowledge and continuous monitoring to ensure everything runs smoothly.</p> <p>Setup Commitlog (PITR) in a couple easy steps here</p>"},{"location":"pitr/restore/","title":"Restore from Point-in-Time","text":""},{"location":"pitr/restore/#requirements","title":"Requirements","text":"<ul> <li>Make sure Cassandra service is stopped on every node in the Cluster/Data Center(DC)</li> <li>Make sure Cassandra commitlog directory is emptied for every node you want to restore too.</li> <li>Check Cassandra Application Keyspace table directories are emptied for every node and keyspace/table you want to restore. </li> </ul>"},{"location":"pitr/restore/#steps","title":"Steps:","text":"<p>In the AxonOps Dashboard on the left hand menu navigate to Operations --&gt; Restore</p> <p></p> <p>On the top tab select Point-In-Time Recovery</p> <p></p> <p>You will be presented with the Point-In-Time restore screen. There are several steps that need to be done to complete a Point-in-time restore. </p> <p></p>"},{"location":"pitr/restore/#step-1-select-restore-point-in-time","title":"Step 1: Select Restore Point-in-time.","text":"<p>Select the Point-In-Time you want to restore your cluster too. </p> <p>Please complete the Date/Time selection and fields of the keyspace and table/s that you would like to restore.</p> <ul> <li> <p><code>Wide Time Range</code></p> <p>The wide time range is always a calendar month from the 1st to the last day of the selected month.</p> <p>To select a different month please use the  and  arrows on either side of the slider.</p> <p>At the beginning and end of the Wide Time Range slider there is a black bar that you can slide left and right to narrow the Date/Time of when you want to restore to.</p> </li> <li> <p><code>Zoomed Section</code></p> <p>This is a narrower view of the wide selection in the above slider.</p> <p>By default if the Wide Time Range slider is 30 days the Zoomed Section will be 30 days. </p> <p>The narrower the time range in the Wide Time Range slider the more precise the available date/time selection will be in the Zoomed section.</p> <p>Example of what the Zoomed section will look like when snapshots are available to select for PITR.</p> <p> <pre><code>The Camera Icon represents a snapshot at a point-in-time where a commitlog is archived.\n</code></pre></p> </li> <li> <p><code>Point in time</code></p> <p>A text based representation of the selection made in the Zoomed section slider. This is an altenative if you want to manually input a specific time. </p> </li> <li> <p><code>Keyspace(optional)</code></p> <p>The Application or System Keyspaces that were included as part of the commitlog archive process. If you don't specify a Keyspace all Keyspaces will be included in the restore process.</p> </li> <li> <p><code>Tables(optional)</code></p> <p>The Application or System Tables that were included as part of the commitlog archive process. If you don't specify a Table all Tables will be included in the restore process.</p> </li> </ul>"},{"location":"pitr/restore/#step-2-overview-of-point-in-time-nodeskeyspaces-and-tables","title":"Step 2: Overview of Point-in-time nodes,keyspaces and tables.","text":"<p>Confirm by clicking show details that all the tables and keyspaces you want to restore will be part of the restoration. You will be presented with the following screen. </p> <p></p> <ul> <li> <p><code>Tables</code></p> <p>Tables to be restored will be highlighted in Green, those to be excluded will be Greyed out.  The search box will allow you to search for and confirm specific tables or keyspaces will be restored.</p> </li> <li> <p><code>Commit Logs</code></p> <p>The last timestamp and file name of the commit log that will be used to recover to the specified point-in-time.</p> </li> <li> <p><code>Snapshots</code></p> <p>The Snapshot/s that will be used to recover to the specified point-in-time</p> </li> </ul>"},{"location":"pitr/restore/#step-3-confirmation-of-point-in-time-restoration-to-proceed","title":"Step 3: Confirmation of Point-in-time restoration to proceed.","text":"<p>Confirm you are ready to start the Point-in-time restore.</p> <p></p> <ul> <li>Stop Cassandra on the nodes that need to be restored. </li> <li>Delete the SStable files in the target table directories</li> <li>Delete commitlog files in the commitlog directory</li> </ul>"},{"location":"pitr/restore/#step-4-final-checks-and-last-steps-before-point-in-time-restore-starts","title":"Step 4: Final checks and last steps before Point-in-time restore starts.","text":"<p>The Axon-Agent service confirm the Cassandra nodes are stopped. Checks to ensure the clstuer is ready for the Point-in-time restore.</p> <p></p> <p>Once you have clicked Check PITR Readiness it will confirm that the Cluster is in the correct state for recovery to proceed.</p> <p>If any of the data directories still contain sstables a warning may be displayed. </p> <p>Click on show details button to view the tables affected and any errors/warnings.</p> <p></p>"},{"location":"pitr/restore/#step-5-start-the-point-in-time-restore","title":"Step 5: Start the Point-in-time restore.","text":"<p>Start the restore process</p> <p></p>"},{"location":"pitr/restore/#step-6-wait-for-completion-and-start-cassandra-nodes","title":"Step 6: Wait for completion and start Cassandra nodes.","text":"<p>The Status page of the Point-in-time restore process.  Once the restore has been completed the status will remain in a progressing state until the Cassandra nodes are restarted.</p> <p></p>"},{"location":"pitr/restore/#step-7-check-data-in-cassandra-has-been-recovered-to-specified-point-in-time","title":"Step 7: Check data in Cassandra has been recovered to specified Point-in-time.","text":"<p>Successful Point-in-time restore.  Please confirm the details and data in the tables that have been restored are present as expected.</p> <p></p>"},{"location":"system-requirements/","title":"System requirements","text":"<p>Minimum system requirements for installing AxonOps Server and AxonOps Dashboard on a Self-Hosted Server to Monitor and Manage the 6 nodes that are part of the Starter Tier</p> <p>Please note this is not a system requirements spec for your Cassandra nodes where the AxonOps Agent will be installed. </p> System Requirements AxonOps Server and AxonOps Dashboard Processor 1 gigahertz (GHz)\u202for\u202ffaster with 4 or more cores on a\u202fcompatible 64-bit processor. RAM 8 gigabytes (GB) Storage 20 GB or larger storage device. Elasticsearch Metrics Store Processor 1 gigahertz (GHz)\u202for\u202ffaster with 4, ideally 8 or more cores on a\u202fcompatible 64-bit processor. RAM 16 gigabytes (GB) Storage 120 GB or larger storage device. <p>If you need to increase the amount of nodes to be monitored, you can get in touch with us at here</p>"},{"location":"workbench/cassandra/license/","title":"License","text":"<p>This license applies to the AxonOps\u2122 Workbench product. Source Code for AxonOps\u2122 Workbench is available at https://github.com/axonops/axonops-workbench-cassandra under the Apache License 2.0 agreement at https://github.com/axonops/axonops-workbench-cassandra/blob/main/LICENSE</p>"},{"location":"workbench/cassandra/license/#end-user-license-agreement-eula-for-axonopstm-workbench","title":"End-User License Agreement (EULA) for AxonOps\u2122 Workbench","text":"<p>This End-User License Agreement (\"EULA\") is a legal agreement between you (either an individual or an entity) and AxonOps Limited (\"Licensor\") regarding your use of the AxonOps\u2122 Workbench desktop application and associated documentation (the \"Software\"). By downloading, installing, or using the Software, you agree to be bound by the terms of this EULA. If you do not agree to the terms of this EULA, do not download, install, or use the Software.</p> <ol> <li> <p>License Grant</p> <p>The Licensor grants you a non-exclusive, non-transferable, revocable license to use the Software for personal or internal business purposes in accordance with this EULA.</p> </li> <li> <p>Ownership</p> <p>The Software is licensed, not sold. The Licensor retains all rights, title, and interest in and to the Software, including all intellectual property rights.</p> </li> <li> <p>Restrictions</p> <p>You may not: Modify, reverse engineer, decompile, or disassemble the Software. Rent, lease, lend, sell, redistribute, or sublicense the Software. Use the Software for any unlawful purpose or in violation of any applicable laws.</p> </li> <li> <p>Telemetry and Data Collection</p> <p>The Software includes telemetry tracking that collects usage data, crash reports, and other related information to improve the Software. This data is collected anonymously and does not include personal information. By using the Software, you consent to this data collection.</p> </li> <li> <p>Disclaimer of Warranties</p> <p>The Software is provided \"as is\" without any warranties of any kind, either express or implied, including but not limited to implied warranties of merchantability, fitness for a particular purpose, or non-infringement. The Licensor does not warrant that the Software will be error-free or that its operation will be uninterrupted.</p> </li> <li> <p>Limitation of Liability</p> <p>In no event shall the Licensor be liable for any damages (including, without limitation, lost profits, business interruption, or lost information) arising out of the use of or inability to use the Software, even if the Licensor has been advised of the possibility of such damages. In no event will the Licensor's total liability to you exceed the amount you paid for the Software.</p> </li> <li> <p>Termination</p> <p>This EULA is effective until terminated. Your rights under this EULA will terminate automatically without notice from the Licensor if you fail to comply with any term(s) of this EULA. Upon termination, you must cease all use of the Software and destroy all copies of the Software.</p> </li> <li> <p>Governing Law</p> <p>This EULA shall be governed by and construed in accordance with the laws of England and Wales, without regard to its conflict of law principles.</p> </li> <li> <p>Entire Agreement</p> <p>This EULA constitutes the entire agreement between you and the Licensor concerning the Software and supersedes all prior or contemporaneous understandings regarding such subject matter. No amendment to or modification of this EULA will be binding unless in writing and signed by the Licensor.</p> </li> <li> <p>Contact Information</p> <p>If you have any questions about this EULA, please contact AxonOps Limited at 124 City Road, London, EC1V 2NX.</p> </li> </ol>"}]}